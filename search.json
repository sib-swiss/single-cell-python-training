[
  {
    "objectID": "ipynb/day3-4_velocity2.html",
    "href": "ipynb/day3-4_velocity2.html",
    "title": "RNA velocity with VeloCycle",
    "section": "",
    "text": "The cell cycle is an interesting case for RNA velocity estimation, as pseudotime methods along often fail as estimations of cyclical processes. Moreover, RNA velocity corresponds roughly to cell cycle speed, which is both experimentally verifiable. The cell cycle also unfolds on a timescale of less than 24 hours, which is well suited for studying cell dynamics using RNA lifecycle kinetics, such as with RNA velocity.\nA recent method has been developed called VeloCycle to estimate RNA velocity of the cell cycle on the real time scale. This method offers several advantages over existing approaches: - The ability to estimate uncertainty of velocity estimates (i.e. velocity confidence). - The ability to estimate both the low dimensional manifold and the velocity jointly. - The ability to perform statistical tests of velocity between conditions. - The ability to convert velocity estimates to a “real” time scale.\nComparing cell cycle velocities might be useful in a variable of scientific contexts: - Do two cancer subtumors proliferate as similar speeds? - Does a particular gene knockout or mutant impact the cell cycle speed? - Do progenitor cells in different regions of an organ (i.e., brain) or at different developmental stages divide equally quickly?\nHere, we will offer a short tutorial into VeloCycle, using the ductal cells from the pancreas dataset above. This will also offer insight into probabilistic modeling in Pyro, which is an advanced method used by many tools for modeling complex biological data.\nimport velocycle as vcy\nimport scvelo as scv\nimport scanpy as sc\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom velocycle import *\nimport anndata\nimport pyro\nimport torch\nimport copy\nadata_raw = scv.datasets.pancreas()\nadata_cycling = adata_raw[adata_raw.obs[\"clusters\"].isin([\"Ductal\"])].copy()\nadata_cycling\n\nAnnData object with n_obs × n_vars = 916 × 27998\n    obs: 'clusters_coarse', 'clusters', 'S_score', 'G2M_score'\n    var: 'highly_variable_genes'\n    uns: 'clusters_coarse_colors', 'clusters_colors', 'day_colors', 'neighbors', 'pca'\n    obsm: 'X_pca', 'X_umap'\n    layers: 'spliced', 'unspliced'\n    obsp: 'distances', 'connectivities'\nimport velocycle as vc from velocycle import *"
  },
  {
    "objectID": "ipynb/day3-4_velocity2.html#load-and-filter-dataset",
    "href": "ipynb/day3-4_velocity2.html#load-and-filter-dataset",
    "title": "RNA velocity with VeloCycle",
    "section": "Load and filter dataset",
    "text": "Load and filter dataset\n\nadata = adata_cycling.copy()\n\n\nsc.pl.umap(adata, color='clusters')\n\n\n\n\n\n\n\n\n\nfull_adatas = {\"pancreas_ductal\":adata[adata.obs[\"clusters\"].isin([\"Ductal\"])].copy()}\n\n\n# Filter lowly-expressed genes and concatenate all datasets\nfor a in full_adatas.keys(): \n    print(full_adatas[a].shape)\n    sc.pp.filter_genes(full_adatas[a], min_cells=int((full_adatas[a].n_obs)*0.10))\n    \ndata = anndata.concat(full_adatas, label=\"batch\", join =\"outer\")\n\n(916, 27998)\n\n\n\n# Perform some very basic gene filtering by unspliced counts\ndata = data[:, (data.layers[\"unspliced\"].toarray().mean(0) &gt; 0.1)].copy()\n\n# Perform some very basic gene filtering by spliced counts\ndata = data[:, data.layers[\"spliced\"].toarray().mean(0) &gt; 0.2].copy()\n\n\ndata.var.index = [i.upper() for i in data.var.index]\ndata\n\nAnnData object with n_obs × n_vars = 916 × 1394\n    obs: 'clusters_coarse', 'clusters', 'S_score', 'G2M_score', 'batch'\n    obsm: 'X_pca', 'X_umap'\n    layers: 'spliced', 'unspliced'\n\n\n\n# Create design matrix for dataset with a single batch\nbatch_design_matrix = preprocessing.make_design_matrix(data, ids=\"batch\")\n\n\n# Rough approximation of the cell cycle phase using categorical approaches \nsc.tl.score_genes_cell_cycle(data, s_genes=utils.S_genes_human, g2m_genes=utils.G2M_genes_human)\n\nWARNING: genes are not in var_names and ignored: ['BLM', 'BRIP1', 'CASP8AP2', 'CCNE2', 'CDC45', 'CDC6', 'CDCA7', 'CHAF1B', 'CLSPN', 'DSCC1', 'DTL', 'E2F8', 'EXO1', 'FEN1', 'GINS2', 'GMNN', 'MCM2', 'MCM4', 'MCM5', 'MCM6', 'MLF1IP', 'MSH2', 'PCNA', 'POLD3', 'RAD51', 'RAD51AP1', 'RPA2', 'RRM1', 'RRM2', 'SLBP', 'UBR7', 'UHRF1', 'UNG', 'USP1', 'WDR76']\nWARNING: genes are not in var_names and ignored: ['ANLN', 'AURKA', 'AURKB', 'BIRC5', 'BUB1', 'CCNB2', 'CDC20', 'CDC25C', 'CDCA3', 'CDCA8', 'CENPA', 'CENPF', 'CKAP2L', 'CKS1B', 'CTCF', 'DLGAP5', 'ECT2', 'FAM64A', 'G2E3', 'GAS2L3', 'GTSE1', 'HJURP', 'HMGB2', 'HMMR', 'HN1', 'KIF20B', 'KIF2C', 'LBR', 'MKI67', 'NDC80', 'NEK2', 'NUF2', 'PSRC1', 'TACC3', 'TMPO', 'TTK', 'TUBB4B', 'UBE2C']\n\n\n\n# Create size-normalized data layers\npreprocessing.normalize_total(data)\n\n\n# Get biologically-relevant gene set to use for velocity estimation\nfull_keep_genes = utils.get_cycling_gene_set(size=\"Medium\", species=\"Human\")"
  },
  {
    "objectID": "ipynb/day3-4_velocity2.html#run-the-manifold-learning-module",
    "href": "ipynb/day3-4_velocity2.html#run-the-manifold-learning-module",
    "title": "RNA velocity with VeloCycle",
    "section": "Run the manifold-learning module",
    "text": "Run the manifold-learning module\n\npyro.clear_param_store()\n\n\n# Set batch effect to zero because there is only a single dataset/batch\nΔν = torch.zeros((batch_design_matrix.shape[1], S.shape[1], 1)).float()\ncondition_on_dict = {\"Δν\": Δν}\n\n\nmetapar = preprocessing.preprocess_for_phase_estimation(anndata=data_to_fit, \n                                          cycle_obj=cycle_prior, \n                                          phase_obj=phase_prior, \n                                          design_mtx=batch_design_matrix,\n                                          n_harmonics=n_harm,\n                                          condition_on=condition_on_dict)\n\n\nphase_fit = phase_inference_model.PhaseFitModel(metaparams=metapar, \n                                                condition_on=condition_on_dict)\nphase_fit.check_model()\n\n Trace Shapes:                          \n  Param Sites:                          \n Sample Sites:                          \n    cells dist            |             \n         value        916 |             \n    genes dist            |             \n         value         61 |             \n  batches dist            |             \n         value          1 |             \n        ν dist     61   1 |   3         \n         value     61   1 |   3         \n       Δν dist   1 61   1 |             \n         value   1 61   1 |             \n      ϕxy dist        916 |   2         \n         value        916 |   2         \n        ϕ dist            | 916         \n         value            | 916         \n        ζ dist            | 916 3       \n         value            | 916 3       \n    ElogS dist            |   1 1 61 916\n         value            |   1 1 61 916\nshape_inv dist     61   1 |             \n         value     61   1 |             \n        S dist 1 1 61 916 |             \n         value     61 916 |             \n\n\n\nnum_steps = 2000\ninitial_lr = 0.03\nfinal_lr = 0.005\ngamma = final_lr / initial_lr\nlrd = gamma ** (1 / num_steps)\nadam = pyro.optim.ClippedAdam({'lr': initial_lr, 'lrd': lrd, 'betas': (0.80, 0.99)})\n\nphase_fit.fit(optimizer=adam, num_steps=num_steps)"
  },
  {
    "objectID": "ipynb/day3-4_velocity2.html#visualize-the-results",
    "href": "ipynb/day3-4_velocity2.html#visualize-the-results",
    "title": "RNA velocity with VeloCycle",
    "section": "Visualize the results",
    "text": "Visualize the results\n\n# Put estimations in new objects\ncycle_pyro = phase_fit.cycle_pyro\nphase_pyro = phase_fit.phase_pyro\n\n\nfit_ElogS = phase_fit.posterior[\"ElogS\"].squeeze().numpy()\nfit_ElogS2 = phase_fit.posterior[\"ElogS2\"].squeeze().numpy()\n\n\nname2color = {'G1':\"tab:blue\", 'S':\"tab:orange\", 'G2M':\"tab:green\"}\ngene_list = [\"CDK1\", \"HELLS\", \"SON\", \"TOP2A\", \"HAT1\"]\ngene_names = np.array(data_to_fit.var.index)\nplt.figure(None,(24, 4))\nix = 1\nfor i in range(0, len(gene_list)):\n    g = gene_list[i]\n    plt.subplot(1, len(gene_list), ix)\n    plt.scatter(phase_pyro.phis, \n                metapar.S[np.where(gene_names==g)[0][0], :].squeeze().cpu().numpy(), \n                s=10, alpha=0.5, c=[name2color[x] for x in data_to_fit.obs[\"phase\"]])\n    plt.scatter(phase_pyro.phis, \n                np.exp(fit_ElogS2[np.where(gene_names==g)[0][0], :]), \n                s=10, c=\"black\")\n    plt.title(g)\n    plt.xlabel(\"ϕ\")\n    plt.ylabel(\"counts\")\n    ix+=1\n    plt.xticks([0, np.pi/2, np.pi, 3*np.pi/2, 2*np.pi],[\"0\", \"π/2\", \"π\", \"3π/2\", \"2π\"])\n    plt.xlim(0, 2*np.pi)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nxs = phase_fit.fourier_coef[1]\nys = phase_fit.fourier_coef[2]\nr = np.log10( np.sqrt(xs**2+ys**2) / phase_fit.fourier_coef_sd[1:, :].sum(0) )\nangle = np.arctan2(xs, ys)\nangle = (angle)%(2*np.pi)\n\nphis_df = pd.DataFrame([angle, r])\nphis_df.columns = data_to_fit.var.index\n\nphase_data_frame = pd.concat([phase_fit.cycle_pyro.means, phase_fit.cycle_pyro.stds, phis_df]).T\nphase_data_frame.columns = [\"nu0 mean\", \"nu1sin mean\", \"nu1cos mean\",\n                            \"nu0 std\", \"nu1sin std\", \"nu1cos std\", \"peak_phase\", \"amplitude\"]\nphase_data_frame[\"is_seurat_marker\"] = [True if i in list(utils.S_genes_human)+list(utils.G2M_genes_human) else False for i in phase_data_frame.index]\nphase_data_frame.head()\n\nphis_df = pd.DataFrame(phase_fit.phase_pyro.phis.numpy())\nphis_df.index = data_to_fit.obs.index\nphis_df.columns = [\"cell_cycle_phi\"]\nphase_data_frame_cells = data_to_fit.obs.merge(phis_df, left_index=True, right_index=True)\n\n\n# Define the number of bins\nnum_bins = 10\nbin_width = 2 * np.pi / num_bins\n\n# Calculate the bin index for each gene\nphase_data_frame['bin_index'] = ((phase_data_frame['peak_phase'] + 2 * np.pi) % (2 * np.pi) / bin_width).astype(int)\n\n# Group genes by bin index and find top 10 genes in each bin\ntop_genes_per_bin = phase_data_frame.groupby('bin_index', group_keys=False).apply(lambda group: group.nlargest(5, 'amplitude'))\n\n\nkeep_genes = [a.upper() for a in cycle_prior.means.columns]\n\ngene_names = np.array(keep_genes)\n\nfrom matplotlib.colors import ListedColormap, LinearSegmentedColormap\nimport matplotlib.transforms as mtransforms\nimport seaborn as sns\n\nkeep_genes = [a.upper() for a in cycle_prior.means.columns]\ngene_names = np.array(keep_genes)\nS_genes_human = list(utils.S_genes_human)\nG2M_genes_human = list(utils.G2M_genes_human)\nphases_list = [S_genes_human, G2M_genes_human, [i.upper() for i in gene_names if i.upper() not in S_genes_human+G2M_genes_human]]\n\ng = []\ngradient = []\nfor i in range(len(phases_list)):\n    for j in range(len(phases_list[i])):\n        g.append(phases_list[i][j])\n        gradient.append(i)\n\ncolor_gradient_map = pd.DataFrame({'Gene': g,  'Color': gradient}).set_index('Gene').to_dict()['Color']\ncolored_gradient = pd.Series(gene_names).map(color_gradient_map)\n\nxs = phase_fit.fourier_coef[1]\nys = phase_fit.fourier_coef[2]\nr = np.log10( np.sqrt(xs**2+ys**2) / phase_fit.fourier_coef_sd[1:, :].sum(0) )\nangle = np.arctan2(xs, ys)\nangle = (angle)%(2*np.pi)\n\nN=50\nwidth = (2*np.pi) / N\n\nfig = plt.figure(figsize = (6, 6))\nax = fig.add_subplot(projection='polar')\n\n# First: only plot dots with a color assignment\nangle_subset = angle[~np.isnan(colored_gradient.values)]\nr_subset = r[~np.isnan(colored_gradient.values)]\ncolor_subset = colored_gradient.values[~np.isnan(colored_gradient.values)]\n\n# Remove genes with very low expression\nangle_subset = angle_subset[r_subset&gt;=-12]\ncolor_subset = color_subset[r_subset&gt;=-12]\ngene_names_subset = gene_names[r_subset&gt;=-12]\nr_subset = r_subset[r_subset&gt;=-12]\n\nx=100\n# Take a subset of most highly expressing genes to print the names \nangle_subset_best = angle_subset[r_subset&gt;np.percentile(r_subset, x)]\ncolor_subset_best = color_subset[r_subset&gt;=np.percentile(r_subset, x)]\ngene_names_subset_best = gene_names_subset[r_subset&gt;=np.percentile(r_subset, x)]\nr_subset_best = r_subset[r_subset&gt;=np.percentile(r_subset, x)]\n\n# Plot all genes in phases list\nnum2color = {0:\"tab:orange\", 1:\"tab:green\", 2:\"tab:grey\", 3:\"tab:blue\"}\nax.scatter(angle_subset, r_subset, c=[num2color[i] for i in color_subset], s=50, alpha=0.3, edgecolor='none', rasterized=True)\n\n# Select and plot on top the genes marking S and G2M traditionally\nangle_subset = angle_subset[color_subset!=2]\nr_subset = r_subset[color_subset!=2]\ngene_names_subset = gene_names_subset[color_subset!=2]\ncolor_subset = color_subset[color_subset!=2]\n\nax.scatter(angle_subset, r_subset, c=[num2color[i] for i in color_subset], s=50, alpha=1, edgecolor='none',rasterized=True)\n\n# Annotate genes\nfor (i, txt), c in zip(enumerate(gene_names), colored_gradient.values):\n    if txt in top_genes_per_bin.index:\n        ix = np.where(np.array(gene_names)==txt)[0][0]\n        ax.annotate(txt[0]+txt[1:].upper(), (angle[ix], r[ix]+0.02))\n\nplt.xlim(0, 2*np.pi)\nplt.ylim(-1, )\nplt.yticks([-1, -0.5, 0, 0.5, 1], size=15)\nplt.xticks([0, np.pi/2, np.pi, 3*np.pi/2, 2*np.pi],[\"0\", \"π/2\", \"π\", \"3π/2\", \"2π\"], size=15)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "ipynb/day3-4_velocity2.html#run-the-velocity-learning-module",
    "href": "ipynb/day3-4_velocity2.html#run-the-velocity-learning-module",
    "title": "RNA velocity with VeloCycle",
    "section": "Run the velocity-learning module",
    "text": "Run the velocity-learning module\n\npyro.clear_param_store()\n\n\ncondition_design_matrix = copy.deepcopy(batch_design_matrix)\n\n\nn_velo_harmonics = 0\nspeed_prior = angularspeed.AngularSpeed.trivial_prior(condition_names=[\"pancreas_ductal\"], \n                                                      harmonics=n_velo_harmonics)\n\n\ncondition_on_dict = {\"ϕxy\":phase_pyro.phi_xy_tensor.T,\n                     \"ν\": cycle_pyro.means_tensor.T.unsqueeze(-2),\n                     \"Δν\": torch.tensor(phase_fit.delta_nus),\n                     \"shape_inv\": torch.tensor(phase_fit.disp_pyro).unsqueeze(-1)}\n\n\nmetaparameters_velocity = preprocessing.preprocess_for_velocity_estimation(data_to_fit, \n                                                             cycle_pyro, \n                                                             phase_pyro, \n                                                             speed_prior,\n                                                             condition_design_matrix.float(), \n                                                             batch_design_matrix.float(), \n                                                             n_harmonics=n_harm,\n                                                             count_factor=metapar.count_factor,\n                                                             ω_n_harmonics=n_velo_harmonics, \n                                                             μγ=torch.tensor(0.0).detach().clone().float(),\n                                                             σγ=torch.tensor(0.5).detach().clone().float(),\n                                                             μβ=torch.tensor(2.0).detach().clone().float(),\n                                                             σβ=torch.tensor(3.0).detach().clone().float(),\n                                                             model_type=\"lrmn\",\n                                                             condition_on=condition_on_dict)\n\n\nvelocity_fit = velocity_inference_model.VelocityFitModel(metaparams=metaparameters_velocity, \n                                                         condition_on=condition_on_dict)\n\n\nvelocity_fit.check_model()\n\n  Trace Shapes:                              \n   Param Sites:                              \n  Sample Sites:                              \n     cells dist              |               \n          value          916 |               \n     genes dist              |               \n          value           61 |               \n harmonics dist              |               \n          value            1 |               \nconditions dist              |               \n          value            1 |               \n   batches dist              |               \n          value            1 |               \n     logγg dist       61   1 |               \n          value       61   1 |               \n     logβg dist       61   1 |               \n          value       61   1 |               \n  rho_real dist       61   1 |               \n          value       61   1 |               \n        γg dist       61   1 |  61   1       \n          value              |  61   1       \n         ν dist       61   1 |   3           \n          value       61   1 |   3           \n        Δν dist 1 1 1 61   1 |               \n          value 1 1 1 61   1 |               \n       ϕxy dist          916 |   2           \n          value          916 |   2           \n         ϕ dist              | 916           \n          value              | 916           \n         ζ dist              | 916   3       \n          value              | 916   3       \n      ζ_dϕ dist              | 916   3       \n          value              | 916   3       \n        νω dist   1 1  1   1 |               \n          value   1 1  1   1 |               \n        ζω dist              |   1 916       \n          value              |   1 916       \n     ElogS dist              |   1   1 61 916\n          value              |   1   1 61 916\n         ω dist              |   1 916       \n          value              |   1 916       \n     ElogU dist              |   1   1 61 916\n          value              |   1   1 61 916\n shape_inv dist       61   1 |               \n          value       61   1 |               \n         S dist   1 1 61 916 |               \n          value       61 916 |               \n         U dist   1 1 61 916 |               \n          value       61 916 |               \n\n\n\nvelocity_fit.check_guide()\n\n  Trace Shapes:              \n   Param Sites:              \n         ν_locs     61   1  3\n       ν_scales     61   1  3\n        Δν_locs 1 1  1  61  1\n       ϕxy_locs        916  2\n     logβg_locs         61  1\n   logβg_scales         61  1\n            loc            62\n     cov_factor         62  5\n       cov_diag            62\n   rho_real_loc            61\n shape_inv_locs         61  1\n  Sample Sites:              \n     cells dist             |\n          value        916  |\n     genes dist             |\n          value         61  |\n harmonics dist             |\n          value          1  |\nconditions dist             |\n          value          1  |\n   batches dist             |\n          value          1  |\n     logγg dist     61   1  |\n          value     61   1  |\n  rho_real dist     61   1  |\n          value     61   1  |\n     logβg dist     61   1  |\n          value     61   1  |\n        νω dist 1 1  1   1  |\n          value 1 1  1   1  |\n\n\n\nnum_steps = 5000\ninitial_lr = 0.03\nfinal_lr = 0.005\ngamma = final_lr / initial_lr\nlrd = gamma ** (1 / num_steps)\nadam = pyro.optim.ClippedAdam({'lr': initial_lr, 'lrd': lrd, 'betas': (0.80, 0.99)})\n\nvelocity_fit.fit(optimizer=adam, num_steps=num_steps)\n\n\n\n\n\n\n\n\n\n# Put estimations in new objects\ncycle_pyro = velocity_fit.cycle_pyro\nphase_pyro = velocity_fit.phase_pyro\nspeed_pyro = velocity_fit.speed_pyro\n\nfit_ElogS = velocity_fit.posterior[\"ElogS\"].squeeze()\nfit_ElogU = velocity_fit.posterior[\"ElogU\"].squeeze()\n\nfit_ElogS2 = velocity_fit.posterior[\"ElogS2\"].squeeze()\nfit_ElogU2 = velocity_fit.posterior[\"ElogU2\"].squeeze()\n\nlog_gammas = velocity_fit.log_gammas\nlog_betas = velocity_fit.log_betas\n\n\n# Store entire posterior sampling into an object\nfull_pps_velo = velocity_fit.posterior\n\n\nvelocity = full_pps_velo[\"ω\"].squeeze().numpy() / torch.exp(torch.mean(full_pps_velo[\"logγg\"].squeeze().mean(0).detach())).numpy()\n\n\nplt.hist(velocity.mean(1))\nplt.xlabel(\"Velocity Estimate\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Periodic Model Velocity Posterior\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Cell cycle time in hours\nprint(2*np.pi/velocity.mean())\n\n16.528259838181253\n\n\n\nvelocity_fit0 = velocity_fit\n\n\npyro.clear_param_store()\n\n\ncondition_design_matrix = copy.deepcopy(batch_design_matrix)\n\n\nn_velo_harmonics = 1\nspeed_prior = angularspeed.AngularSpeed.trivial_prior(condition_names=[\"pancreas_ductal\"], harmonics=n_velo_harmonics, \n                                                means=0.0, stds=3.0)\n\nspeed_prior.stds.loc[\"nu1_cos\"] = 0.01\nspeed_prior.stds.loc[\"nu1_sin\"] = 0.01\n\n\ncondition_on_dict = {\"ϕxy\":phase_pyro.phi_xy_tensor.T,\n                     \"ν\": cycle_pyro.means_tensor.T.unsqueeze(-2),\n                     \"Δν\": torch.tensor(phase_fit.delta_nus),\n                     \"shape_inv\": torch.tensor(phase_fit.disp_pyro).unsqueeze(-1)}\n\n\nmetaparameters_velocity = preprocessing.preprocess_for_velocity_estimation(data_to_fit, \n                                                             cycle_pyro, \n                                                             phase_pyro, \n                                                             speed_prior,\n                                                             condition_design_matrix.float(), \n                                                             batch_design_matrix.float(), \n                                                             n_harmonics=n_harm,\n                                                             count_factor=metapar.count_factor,\n                                                             ω_n_harmonics=n_velo_harmonics,\n                                                             μγ=torch.tensor(0.0).detach().clone().float(),\n                                                             σγ=torch.tensor(0.5).detach().clone().float(),\n                                                             μβ=torch.tensor(2.0).detach().clone().float(),\n                                                             σβ=torch.tensor(3.0).detach().clone().float(),\n                                                             model_type=\"lrmn\",\n                                                             condition_on=condition_on_dict)\n\n\nvelocity_fit = velocity_inference_model.VelocityFitModel(metaparams=metaparameters_velocity, \n                                                         condition_on=condition_on_dict, early_exit=False,\n                                                        num_samples=500, n_per_bin=50)\n\n\nvelocity_fit.check_model()\n\n  Trace Shapes:                              \n   Param Sites:                              \n  Sample Sites:                              \n     cells dist              |               \n          value          916 |               \n     genes dist              |               \n          value           61 |               \n harmonics dist              |               \n          value            3 |               \nconditions dist              |               \n          value            1 |               \n   batches dist              |               \n          value            1 |               \n     logγg dist       61   1 |               \n          value       61   1 |               \n     logβg dist       61   1 |               \n          value       61   1 |               \n  rho_real dist       61   1 |               \n          value       61   1 |               \n        γg dist       61   1 |  61   1       \n          value              |  61   1       \n         ν dist       61   1 |   3           \n          value       61   1 |   3           \n        Δν dist 1 1 1 61   1 |               \n          value 1 1 1 61   1 |               \n       ϕxy dist          916 |   2           \n          value          916 |   2           \n         ϕ dist              | 916           \n          value              | 916           \n         ζ dist              | 916   3       \n          value              | 916   3       \n      ζ_dϕ dist              | 916   3       \n          value              | 916   3       \n        νω dist   1 3  1   1 |               \n          value   1 3  1   1 |               \n        ζω dist              |   3 916       \n          value              |   3 916       \n     ElogS dist              |   1   1 61 916\n          value              |   1   1 61 916\n         ω dist              |   1 916       \n          value              |   1 916       \n     ElogU dist              |   1   1 61 916\n          value              |   1   1 61 916\n shape_inv dist       61   1 |               \n          value       61   1 |               \n         S dist   1 1 61 916 |               \n          value       61 916 |               \n         U dist   1 1 61 916 |               \n          value       61 916 |               \n\n\n\nnum_steps = 5000\ninitial_lr = 0.03\nfinal_lr = 0.005\ngamma = final_lr / initial_lr\nlrd = gamma ** (1 / num_steps)\nadam = pyro.optim.ClippedAdam({'lr': initial_lr, 'lrd': lrd, 'betas': (0.80, 0.99)})\n\nvelocity_fit.fit(optimizer=adam, num_steps=num_steps)\n\n\n\n\n\n\n\n\n\n# Put estimations in new objects\ncycle_pyro = velocity_fit.cycle_pyro\nphase_pyro = velocity_fit.phase_pyro\nspeed_pyro = velocity_fit.speed_pyro\n\nfit_ElogS = velocity_fit.posterior[\"ElogS\"].squeeze()\nfit_ElogU = velocity_fit.posterior[\"ElogU\"].squeeze()\n\nfit_ElogS2 = velocity_fit.posterior[\"ElogS2\"].squeeze()\nfit_ElogU2 = velocity_fit.posterior[\"ElogU2\"].squeeze()\n\nlog_gammas = velocity_fit.log_gammas\nlog_betas = velocity_fit.log_betas\n\n\n# Store entire posterior sampling into an object\nfull_pps_velo = velocity_fit.posterior\n\n\n# See the value of the mean gamma\ntorch.exp(torch.mean(full_pps_velo[\"logγg\"].squeeze().mean(0).detach())).numpy()\n\narray(0.9531931, dtype=float32)\n\n\n\nomega = full_pps_velo[\"ω\"].squeeze().numpy() / torch.exp(torch.mean(full_pps_velo[\"logγg\"].squeeze().mean(0).detach())).numpy()\nphi = phase_pyro.phis\nomegas = []\nphis = []\nn2n = {\"pancreas_ductal\":0}\nids = np.array([n2n[i] for i in np.array(data_to_fit.obs[\"batch\"])])\nfor i in range(len(data_to_fit.obs[\"batch\"].unique())):\n    omega1 = omega[:,np.where(ids == i)]\n    phi1 = phi[np.where(ids == i)]\n    omegas.append(omega1)\n    phis.append(phi1)\n\nlabels = np.array(data_to_fit.obs[\"batch\"].unique()) #list(adatas.keys())\n\ncolors = [\"tab:blue\"]\nfor i in range(len(omegas)):\n    plt.plot(phis[i][np.argsort(phis[i])], omegas[i].mean(0)[0][np.argsort(phis[i])], c=\"black\", linestyle='dashed')\n    \n    tmp5 = np.percentile(omega[:, ids==i], 5, axis=0)\n    tmp95 = np.percentile(omega[:, ids==i], 95, axis=0)\n    print(((2*np.pi)/omega[:, ids==i]).mean(), ((2*np.pi)/omega[:, ids==i]).std())\n    phi_i = phi[ids==i] \n    plt.fill_between(x=phi_i[np.argsort(phi_i)],\n                     y1=tmp5[np.argsort(phi_i)], \n                     y2=tmp95[np.argsort(phi_i)], \n                     alpha=0.6, color=colors[i], label = labels[i])\nplt.xlabel(\"Cell Cycle Phase\")\nplt.xlim(0, 2*np.pi)\nplt.ylabel(\"Scaled Velocity\")\nplt.legend()\nplt.show()\n\n17.340261 1.869181"
  },
  {
    "objectID": "ipynb/day3-3_trajectory_analysis.html",
    "href": "ipynb/day3-3_trajectory_analysis.html",
    "title": "Trajectory Inference and Pseudotime",
    "section": "",
    "text": "Presentation: Trajectory Inference and RNA velocity\nThis notebook is partially adapted from the PAGA tutorial here: tutorial"
  },
  {
    "objectID": "ipynb/day3-3_trajectory_analysis.html#loading-libraries",
    "href": "ipynb/day3-3_trajectory_analysis.html#loading-libraries",
    "title": "Trajectory Inference and Pseudotime",
    "section": "Loading libraries",
    "text": "Loading libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as pl\nfrom matplotlib import rcParams\nimport scanpy as sc\nimport scvelo as scv\n\nimport scipy\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.simplefilter(action=\"ignore\", category=Warning)"
  },
  {
    "objectID": "ipynb/day3-3_trajectory_analysis.html#reading-data",
    "href": "ipynb/day3-3_trajectory_analysis.html#reading-data",
    "title": "Trajectory Inference and Pseudotime",
    "section": "Reading data",
    "text": "Reading data\n\nadata = scv.datasets.pancreas()\nadata\n\nAnnData object with n_obs × n_vars = 3696 × 27998\n    obs: 'clusters_coarse', 'clusters', 'S_score', 'G2M_score'\n    var: 'highly_variable_genes'\n    uns: 'clusters_coarse_colors', 'clusters_colors', 'day_colors', 'neighbors', 'pca'\n    obsm: 'X_pca', 'X_umap'\n    layers: 'spliced', 'unspliced'\n    obsp: 'distances', 'connectivities'\n\n\n\nadata.obs\n\n\n\n\n\n\n\n\n\nclusters_coarse\nclusters\nS_score\nG2M_score\n\n\nindex\n\n\n\n\n\n\n\n\nAAACCTGAGAGGGATA\nPre-endocrine\nPre-endocrine\n-0.224902\n-0.252071\n\n\nAAACCTGAGCCTTGAT\nDuctal\nDuctal\n-0.014707\n-0.232610\n\n\nAAACCTGAGGCAATTA\nEndocrine\nAlpha\n-0.171255\n-0.286834\n\n\nAAACCTGCATCATCCC\nDuctal\nDuctal\n0.599244\n0.191243\n\n\nAAACCTGGTAAGTGGC\nNgn3 high EP\nNgn3 high EP\n-0.179981\n-0.126030\n\n\n...\n...\n...\n...\n...\n\n\nTTTGTCAAGTGACATA\nPre-endocrine\nPre-endocrine\n-0.235896\n-0.266101\n\n\nTTTGTCAAGTGTGGCA\nNgn3 high EP\nNgn3 high EP\n0.279374\n-0.204047\n\n\nTTTGTCAGTTGTTTGG\nDuctal\nDuctal\n-0.045692\n-0.208907\n\n\nTTTGTCATCGAATGCT\nEndocrine\nAlpha\n-0.240576\n-0.206865\n\n\nTTTGTCATCTGTTTGT\nEndocrine\nEpsilon\n-0.136407\n-0.184763\n\n\n\n\n3696 rows × 4 columns\n\n\n\n\nThis dataset contains single cells from very early in mouse development at multiple cell stages. Given that we know the exact temporal ordering of the cells, this dataset is ideal for demonstrating the purpose of trajectory inference and pseudotemporal ordering of single cells.\n\nadata.obs[\"clusters\"].value_counts()\n\nclusters\nDuctal           916\nNgn3 high EP     642\nPre-endocrine    592\nBeta             591\nAlpha            481\nNgn3 low EP      262\nEpsilon          142\nDelta             70\nName: count, dtype: int64\n\n\nWe first quickly perform the standard steps of normalization, log-transformation, and PCA on the dataset.\n\nsc.pp.normalize_total(adata)\nsc.pp.log1p(adata)\nsc.pp.highly_variable_genes(adata, n_top_genes=3000)\nadata = adata[:, adata.var[\"highly_variable\"]].copy()\nsc.tl.pca(adata)\n\n\nsc.pl.pca(adata, color='clusters')\n\n\n\n\n\n\n\n\nNumerous trajectory inference methods, including PAGA, are graph-based. To obtain such a needed graph, we need to examine the nearest neighbors of each cell. Let’s compute the neighborhood using 50 nearest neighbors and then embed the results on a UMAP.\n\nsc.pp.neighbors(adata, n_pcs = 30, n_neighbors = 50)\nsc.tl.umap(adata, min_dist=0.4, spread=3)\n\n\nsc.pl.umap(adata, color = ['clusters'],\n           legend_loc = 'on data', edges = True)"
  },
  {
    "objectID": "ipynb/day3-3_trajectory_analysis.html#run-paga",
    "href": "ipynb/day3-3_trajectory_analysis.html#run-paga",
    "title": "Trajectory Inference and Pseudotime",
    "section": "Run PAGA",
    "text": "Run PAGA\nUse the ground truth cell types to run PAGA. First we create the graph and initialize the positions using the umap.\n\n# use the umap to initialize the graph layout.\nsc.tl.draw_graph(adata, init_pos='X_umap')\nsc.pl.draw_graph(adata, color='clusters', legend_loc='on data', legend_fontsize = 'xx-small')\n\nWARNING: Package 'fa2' is not installed, falling back to layout 'fr'.To use the faster and better ForceAtlas2 layout, install package 'fa2' (`pip install fa2`).\n\n\n\n\n\n\n\n\n\n\nsc.tl.paga(adata, groups='clusters')\nsc.pl.paga(adata, color='clusters', edge_width_scale = 0.3)"
  },
  {
    "objectID": "ipynb/day3-3_trajectory_analysis.html#embedding-using-paga-initialization",
    "href": "ipynb/day3-3_trajectory_analysis.html#embedding-using-paga-initialization",
    "title": "Trajectory Inference and Pseudotime",
    "section": "Embedding using PAGA-initialization",
    "text": "Embedding using PAGA-initialization\nWe can now redraw the graph using another starting position from the paga layout. The following is just as well possible for a UMAP.\n\nsc.tl.draw_graph(adata, init_pos='paga')\n\nWARNING: Package 'fa2' is not installed, falling back to layout 'fr'.To use the faster and better ForceAtlas2 layout, install package 'fa2' (`pip install fa2`).\n\n\nNow we can see all marker genes also at single-cell resolution in a meaningful layout.\n\nsc.pl.draw_graph(adata, color=['clusters'], legend_loc='on data')"
  },
  {
    "objectID": "ipynb/day3-3_trajectory_analysis.html#gene-changes",
    "href": "ipynb/day3-3_trajectory_analysis.html#gene-changes",
    "title": "Trajectory Inference and Pseudotime",
    "section": "Gene changes",
    "text": "Gene changes\nWe can reconstruct gene changes along PAGA paths for a given set of genes\nChoose a root cell for diffusion pseudotime from the zygotic cells.\n\nadata.uns['iroot'] = np.flatnonzero(adata.obs['clusters']  == 'Ductal')[0]\nsc.tl.diffmap(adata)\nsc.tl.dpt(adata)\n\nUse the full raw data for visualization.\n\nsc.pl.draw_graph(adata, color=['clusters', 'dpt_pseudotime'], cmap=plt.cm.Spectral, wspace=0.2)\n\n\n\n\n\n\n\n\nBy looking at the different know lineages and the layout of the graph we define manually some paths to the graph that corresponds to specific lineages.\n\n# Define paths\n\npaths = [('beta', ['Ductal', 'Ngn3 low EP', 'Ngn3 high EP', 'Pre-endocrine', 'Beta']),\n        ('alpha', ['Ductal', 'Ngn3 low EP', 'Ngn3 high EP', 'Pre-endocrine', 'Alpha']),\n        ('delta', ['Ductal', 'Ngn3 low EP', 'Ngn3 high EP', 'Pre-endocrine', 'Delta']),\n        ('epsilon', ['Ductal', 'Ngn3 low EP', 'Ngn3 high EP', 'Pre-endocrine', 'Epsilon'])]\n\nadata.obs['distance'] = adata.obs['dpt_pseudotime']\n\nThen we select some genes that can vary in the lineages and plot onto the paths.\n\nsc.tl.rank_genes_groups(adata, \"clusters\", method=\"t-test\", n_genes=10)\n\n\npd.DataFrame(adata.uns[\"rank_genes_groups\"][\"names\"])\n\n\n\n\n\n\n\n\n\nDuctal\nNgn3 low EP\nNgn3 high EP\nPre-endocrine\nBeta\nAlpha\nDelta\nEpsilon\n\n\n\n\n0\nSpp1\nSpp1\nNeurog3\nMap1b\nPcsk2\nCpe\nRbp4\nGhrl\n\n\n1\nDbi\nDbi\nBtbd17\nFev\nGnas\nTmem27\nPyy\nIsl1\n\n\n2\nCldn3\nSparc\nSox4\nHmgn3\nRbp4\nPcsk1n\nHhex\nRbp4\n\n\n3\nMgst1\nMgst1\nMdk\nBex2\nMafb\nGnas\nHmgn3\nBex2\n\n\n4\nAnxa2\n1700011H14Rik\nGadd45a\nYpel3\nSec61b\nPyy\nIsl1\nFam183b\n\n\n5\nBicc1\nCldn3\nSmarcd2\nChga\nCpe\nTtr\nFam183b\nMaged2\n\n\n6\nKrt18\nAnxa2\nBtg2\nEmb\nGng12\nTspan7\nHadh\nCck\n\n\n7\nMt1\nClu\nTmsb4x\nCpe\nPcsk1n\nMeis2\nIapp\nAnpep\n\n\n8\nClu\nVim\nHes6\nCryba2\nRap1b\nGpx3\nArg1\nCard19\n\n\n9\n1700011H14Rik\nMt1\nCd63\nGlud1\nTuba1a\nFam183b\nSst\nArg1\n\n\n\n\n\n\n\n\n\ngene_names = [\"Spp1\", \"Dbi\", \"Cldn3\", \"Sparc\", \"Mgst1\", \"Cldn3\", \"Neurog3\", \"Btbd17\", \"Sox4\",\n              \"Map1b\", \"Fev\", \"Hmgn3\", \"Bex2\", \"Sec61b\", \"Tuba1a\", \"Meis2\", \"Pcsk1n\", \"Sst\", \"Arg1\",\n              \"Rbp4\", \"Pyy\", \"Hhex\", \"Ghrl\", \"Isl1\", \"Rbp4\", \"Bex2\"]\n\n\n_, axs = pl.subplots(ncols=4, figsize=(16, 8), gridspec_kw={\n                     'wspace': 0.05, 'left': 0.12})\npl.subplots_adjust(left=0.05, right=0.98, top=0.82, bottom=0.2)\nfor ipath, (descr, path) in enumerate(paths):\n    _, data = sc.pl.paga_path(\n        adata, path, gene_names,\n        show_node_names=False,\n        ax=axs[ipath],\n        ytick_fontsize=12,\n        left_margin=0.15,\n        n_avg=50,\n        annotations=['distance'],\n        show_yticks=True if ipath == 0 else False,\n        show_colorbar=False,\n        color_map='Greys',\n        groups_key='clusters',\n        color_maps_annotations={'distance': 'viridis'},\n        title='{} path'.format(descr),\n        return_data=True,\n        use_raw=False,\n        show=False)\npl.show()"
  },
  {
    "objectID": "ipynb/day2-3_clustering.html",
    "href": "ipynb/day2-3_clustering.html",
    "title": "Clustering and cell annotation",
    "section": "",
    "text": "import scanpy as sc\nimport pandas as pd # for handling data frames (i.e. data tables)\nimport numpy as np # for handling numbers, arrays, and matrices\nimport matplotlib.pyplot as plt # plotting package\nimport seaborn as sns # plotting package\n\nExercise 0: Before we continue in this notebook with the next steps of the analysis, we need to load our results from the previous notebook using the sc.read_h5ad function and assign them to the variable name adata. Give it a try!\n\nadata = sc.read_h5ad(\"PBMC_analysis_SIB_tutorial4.h5ad\")\n\n\nadata\n\nAnnData object with n_obs × n_vars = 5465 × 3000\n    obs: 'sample', 'n_counts', 'n_genes', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_20_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'log1p_total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'log1p_total_counts_hb', 'pct_counts_hb', 'is_doublet', 'S_score', 'G2M_score', 'phase'\n    var: 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'mean', 'std'\n    uns: 'hvg', 'log1p', 'neighbors', 'pca', 'phase_colors', 'sample_colors', 'umap'\n    obsm: 'X_pca', 'X_pcahm', 'X_umap'\n    varm: 'PCs'\n    obsp: 'connectivities', 'distances'\n\n\nClustering the data helps to identify cells with similar gene expression properties that may belong to the same cell type or cell state. There are two popular clustering methods, both available in scanpy: Leiden clustering.\nExercise 1: Run Leiden clustering algorithm. Visualize the clusters on your UMAP representation. Are the clusters different from each method? Visualize the clusters again, this time on the tSNE embedding instead of the UMAP embedding. Are there differences in which clusters are grouped together?\n\nsc.tl.leiden(adata)\n\nNext, you can visualize your UMAP and tSNE representations of the scRNA-seq and color by various metadata attributes (including Louvian or Leiden clusters) from the prior steps. For example:\n\nsc.pl.umap(adata, color=\"leiden\",legend_loc=\"on data\")\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py:394: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  cax = scatter(\n\n\n\n\n\n\n\n\n\nExercise 2: How many cells do you have per cluster? Can you plot a histogram of this?\n\nn_clusters = adata.obs[\"leiden\"].value_counts()\nax = n_clusters.plot(kind=\"bar\")\nplt.show()\n\n\n\n\n\n\n\n\nExercise 3: Visualize some of the other metadata on the UMAP embedding, including the n_counts, sample, n_genes, pct_counts_mt, and phase metadata found in adata.obs.\nDo any clusters seem to have an obvious bias towards particular attributes? This might be a sign that we want to optimize prior steps of the analysis, such as adjusting the number of principal components used in the neighborhood smoothing or regressing out particular variables. As with a pandas dataframe, you can also examine the frequency of various attributes using a command such as: adata.obs[\"phase\"].value_counts().\n\nsc.pl.umap(adata, color=['n_counts', 'sample'])\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py:394: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  cax = scatter(\n\n\n\n\n\n\n\n\n\n\nsc.pl.umap(adata, color=['n_genes', 'pct_counts_mt', 'phase'])\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py:394: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  cax = scatter(\n\n\n\n\n\n\n\n\n\nExercise 4: Let’s proceed with Leiden clustering and UMAP embeddings for the time being.\n\nCreate a new metadata attribute for your current clusters, i.e. adata.obs[\"leiden_res1\"] = adata.obs[\"leiden\"].\nRepeat leiden clustering using different values for the resolution parameter: 0.1, 0.5, 2.0.\nSave the clusters in a new metadata column and visualize them on the UMAP representation.\nHow does the number of clusters change with adjustments to the resolution parameter? Using the resolution=1 as a basis, do any clusters divide into two smaller clusters upon changing the resolution parameter? Do any clusters merge together? Can you plot the three different clustering results side-by-side on the UMAP to compare?\n\n\nadata.obs[\"leiden_res1\"] = adata.obs[\"leiden\"]\n\n\nsc.tl.leiden(adata, key_added=\"leiden_res0_1\", resolution=0.1)\nsc.tl.leiden(adata, key_added=\"leiden_res0_5\", resolution=0.5)\nsc.tl.leiden(adata, key_added=\"leiden_res2\", resolution=2)\n\n\nsc.pl.umap(\n    adata,\n    color=[\"leiden_res0_1\", \"leiden_res0_5\", \"leiden_res1\", \"leiden_res2\"],\n    legend_loc=\"on data\",\n    ncols=2,\n)\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py:394: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  cax = scatter(\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py:394: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  cax = scatter(\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py:394: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  cax = scatter(\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py:394: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  cax = scatter(\n\n\n\n\n\n\n\n\n\nOPTIONAL Exercise 5: Let’s take a few steps back to understand the previous steps a little bit better! For example, the number of principal components used in computing the neighborhood graph will greatly impact the visualizations. Rerun previous code using the following number of PCs and visualize the different UMAPs and number of clusters: 4 PCs, 8 PCs, 15 PCs, 30 PCs. What changes with the different number of PCs used? Choose an “optimal” number of PCs by examining the contribution of each PC to the total variance with the command: sc.pl.pca_variance_ratio(adata, log=True).\n\nadata.write_h5ad(\"PBMC_analysis_SIB_tutorial5.h5ad\")"
  },
  {
    "objectID": "ipynb/day2-1_dimensionality_reduction.html",
    "href": "ipynb/day2-1_dimensionality_reduction.html",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "After having completed this chapter you will be able to:\n\nUnderstand the differences between PCA and UMAP\nPerform PCA and select a reasonable number of components for downstream UMAP embeddings\nVisualize the contribution of different genes to each principal component\n\n\nimport scanpy as sc\nimport pandas as pd # for handling data frames (i.e. data tables)\nimport numpy as np # for handling numbers, arrays, and matrices\nimport matplotlib.pyplot as plt # plotting package\nimport seaborn as sns # plotting package\n\nExercise 0: Before we continue in this notebook with the next steps of the analysis, we need to load our results from the previous notebook using the sc.read_h5ad function and assign them to the variable name adata. Give it a try!\n\nClick for Answer\n\n\nAnswer: Load your data from the previous notebook into an h5ad object with sc.read_h5ad\n    adata = sc.read_h5ad(\"PBMC_analysis_SIB_tutorial2.h5ad\")"
  },
  {
    "objectID": "ipynb/day2-1_dimensionality_reduction.html#learning-outcomes",
    "href": "ipynb/day2-1_dimensionality_reduction.html#learning-outcomes",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "After having completed this chapter you will be able to:\n\nUnderstand the differences between PCA and UMAP\nPerform PCA and select a reasonable number of components for downstream UMAP embeddings\nVisualize the contribution of different genes to each principal component\n\n\nimport scanpy as sc\nimport pandas as pd # for handling data frames (i.e. data tables)\nimport numpy as np # for handling numbers, arrays, and matrices\nimport matplotlib.pyplot as plt # plotting package\nimport seaborn as sns # plotting package\n\nExercise 0: Before we continue in this notebook with the next steps of the analysis, we need to load our results from the previous notebook using the sc.read_h5ad function and assign them to the variable name adata. Give it a try!\n\nClick for Answer\n\n\nAnswer: Load your data from the previous notebook into an h5ad object with sc.read_h5ad\n    adata = sc.read_h5ad(\"PBMC_analysis_SIB_tutorial2.h5ad\")"
  },
  {
    "objectID": "ipynb/day2-1_dimensionality_reduction.html#principal-component-analysis",
    "href": "ipynb/day2-1_dimensionality_reduction.html#principal-component-analysis",
    "title": "Dimensionality Reduction",
    "section": "Principal component analysis",
    "text": "Principal component analysis\nDimensionality reduction methods seek to take a large set of variables and return a smaller set of components that still contain most of the information in the original dataset. One of the simplest forms of dimensionality reduction is PCA. Principal component analysis (PCA) is a mathematical procedure that transforms a number of possibly correlated (e.g., expression of genes in a network) variables into a (smaller) number of uncorrelated variables called principal components (“PCs”).\nExercise 1: Run PCA analysis using sc.pp.pca in scanpy. Plot a scatter plot of the first principal components and try to identify how many components are needed to explain 50% of the variance in the data.\n\nsc.pp.pca(adata, svd_solver=\"arpack\", n_comps=50)\n\nWe can then visualize the first two components of the PCA as a scatter plot using ss.pl.pca. The first two components capture the largest axes of variability in the data.\n\nsc.pl.pca_scatter(adata, color=\"sample\")\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py:394: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  cax = scatter(\n\n\n\n\n\n\n\n\n\nExercise 2: We can color the PCA plot according to any factor that is present in adata.obs, or for any gene’s expression. Can you color by the column n_counts and phase? What about by the genes HBA1 (an alpha subunit of hemoglobin) IGKC (one of the most highly variable genes)?\n\n# visualize the first PCs, color by cell cycle phase and n_counts (unnormalized)\nsc.pl.pca_scatter(adata, color=[\"n_counts\", \"phase\"])\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py:394: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  cax = scatter(\n\n\n\n\n\n\n\n\n\nEach principal component scores the contribution of each gene to that component. Therefore, we can see which genes are more highly correlated to one component compared to the others.\n\nsc.pl.pca_loadings(adata, components = '1,2,3')\n\n\n\n\n\n\n\n\nSince PCA is a geometric form of dimensionality reduction, we can visualize the specific genes that most strongly contribute to each principal component. This allows us to get an idea of which components discriminate which cells from each other.\n\ndef pca_heatmap(adata, component, groupby, use_raw=False, layer=None):\n    attr = 'varm'\n    keys = 'PCs'\n    scores = getattr(adata, attr)[keys][:, component]\n    dd = pd.DataFrame(scores, index=adata.var_names)\n    var_names_pos = dd.sort_values(0, ascending=False).index[:20]\n\n    var_names_neg = dd.sort_values(0, ascending=True).index[:20]\n\n    pd2 = pd.DataFrame(adata.obsm['X_pca'][:, component], index=adata.obs.index)\n\n    bottom_cells = pd2.sort_values(0).index[:300].tolist()\n    top_cells = pd2.sort_values(0, ascending=False).index[:300].tolist()\n\n    sc.pl.heatmap(adata[top_cells+bottom_cells], list(var_names_pos) + list(var_names_neg), \n                        show_gene_labels=False, groupby=groupby,\n                        swap_axes=True, cmap='viridis', \n                        use_raw=use_raw, layer=layer, figsize=(6,4))\n\nExercise 3: Use the pca_heatmap function defined above to plot the 0th and 1st components. Group the top/bottom cells by their sample identifier?\n\npca_heatmap(adata, component=0, groupby=\"sample\")\n\n\n\n\n\n\n\n\n\npca_heatmap(adata, component=1, groupby=\"sample\")\n\n\n\n\n\n\n\n\nExercise 4: Do you see anything strange about these results? Is there something undesirable about the way in which the PCs discriminate cell populations that might not be biological?\nFor further dimensionality reduction, we need to select a number of PCs to use (the rest are excluded). Ideally, we want to capture as much data variance as possible in as few PCs as possible. The plot generated with pca_variance_ratio can help you in determining how many PCs to use for downstream analysis such as UMAP:\n\nsc.pl.pca_variance_ratio(adata, log=False) # see contribution of each PC to variance\n\n\n\n\n\n\n\n\nThis plot ranks principle components based on the percentage of variance explained by each one. Where we observe an “elbow” or flattening curve, the majority of true signal is captured by this number of PCs, eg around 20 PCs for the adata dataset.\nIncluding too many PCs usually does not affect much the result, while including too few PCs can affect the results very much\nAnother - maybe more intuitive - way to see it, is to use a cumulative sum to show the % unexplained variance in function of the number of PCs. If we look at the first PCs, we can see that we passed the elbow with 30 to 50 PCs, which should ensure that we capture the major part of the variability in our dataset to be shown on a 2D representation like UMAP or tSNE.\n\nplt.plot(100 - (np.cumsum(adata.uns[\"pca\"][\"variance_ratio\"])*100)/sum(adata.uns[\"pca\"][\"variance_ratio\"]))\nplt.ylabel('% unexplained variance') ; plt.xlabel('N PCs')\n\nText(0.5, 0, 'N PCs')"
  },
  {
    "objectID": "ipynb/day2-1_dimensionality_reduction.html#dimensionality-reduction",
    "href": "ipynb/day2-1_dimensionality_reduction.html#dimensionality-reduction",
    "title": "Dimensionality Reduction",
    "section": "Dimensionality reduction",
    "text": "Dimensionality reduction\nExercise 5: Compute the neighborhood graph of cells using the PCA representation of the data matrix. The purpose of this step is to understand the “distance” between individual cells in the lower-dimensional PCA space, important for creating 2D scatter plot representations of your data. The number of neighbors used will influence how much the data is smoothened, which is a necessary step due to the sparsity (missing values) widely present in scRNA-seq data compared to bulk methods.\n\nsc.pp.neighbors(adata, n_pcs = 30) # specify the number of neighbors and number of PCs you wish to use\n\nEmbedding the graph in a 2D representation can be performed using either tSNE or UMAP algorithms.\nExercise 6: Run UMAP algorithm on your data after completing the previous steps with default parameters. We will evaluate the quality of each approach in later exercises.\n\nsc.tl.umap(adata)\n\nTo view the UMAP plot:\n\nsc.pl.umap(adata, color=[\"sample\"])\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py:394: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  cax = scatter(\n\n\n\n\n\n\n\n\n\nExercise 7: Try to change:\nA. Color the dots in the UMAP according to a variable (e.g. n_counts or HBA1). Any idea where the erythrocytes probably are in the UMAP?\nB. The number of neighbors used for the calculation of the UMAP. Which is the parameter to change and how did it affect the output. What is the default? In which situation would you lower/increase this?\nC. The number of principal components (n_pcs) to extremely few (5) or many (50). How does this it affect the output? In your opinion, it is better with fewer or more PCs? Why does n_pcs=150 not work? When would more precision be needed?\n\nsc.pl.umap(adata, color=[\"n_counts\", \"HBA1\"])\n\n\n\n\n\n\n\n\nSave your results!\n\nadata.write_h5ad(\"PBMC_analysis_SIB_tutorial3.h5ad\")"
  },
  {
    "objectID": "ipynb/day1-2_analysis_tools_qc.html",
    "href": "ipynb/day1-2_analysis_tools_qc.html",
    "title": "Analysis tools and quality control (QC)",
    "section": "",
    "text": "In this exercise, you will begin to learn about the standard workflow for analyzing scRNA-seq count data in Python. As single cell data is complex and often tailored to the particular experimental design, so there is not one “correct” approach to analyzing these data. However, certain steps have become accepted as a sort of standard “best practice.”\nA useful overview on the current best practices is found in the articles below, which we also borrow from in this tutorial. We thank the authors for compiling such handy resources!\nCurrent best practices in single-cell RNA-seq analysis are explained in a recent Nature Review: https://www.nature.com/articles/s41576-023-00586-w\nAccompanying this review is an online webpage, which is still under development but can be quite handy nonetheless: https://www.sc-best-practices.org/preamble.html"
  },
  {
    "objectID": "ipynb/day1-2_analysis_tools_qc.html#learning-outcomes",
    "href": "ipynb/day1-2_analysis_tools_qc.html#learning-outcomes",
    "title": "Analysis tools and quality control (QC)",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nAfter having completed this chapter you will be able to:\n\nLoad single cell data into Python.\nExplain the basic structure of a AnnData object and extract count data and metadata.\nCalculate and visualize quality measures based on:\n\nmitochondrial genes\nribosomal genes\nhemoglobin genes\nrelative gene expression\n\nInterpret the above quality measures per cell.\nPerform cell filtering based on user-selected quality thresholds."
  },
  {
    "objectID": "ipynb/day1-2_analysis_tools_qc.html#loading-scrnaseq-data",
    "href": "ipynb/day1-2_analysis_tools_qc.html#loading-scrnaseq-data",
    "title": "Analysis tools and quality control (QC)",
    "section": "Loading scRNAseq data",
    "text": "Loading scRNAseq data\nAfter the generation of the count matrices with CellRanger, the next step is the data analysis. The scanpy package is currently the most popular software in Python to do this. To start working with scanpy, you must import the package into your Jupyter notebook as follows:\n\nimport scanpy as sc\n\nAn excellent resource for documentation on scanpy can be found on the software page at the following link: https://scanpy.readthedocs.io/en/stable/\nThere are some supplemental packages for data handling and visualization that are also very useful to import into your notebook as well.\n\nimport pandas as pd # for handling data frames (i.e. data tables)\nimport numpy as np # for handling numbers, arrays, and matrices\nimport matplotlib.pyplot as plt # plotting package\n\nFirst, we will load a file specifying the different samples, and create a dictionary “datadirs” specifying the location of the count data:\n\nsample_info = pd.read_csv(\"../course_data/sample_info_course.csv\")\n\ndatadirs = {}\nfor sample_name in sample_info[\"SampleName\"]:\n    if \"PBMMC\" in sample_name:\n        datadirs[sample_name] = \"../course_data/count_matrices/\" + sample_name + \"/outs/filtered_feature_bc_matrix\"\n\nTo run through a typical scanpy analysis, we will use the files that are in the directory outs/filtered_feature_bc_matrix. This directory is part of the output generated by CellRanger.\nWe will use the list of file paths generated in the previous step to load each sample into a separate AnnData object. We will then store all six of those samples in a list called adatas, and combine them into a single AnnData object for our analysis.\n\nadatas = []\nfor sample in datadirs.keys():\n    print(\"Loading: \", sample)\n    curr_adata = sc.read_10x_mtx(datadirs[sample]) # load file into an AnnData object\n    curr_adata.obs[\"sample\"] = sample\n    curr_adata.X = curr_adata.X.toarray()\n    adatas.append(curr_adata)\n    \nadata = sc.concat(adatas) # combine all samples into a single AnnData object\nadata.obs_names_make_unique() # make sure each cell barcode has a unique identifier\n\nLoading:  PBMMC_1\nLoading:  PBMMC_2\nLoading:  PBMMC_3\n\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/anndata/_core/anndata.py:1838: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n  utils.warn_names_duplicates(\"obs\")\n\n\nThe AnnData object is similar to a detailed spreadsheet! Some basic commands to view the object are shown below. For a new dataset, there will be little to no metdata other than Cell IDs and gene names, but as you perform analyses, the metadata fields will be populated with more detail.\nExercise 1: Check what’s in the adata object, by typing adata in the Python console. How many gene features are in there? And how many cells?\n\nClick for Answer\n\n\nAnswer: typing adata should return:\n    AnnData object with n_obs × n_vars = 6946 × 33694\n    obs: 'sample'\n\nYou can also confirm the number of observations (cells) and variables (genes/features) using the commands below:\n    adata.n_obs # number of cells\n    adata.n_vars # number of genes"
  },
  {
    "objectID": "ipynb/day1-2_analysis_tools_qc.html#the-anndata-object",
    "href": "ipynb/day1-2_analysis_tools_qc.html#the-anndata-object",
    "title": "Analysis tools and quality control (QC)",
    "section": "The AnnData object",
    "text": "The AnnData object\nThe adata object we have created has the class AnnData. The object contains the single-cell count matrix, accessible with the command adata.X as well as various slots that specify sample metadata. This metadata is pretty limited when first loading the output from cellranger, but we will populate it with more useful information (i.e., cluster information) in later steps of the analysis.\nTo access the metadata corresponding to the cells (i.e. cell barcode, batch), you can enter the command adata.obs. To access the metadata corresponding to the genes (i.e, gene names, chromosome, etc), you can enter the command adata.var. These commands return a pandas.dataframe object. This data frame can be manipulated using any functions from the pandas package, including sum(), mean(), groupby(), and value_counts().\n\nadata.X # view the count matrix (rows x columns, cells x genes)\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)\n\n\n\nadata.obs.head() # view a pandas data frame containing metadata on the cells\n\n\n\n\n\n\n\n\n\nsample\n\n\n\n\nAAACCTGCAGACGCAA-1\nPBMMC_1\n\n\nAAACCTGTCATCACCC-1\nPBMMC_1\n\n\nAAAGATGCATAAAGGT-1\nPBMMC_1\n\n\nAAAGCAAAGCAGCGTA-1\nPBMMC_1\n\n\nAAAGCAACAATAACGA-1\nPBMMC_1\n\n\n\n\n\n\n\n\n\nadata.var.head() # view a pandas data frame containing metadata on the cells\n\n\n\n\n\n\n\n\n\n\n\n\n\nRP11-34P13.3\n\n\nFAM138A\n\n\nOR4F5\n\n\nRP11-34P13.7\n\n\nRP11-34P13.8\n\n\n\n\n\n\n\n\nExercise 2: Use the pandas value_counts() function to determine how many cells were collected for each of the six samples saved into your adata object? Keep in mind, since this is cell-specific metadata, we will want to work with the data frame returned by typing adata.obs.\n\nClick for Answer\n\n\nAnswer: You can run the command adata.obs[“sample”].value_counts() to view the number of cells per sample.\nThe output should be:\n    sample\n    PBMMC_2         3105\n    PBMMC_3         2229\n    PBMMC_1         1612\n    Name: count, dtype: int64"
  },
  {
    "objectID": "ipynb/day1-2_analysis_tools_qc.html#quality-control",
    "href": "ipynb/day1-2_analysis_tools_qc.html#quality-control",
    "title": "Analysis tools and quality control (QC)",
    "section": "Quality control",
    "text": "Quality control\nIn general, quality control (QC) should be done before any downstream analysis is performed. How the data is cleaned will likely have huge effects on downstream results, so it’s imperative to invest the time in choosing QC methods that you think are appropriate for your data! There are some “best practices” but these are by no means strict standards and also have certain limitations: https://www.sc-best-practices.org/preprocessing_visualization/quality_control.html\nGoals: - Filter the data to only include cells that are of high quality. This includes empty droplets, cells with a low total number of UMIs, doublets (two cells that got the same cell barcode), and dying cells (with a high fraction of mitochondrial reads).\nChallenges: - Delineating cells that are poor quality from less complex cell types - Choosing appropriate thresholds for filtering, so as to keep high quality cells without removing biologically relevant cell types or cell states.\nBefore analyzing the scRNA-seq gene expression data, we should ensure that all cellular barcode data corresponds to viable cells. Cell QC is commonly performed based on three QC covariates: - Library size: the number of counts per barcode (count depth) - Detected genes: the number of genes per barcode - Mitochondrial reads: the fraction of counts from mitochondrial genes per barcode.\nLibrary size: First, consider the total number of reads (UMIs) detected per cell. Cells with few reads are likely to have been broken or failed to capture a cell, and should thus be removed. Cells with many reads above the average for a sample are likely to be doublets, or two cells encapsulated in the gel bead during the protocol.\nExercise 3: Using the scanpy and matplotlib packages, visualize a histogram of the distribution of total counts per cell in the dataset. Save this information as metadata to the adata.obs dataframe using a command such as: adata.obs[\"n_counts\"] = n_counts_array. Choose lower and upper boundaries to filter out poor-quality cells and doublets.\nThe histogram function is: plt.hist() https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html\nTip: each function, such as plt.hist() has a set of required arguments. To view those required arguments, as well as an optional arguments, from your jupyter notebook, simply click with your cursor between the () parenthesis of the function, and tab tab+shift on your keyboard.\n\nn_counts_array = adata.X.sum(axis=1) # axis=1 to sum over genes, axis=0 to sum over cells\nadata.obs['n_counts'] = n_counts_array\n\n\nplt.hist(adata.obs['n_counts'], bins=100)\nplt.xlabel(\"Number of UMIs\")\nplt.ylabel(\"Number of cells\")\nplt.axvline(2000, c=\"r\") # specify the lower cutoff for total UMIs\nplt.axvline(12500, c=\"r\") # specify the upper cutoff for total UMIs\nplt.xlim(0, 20000)\nplt.show()\n\n\n\n\n\n\n\n\nExercise 4: Using the scanpy and matplotlib packages, visualize a histogram of the distribution of total genes expressed per cell in the dataset. Save this information as metadata to the adata.obs dataframe using a command such as: adata.obs[\"n_genes\"] = n_genes_array. Choose lower and upper boundaries to filter out low-diversity cells.\n\nexpressed_genes = np.sum(adata.X &gt; 0, 1)\nadata.obs['n_genes'] = expressed_genes\n\nplt.hist(adata.obs['n_genes'], bins=100)\nplt.axvline(500, c=\"r\") # specify the lower cutoff for number of detected genes\nplt.axvline(4000, c=\"r\") # specify the upper cutoff for number of detected genes\nplt.xlabel(\"Number of Genes\")\nplt.ylabel(\"Number of Cells\")\nplt.show()\n\n\n\n\n\n\n\n\nNext, we want to consider filtering cells with high levels of certain classes of genes, namely mitochondrial, ribosomal, and/or hemoglobin genes. There is a different rationale for filtering cells with high levels of these gene classes:\n\nMitochondrial genes: If a cell membrane is damaged, it looses free RNA quicker compared to mitochondrial RNA, because the latter is part of the mitochondrion. A high relative amount of mitochondrial counts can therefore point to damaged cells (Lun et al. 2016).\nRibosomal genes: Are not rRNA (ribosomal RNA) but is mRNA that code for ribosomal proteins. They do not point to specific issues, but it can be good to have a look at their relative abundance. They can have biological relevance (e.g. Caron et al. 2020).\nHemoglobin genes: these transcripts are very abundant in erythrocytes. Depending on your application, you can expect ‘contamination’ of erythrocytes and select against it.\n\nIn order to have an idea about the relative counts of these type of genes in our dataset, we can calculate their expression as relative counts in each cell. We do that by selecting genes based on patterns (e.g. ^MT- matches with all gene names starting with MT, i.e. mitochondrial genes):\n\n# mitochondrial genes\nadata.var[\"mt\"] = adata.var_names.str.startswith(\"MT-\")\n\n# ribosomal genes\nadata.var[\"ribo\"] = adata.var_names.str.startswith((\"RPS\", \"RPL\"))\n\n# hemoglobin genes.\nadata.var[\"hb\"] = adata.var_names.str.contains((\"^HB[^(P)]\"))\n\n\nsc.pp.calculate_qc_metrics(adata, qc_vars=[\"mt\", \"ribo\", \"hb\"], inplace=True, percent_top=[20]) # this step can be a little slow to run\n\nExercise 5: Run the commands and check out the metadata data frame at adata.obs. What has changed?\n\nClick for Answer\n\n\nAnswer: If we type adata.obs, a lot more metadata is present compared to before! This should include the following columns:\nThe output should be:\n    pct_counts_mt\n    total_counts_mt\n    pct_counts_ribo\n    total_counts_ribo\n    pct_counts_hb\n    total_counts_hb"
  },
  {
    "objectID": "ipynb/day1-2_analysis_tools_qc.html#plotting-experiment-metadata",
    "href": "ipynb/day1-2_analysis_tools_qc.html#plotting-experiment-metadata",
    "title": "Analysis tools and quality control (QC)",
    "section": "Plotting experiment metadata",
    "text": "Plotting experiment metadata\nExercise 6: Using scanpy’s sc.pl.violin function, create a violin plot of the percent of reads corresponding to mitochondrial, ribosomal, and hemoglobin genes per cell. Choose an upper boundary to filter out poor quality cells with high mitochondrial reads. Note that we might want to view the results as a separate violin plot for each of our six samples. To do this, please use the optional groupby=\"sample\" argument.\nPlease note that depending on your experimental setup, it might not make sense to filter on all these criteria.\n\n# Violin plots for all samples together\nsc.pl.violin(adata, [\"pct_counts_mt\", \"pct_counts_ribo\", \"pct_counts_hb\"])\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_anndata.py:839: FutureWarning: \n\nThe `scale` parameter has been renamed and will be removed in v0.15.0. Pass `density_norm='width'` for the same effect.\n  ax = sns.violinplot(\n\n\n\n\n\n\n\n\n\n\n# Violin plots for each sample separately\nsc.pl.violin(adata, [\"pct_counts_mt\", \"pct_counts_ribo\", \"pct_counts_hb\"], groupby=\"sample\")\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_anndata.py:839: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  ax = sns.violinplot(\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_anndata.py:839: FutureWarning: \n\nThe `scale` parameter has been renamed and will be removed in v0.15.0. Pass `density_norm='width'` for the same effect.\n  ax = sns.violinplot(\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_anndata.py:839: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  ax = sns.violinplot(\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_anndata.py:839: FutureWarning: \n\nThe `scale` parameter has been renamed and will be removed in v0.15.0. Pass `density_norm='width'` for the same effect.\n  ax = sns.violinplot(\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_anndata.py:839: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  ax = sns.violinplot(\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_anndata.py:839: FutureWarning: \n\nThe `scale` parameter has been renamed and will be removed in v0.15.0. Pass `density_norm='width'` for the same effect.\n  ax = sns.violinplot(\n\n\n\n\n\n\n\n\n\nYou can see that PBMMC-2 is quite different from the two others, it has a group of cells with very low ribosomal counts and one with very high globin counts. Maybe these two percentages are negatively correlated? Let’s have a look, by plotting the two percentages against each other:\n\nsc.pl.scatter(adata, x=\"pct_counts_hb\", y=\"pct_counts_ribo\", color='sample')\n\n\n\n\n\n\n\n\nExercise 7: Are they correlated? What kind of cells might have a high abundance of hemoglobin transcripts and low ribosomal transcripts?\n\nClick for Answer\n\n\nAnswer: Yes there is a negative correlation. Erythrocytes (red blood cells) have a high abundance of hemoglobin transcripts and low abundance of ribosomal transcripts. These are most likely erythroid cells, i.e. the precursor cells for erythrocytes in the bone marrow."
  },
  {
    "objectID": "ipynb/day1-2_analysis_tools_qc.html#plotting-genes-express-as-a-function-of-total-counts",
    "href": "ipynb/day1-2_analysis_tools_qc.html#plotting-genes-express-as-a-function-of-total-counts",
    "title": "Analysis tools and quality control (QC)",
    "section": "Plotting genes express as a function of total counts",
    "text": "Plotting genes express as a function of total counts\nWe can also evaluate the relative expression of other genes in our dataset, for example, the ones that are most highly expressed. Some very highly expressed genes might point to a technical cause, and we might consider to remove them. Below you will find a simple function to generate a boxplot of relative counts per gene per cell:\n\nsc.pl.highest_expr_genes(adata, n_top=20)"
  },
  {
    "objectID": "ipynb/day1-2_analysis_tools_qc.html#doublet-detection",
    "href": "ipynb/day1-2_analysis_tools_qc.html#doublet-detection",
    "title": "Analysis tools and quality control (QC)",
    "section": "Doublet detection",
    "text": "Doublet detection\nThere are several tools for identifying doublets (i.e. two cells that were encapsulated with the same gel bead, obtaining the same cell barcode). Recently a benchmarking study was conducted comparing approaches: https://www.sciencedirect.com/science/article/pii/S2405471220304592\nHere, we suggest you use DoubletDetection: https://doubletdetection.readthedocs.io/en/latest/tutorial.html\nIf you want to compare doublet detection methods, another method is scrublet: https://www.cell.com/cell-systems/pdfExtended/S2405-4712(18)30474-5\nA tutorial with scanpy is described below: https://github.com/swolock/scrublet\n\nimport doubletdetection\n\n\nclf = doubletdetection.BoostClassifier()\n\n\n# raw_counts is a cells by genes count matrix\nlabels = clf.fit(adata.X).predict()\n\n# higher means more likely to be doublet\nscores = clf.doublet_score()\n\n\n\n\nExercise 8: Run the above steps. The variable labels will store the output of the doublet detection: if labels[i]==0, the cell at that position is not a doublet, whereas if labels[i]==1, then the cell at that position is a doublet. How many doublets are predicted? Can you assign this labels metadata to your adata object with adata.obs[\"is_doublet\"] and then use value_counts() to see the number of doublets?\n\nadata.obs[\"is_doublet\"] = labels\n\n\nadata.obs[\"is_doublet\"].value_counts()\n\nis_doublet\n0.0    6879\n1.0      59\nName: count, dtype: int64\n\n\nFinally, let’s filter out the doublet cells!\n\nadata = adata[adata.obs[\"is_doublet\"]==0].copy()\n\n\nadata # is now populated with some more metadata from the QC steps above\n\nAnnData object with n_obs × n_vars = 6879 × 33694\n    obs: 'sample', 'n_counts', 'n_genes', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_20_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'log1p_total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'log1p_total_counts_hb', 'pct_counts_hb', 'is_doublet'\n    var: 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts'\n    uns: 'sample_colors'"
  },
  {
    "objectID": "ipynb/day1-2_analysis_tools_qc.html#cell-filtering",
    "href": "ipynb/day1-2_analysis_tools_qc.html#cell-filtering",
    "title": "Analysis tools and quality control (QC)",
    "section": "Cell Filtering",
    "text": "Cell Filtering\nExercise 9: Once you have selected your QC filtering criteria, you need to actually do the filtering! You can do this either (1) using the QC thresholds you selected above or (2) obtaining automated thresholds using scanpy quality control metrics. Whether you choose the (1) or (2) approach is up to you. Unfortunately, there is not much automation in the quality control steps at this stage, although there are ongoing efforts by the single cell community to create a more unbiased QC approaches.\nUse the following scanpy quality control checks to filter out poor quality cells based either (1) your semi-subjective criteria OR (2) the somewhat automated approach offered described here: https://www.sc-best-practices.org/preprocessing_visualization/quality_control.html\nHint: you can also filter an AnnData object using indexing approaches, as with numpy arrays and pandas data frames. For instance, the follow command filters genes (columns) on a qc metric for percent mitochondrial reads: adata = adata[adata.obs['percent_mito'] &lt; 0.08, :].copy()\n\nsc.pp.filter_cells(adata, min_counts=2000) # apply threshold from above to actually do the filtering\nsc.pp.filter_cells(adata, max_counts=12500) # apply threshold from above to actually do the filtering\n\n\nsc.pp.filter_cells(adata, min_genes=200) # apply threshold from above to actually do the filtering\nsc.pp.filter_cells(adata, max_genes=5000) # apply threshold from above to actually do the filtering\n\n\nadata = adata[adata.obs['pct_counts_mt'] &lt;= 8, :].copy() # apply threshold from above to actually do the filtering\n\n\nadata\n\nAnnData object with n_obs × n_vars = 5469 × 33694\n    obs: 'sample', 'n_counts', 'n_genes', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_20_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'log1p_total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'log1p_total_counts_hb', 'pct_counts_hb', 'is_doublet'\n    var: 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts'\n    uns: 'sample_colors'\n\n\nExercise 10: We have been discussing cell filtering, but you may also want to filter out genes that are not detected in your data! For this, you can use the function sc.pp.filter_genes. Try doing this to filter out genes expressed in fewer than 1% of your total cells. How many genes are removed (you can check the value of adata.n_vars before and after filtering with sc.pp.filter_genes.\n\nn_cells = adata.n_obs\nsc.pp.filter_genes(adata, min_cells=int(n_cells*0.01)) # specify min cells equal to 1% of your total cell count\n\n\nadata\n\nAnnData object with n_obs × n_vars = 5469 × 10839\n    obs: 'sample', 'n_counts', 'n_genes', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_20_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'log1p_total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'log1p_total_counts_hb', 'pct_counts_hb', 'is_doublet'\n    var: 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts', 'n_cells'\n    uns: 'sample_colors'\n\n\nExercise 11: You have finished this set of exercises! One important final step: in case you want to save your results at any time, you can use the command adata.write_h5ad() to save your AnnData object for later use. Try doing this here, so that you can load the adata object into the next Jupyter notebook tutorial on Normalization and Scaling.\n\nadata.write_h5ad(\"PBMC_analysis_SIB_tutorial.h5ad\") # feel free to choose whichever file name you prefer!"
  },
  {
    "objectID": "ipynb/day1-1_cellranger.html",
    "href": "ipynb/day1-1_cellranger.html",
    "title": "Cell Ranger",
    "section": "",
    "text": "Presentation: General introduction\nPresentation: Introduction to scRNA-seq"
  },
  {
    "objectID": "ipynb/day1-1_cellranger.html#learning-outcomes",
    "href": "ipynb/day1-1_cellranger.html#learning-outcomes",
    "title": "Cell Ranger",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nAfter having completed this chapter you will be able to:\n\nExplain what kind of information single-cell RNA-seq (scRNA-seq) can give you to answer a biological question\nDescribe essential considerations during the design of a single-cell RNA-seq experiment\nDescribe the pros and cons of different single-cell sequencing methods\nUse cellranger to:\n\nTo align reads and generate count tables\nPerform basic QC on alignments and counts"
  },
  {
    "objectID": "ipynb/day1-1_cellranger.html#running-cellranger-count",
    "href": "ipynb/day1-1_cellranger.html#running-cellranger-count",
    "title": "Cell Ranger",
    "section": "Running cellranger count",
    "text": "Running cellranger count\nThe exercises below assume that you are enrolled in the course, and have access to the server. These exercises are not essential to run for the rest of the course, so you can skip them. If you want to do them anyway, you can try to install cellranger locally (only on Linux or WSL). In addition, you will need to download the references. You can get it by with this code (choose your OS):"
  },
  {
    "objectID": "ipynb/day1-1_cellranger.html#linuxmacoswsl",
    "href": "ipynb/day1-1_cellranger.html#linuxmacoswsl",
    "title": "Cell Ranger",
    "section": "Linux/MacOS/WSL",
    "text": "Linux/MacOS/WSL\nRun the following commands in the terminal or other command line prompt application:\n    wget https://single-cell-transcriptomics.s3.eu-central-1.amazonaws.com/cellranger_index.tar.gz\n    tar -xvf cellranger_index.tar.gz\n    rm cellranger_index.tar.gz"
  },
  {
    "objectID": "ipynb/day1-1_cellranger.html#windows",
    "href": "ipynb/day1-1_cellranger.html#windows",
    "title": "Cell Ranger",
    "section": "Windows",
    "text": "Windows\nDownload using the link, and unpack in your working directory. This will download and extract the index in the current directory. Specify the path to this reference in the exercises accordingly."
  },
  {
    "objectID": "ipynb/day1-1_cellranger.html#data-overview",
    "href": "ipynb/day1-1_cellranger.html#data-overview",
    "title": "Cell Ranger",
    "section": "Data overview",
    "text": "Data overview\nHave a look in the directory course_data/reads and /data/cellranger_index. In the reads directory you will find reads on one sample: ETV6-RUNX1_1. In the analysis part of the course we will work with six samples, but due to time and computational limitations we will run cellranger count on one of the samples, and only reads originating from chromsome 21 and 22.\nThe input you need to run cellranger count are the sequence reads and a reference. Here, we have prepared a reference only with chromosome 21 and 22, but in ‘real life’ you would of course get the full reference genome of your species. The reference has a specific format. You can download precomputed human and mouse references from the 10X website. If your species of interest is not one of those, you will have to generate it yourself. For that, have a look here.\nTo be able to run cellranger in the compute environment, first run:\nexport PATH=/data/cellranger-8.0.0:$PATH\nHave a look at the documentation of cellranger count (scroll down to Command-line argument reference).\nYou can find the input files here:\n\nreads: single_cell_course/course_data/reads/ (from the downloaded tar package in your home directory)\npre-indexed reference: /data/cellranger_index\n\nExercise 1: Fill out the missing arguments (at FIXME) in the script below, and run it:\ncellranger count \\\n--id=FIXME \\\n--sample=FIXME \\\n--transcriptome=FIXME \\\n--fastqs=FIXME \\\n--localcores=4 \\\n--create-bam=true\n\nThis will take a while…\nOnce started, the process will need approximately 15 minutes to finish. Have a coffee and/or have a look at the other exercises.\n\nClick for Answer\n\n\n    cellranger count \\\n    --id=ETV6-RUNX1_1 \\\n    --sample=ETV6-RUNX1_1 \\\n    --transcriptome=/data/cellranger_index \\\n    --fastqs=single_cell_course/course_data/reads \\\n    --localcores=4 \\\n    --create-bam=true"
  },
  {
    "objectID": "ipynb/day1-1_cellranger.html#opening-cellranger-output",
    "href": "ipynb/day1-1_cellranger.html#opening-cellranger-output",
    "title": "Cell Ranger",
    "section": "Opening cellranger output",
    "text": "Opening cellranger output\nHave a look out the output directory (i.e. ~/ETV6-RUNX1_1/outs). The analysis report (web_summary.html) is usually a good place to start.\nExercise 2: Have a good look inside web_summary.html. Anything that draws your attention? Is this report good enough to continue the analysis?\n\nClick for Answer\n\n\nNot really. First of all there is a warning: Fraction of RNA read bases with Q-score &gt;= 30 is low. This means that there is a low base quality of the reads. A low base quality gives results in more sequencing error and therefore possibly lower performance while mapping the reads to genes. However, a Q-score of 30 still represents 99.9% accuracy.\nWhat should worry us more is the number of reads per cell (363) and the sequencing saturation (7.9%). In most cases you should aim for 30.000 - 50.000 reads per cell (depending on the application). We therefore don’t have enough reads per cell. However, as you might remember, this was a subset of reads (1 million) mapped against chromosome 21 & 22, while the original dataset contains 210,987,037 reads. You can check out the original report at course_data/count_matrices/ETV6-RUNX1_1/outs/web_summary.html.\nFor more info on sequencing saturation, have a look here."
  },
  {
    "objectID": "ipynb/day1-1_cellranger.html#understanding-the-cellranger-output",
    "href": "ipynb/day1-1_cellranger.html#understanding-the-cellranger-output",
    "title": "Cell Ranger",
    "section": "Understanding the CellRanger output",
    "text": "Understanding the CellRanger output\nThe run summary from cellranger count can be viewed by clicking “Summary” in the top left corner. The summary metrics describe sequencing quality and various characteristics of the detected cells. Similar web summaries are also output from the cellranger reanalyze and cellranger aggr pipelines.\nThis report will serve you as first-line feedback on how the experiment went. It provides an easily accessible summary to scrutinize the success of the experiment.\nIt will help answering the questions like:\n\nWhat is the quality of the run?\nHow many cells do you have?\nIs the cell count estimate credible?\nWas the sample sequenced deep enough? Where the cells intact and well?\nIs the quality of the cells uniform?\n\nNote that some of these questions will be more definitively answered during a successive (more hands-on) part of quality control (QC) process. Consider this just the beginning of the scrutiny.\nBasic QC metrics\nThe number of cells detected, the mean reads per cell, and the median genes detected per cell are prominently displayed near the top of the page.\n\nEstimated Cell number: This is determined from the number of cell barcodes with a ‘reasonable’ numbers of observations. This number is an estimate because there is no binary flag “full/empty” that tell us if a droplet had a cell inside or not. Every droplet will enter in contact with some free-floating RNA, therefore some threshold needs to be set to cell-associated barcodes vs noise from empty GEMs. However, this threshold cannot be a fixed number as it will depend on the overall quality of the experiment, size of the cells and depth of the sequences and mis-called sequences. So, this number automatically estimated from the “Barcode Rank Plot” that we will see below.\nNote: this number is estimated using the thresholds to cellranger count as a bias, if the threshold is changed the count can give different predictions, and in some cases it will be necessary to do so. For example: to account for a not-so-successful experiment with high level of free-floating mRNA in the input cell suspension or a lysis caused mixing the RT mix with the cell suspension.\nMean Read per cell: This is the mean of sequencing reads that is obtained on average to the cells. Note that this refers only to the ones counted in “Estimated number of cells” and therefore:\nEstimated_Cell_number * Mean_Read_per_cell ≠ Illumina_Reads.\nAlso note that this number DOES NOT correspond to the number of UMI per cells (the value that is actually used for the analysis).\nExercise 3: On the basis of what learned in the lectures and above, can you explain how “Mean Reads per cell” and “UMI count” are related? How is “UMI count” obtained by the pipeline? Will doubling the number of reads double the number of UMIs?\n\nClick for Answer\n\n\nThe mean reads per cell is the number of FASTQ READS containing a cell barcode assigned to a particular cell. The UMI count is the number of unique RNA MOLECULES that were sequenced assigned to a particular gene and cell. The cellranger pipeline determines the UMI count using the UMI barcode, which is different from the cell barcode. The UMI barcode is unique to each distinct RNA molecule in a cell. So, there is a possibility of multiple reads containing the same UMI barcode and being “collapsed” into a single count for the UMI count.\nDoubling the number of reads will likely increase the number of UMIs, due to the detection of new RNA molecules with increased sequencing depth, but it will not double the number of UMIs because many additional reads will be assigned to the same RNA/UMI barcode.\n\n\nDiagnostic QC metrics – Sequencing\n\n\nNumber of Reads\nTotal number of read pairs that were assigned to this library in demultiplexing.\n\n\nValid Barcodes\nFraction of reads with barcodes that match the whitelist after barcode correction.\n\n\nSequencing Saturation\nThe fraction of reads originating from an already-observed UMI. This is a function of library complexity and sequencing depth. More specifically, this is the fraction of confidently mapped, valid cell-barcode, valid UMI reads that had a non-unique (cell-barcode, UMI, gene). This metric was called “cDNA PCR Duplication” in versions of Cell Ranger prior to 1.2.\n\n\nQ30 Bases in Barcode\nFraction of cell barcode bases with Q-score &gt;= 30, excluding very low quality/no-call (Q &lt;= 2) bases from the denominator.\n\n\nQ30 Bases in RNA Read\nFraction of RNA read bases with Q-score &gt;= 30, excluding very low quality/no-call (Q &lt;= 2) bases from the denominator. This is Read 1 for the Single Cell 3’ v1 chemistry and Read 2 for the Single Cell 3’ v2 chemistry.\n\n\nQ30 Bases in Sample Index\nFraction of sample index bases with Q-score &gt;= 30, excluding very low quality/no-call (Q &lt;= 2) bases from the denominator.\n\n\nQ30 Bases in UMI\nFraction of UMI bases with Q-score &gt;= 30, excluding very low quality/no-call (Q &lt;= 2) bases from the denominator.\n\n\n\nDiagnostic QC metrics – Mapping\n\n\nReads Mapped to Genome\nFraction of reads that mapped to the genome.\n\n\nReads Mapped Confidently to Genome\nFraction of reads that mapped uniquely to the genome. If a gene mapped to exonic loci from a single gene and also to non-exonic loci, it is considered uniquely mapped to one of the exonic loci.\n\n\nReads Mapped Confidently to Intergenic Regions\nFraction of reads that mapped uniquely to an intergenic region of the genome.\n\n\nReads Mapped Confidently to Intronic Regions\nFraction of reads that mapped uniquely to an intronic region of the genome.\n\n\nReads Mapped Confidently to Exonic Regions\nFraction of reads that mapped uniquely to an exonic region of the genome.\n\n\nReads Mapped Confidently to Transcriptome\nFraction of reads that mapped to a unique gene in the transcriptome. The read must be consistent with annotated splice junctions. These reads are considered for UMI counting.\n\n\nReads Mapped Antisense to Gene\nFraction of reads confidently mapped to the transcriptome, but on the opposite strand of their annotated gene. A read is counted as antisense if it has any alignments that are consistent with an exon of a transcript but antisense to it, and has no sense alignments."
  },
  {
    "objectID": "ipynb/day1-1_cellranger.html#ranked-barcode-plot",
    "href": "ipynb/day1-1_cellranger.html#ranked-barcode-plot",
    "title": "Cell Ranger",
    "section": "Ranked Barcode Plot",
    "text": "Ranked Barcode Plot\nThe Barcode Rank Plot under the “Cells” dashboard shows the distribution of barcode counts and which barcodes were inferred to be associated with cells. It is one of the most informative QC plots, it enables one to assess sample quality and to formulate hypothesis of what might have gone wrong if the experiment was not perfectly successful.\nTo obtain this plot, reads are grouped by barcode, the number of UMI is counted, resulting in a vector of UMI count per barcode (note: one barcode - one GEM!). The counts are then sorted and the vector is displayed in rank vs counts plot:\nThe y-axis is the number of UMI counts mapped to each barcode and the x-axis is the number of barcodes below that value.\nNote that due to the high number of GEMs with at least one UMI the only way to visualize all the data is a log-log axes plot.\nHow does one interpret the plot? What to expect?\nIdeally there is a steep dropoff separating high UMI count cells from low UMI count background noise:\nA steep drop-off is indicative of good separation between the cell-associated barcodes and the barcodes associated with empty partitions.\nBarcodes can be determined to be cell-associated based on their UMI count or by their RNA profiles, therefore some regions of the graph can contain both cell-associated and background-associated barcodes.\nThe color of the graph represents the local density of barcodes that are cell-associated.\nIn fact, the cutoff is determined with a two-step procedure:\n\nIt uses a cutoff based on total UMI counts of each barcode to identify cells. This step identifies the primary mode of high RNA content cells.\nThen the algorithm uses the RNA profile of each remaining barcode to determine if it is an “empty” or a cell containing partition. This second step captures low RNA content cells, whose total UMI counts may be similar to empty GEMs."
  },
  {
    "objectID": "ipynb/day1-1_cellranger.html#saturation---is-there-a-gain-in-sequencing-more",
    "href": "ipynb/day1-1_cellranger.html#saturation---is-there-a-gain-in-sequencing-more",
    "title": "Cell Ranger",
    "section": "Saturation - is there a gain in sequencing more?",
    "text": "Saturation - is there a gain in sequencing more?\nThe sequencing saturation plot allows the user to assess the relative tradeoffs of sequencing deeper or shallower. As sequencing saturation increases, the total number of molecules detected in the library increases, but with diminishing returns as saturation is approached.\nA good rule of thumb for most cell types is that: An average of 40k reads per cell is a minimal sufficient that with 80k reads being usually an excellent depth. There is certainly gain in sequencing more but it is not cost-effective in general. So, it is important to evaluate if going deeper has a value to your scientific question."
  },
  {
    "objectID": "ipynb/day1-1_cellranger.html#analysis-view",
    "href": "ipynb/day1-1_cellranger.html#analysis-view",
    "title": "Cell Ranger",
    "section": "Analysis view",
    "text": "Analysis view\nThe automated secondary analysis results can be viewed by clicking “Analysis” in the top left corner. The secondary analysis provides the following:\nA dimensional reduction analysis which projects the cells into a 2-D space (t-SNE), including an automated clustering analysis which groups together cells that have similar expression profiles.\n\nA list of genes that are differentially expressed between the selected clusters\nPlots showing the effect of decreased sequencing depth on observed library complexity and on median genes per cell detected"
  },
  {
    "objectID": "ipynb/day1-1_cellranger.html#troubleshooting-when-things-go-bad",
    "href": "ipynb/day1-1_cellranger.html#troubleshooting-when-things-go-bad",
    "title": "Cell Ranger",
    "section": "Troubleshooting: when things go bad",
    "text": "Troubleshooting: when things go bad\n\n\nFor other situations like these two below, there is usually little you can do and you’d better contact 10X genomics support and/or the sequencing core facility at your institution\n\nExercise 4: We provide you with some web_summary.html files. Use what you have learned above to evaluate each one of the experiments and write a short evaluation of what you observe at least ~50 words per each file. When you write a short summary, imagine you are reporting to your supervisor about how the experiment went.\n\n\nExperiment 1\n\n\nExperiment 2\n\n\nExperiment 3\n\n\nExperiment 4\n\n\nExperiment 5\n\n\nExperiment 6\n\n\nExperiment 7\n\n\nExperiment 8\n\n\nExperiment 9"
  },
  {
    "objectID": "ipynb/Normalization_And_Scaling_D1B3_Controls.html",
    "href": "ipynb/Normalization_And_Scaling_D1B3_Controls.html",
    "title": "Single-cell transcriptomics with Python",
    "section": "",
    "text": "import scanpy as sc\nimport pandas as pd # for handling data frames (i.e. data tables)\nimport numpy as np # for handling numbers, arrays, and matrices\nimport matplotlib.pyplot as plt # plotting package\nimport seaborn as sns # plotting package\nExercise 0: Before we continue in this notebook with the next steps of the analysis, we need to load our results from the previous notebook using the sc.read_h5ad function and assign them to the variable name adata. Give it a try!\nadata = sc.read_h5ad(\"PBMC_analysis_SIB_tutorial.h5ad\")"
  },
  {
    "objectID": "ipynb/Normalization_And_Scaling_D1B3_Controls.html#normalization",
    "href": "ipynb/Normalization_And_Scaling_D1B3_Controls.html#normalization",
    "title": "Single-cell transcriptomics with Python",
    "section": "Normalization",
    "text": "Normalization\nEach count in a count matrix represents the successful capture, reverse transcription and sequencing of a molecule of cellular mRNA. Count depths for identical cells can differ due to the variability inherent in each of these steps. Thus, when gene expression is compared between cells based on count data, any difference may have arisen solely due to sampling effects. Normalization addresses this issue by e.g. scaling count data to obtain correct relative gene expression abundances between cells.\nA recent benchmark published by Ahlmann-Eltze and Huber (2023) compared 22 different transformations for single-cell data, which surprisingly showed that a seemingly simple shifted logarithm transformation outperformed gold-standard SCTransform (the default method in analysis package’s such as Seurat in R).\n\nShift logarithm normalization\nWe will thus use the shifted logarithm for normalizing our data, which is based on the delta method v\n\\[f(y) = \\log \\left( \\frac{y}{s} + y_0 \\right) \\]\nwith \\(y\\) being the raw counts, \\(s\\) being a so-called size factor and \\(y_0\\) describing a pseudo-count. The size factors are determined for each cell to account for variations in sampling effects and different cell sizes. The size factor for a cell \\(c\\) can be calculated by:\n\\[ s_c = \\frac{\\sum_g y_{gc}}{L} \\]\nwith \\(g\\) indexing different genes and \\(L\\) describing a target sum. There are different approaches to determine the size factors from the data. We will leverage the scanpy default in this section with \\(L\\) being the median raw count depth in the dataset. Many analysis templates use fixed values for \\(L\\), for example \\(L = 10^6\\), resulting in “counts per million” metric (CPM). For a beginner, these values may seem arbitrary, but it can lead to an overestimation of the variance. Indeed, Ahlmann-Eltze and Huber (2023) showed that log(CPM+1) performed poorly on a variety of tasks.\nThe shifted logarithm is a fast normalization technique, outperforms other methods for uncovering the latent structure of the dataset (especially when followed by principal component analysis) and works beneficial for stabilizing variance for subsequent dimensionality reduction and identification of differentially expressed genes.\nExercise 1: Perform normalization using the logarithm shift, following the above formula. Without relying on scanpy implemented function, you should be able to compute the log-normalized data, and store the resulting matrix in the object log_shifted_matrix below. We then provide code to display the distribution of the raw counts versus the normalized data (exercise courtesy of Alexandre Coudray).\n\nmedian_raw_counts = np.median(adata.obs['n_counts'])\n\nsize_factors = np.array(adata.obs['n_counts'] / median_raw_counts)\n\nlog_shifted_matrix = np.log(adata.X / size_factors[:,None] + 1) # we use a pseudo-count of 1\n\nOnce you have performed the log-normalization shift, you can visualize your results here to see how the counts at adata.X have changed:\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\np1 = sns.histplot(adata.obs[\"n_counts\"], bins=100, kde=False, ax=axes[0])\naxes[0].set_title(\"Total counts\")\np2 = sns.histplot(log_shifted_matrix.sum(1), bins=100, kde=False, ax=axes[1])\naxes[1].set_title(\"Shifted logarithm\")\nplt.show()\n\n\n\n\n\n\n\n\nThe shifted logarithm can be conveniently called with scanpy by running pp.normalized_total with target_sum=None. We then apply a log transformation with a pseudo-count of 1, which can be easily done with the function sc.pp.log1p.\n\n# This can be easily done with scanpy normalize_total and log1p functions\nscales_counts = sc.pp.normalize_total(adata, target_sum=None, inplace=False)\n# log1p transform - log the data and adds a pseudo-count of 1\nscales_counts = sc.pp.log1p(scales_counts[\"X\"], copy=True)\n\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\np1 = sns.histplot(adata.obs[\"n_counts\"], bins=100, kde=False, ax=axes[0])\naxes[0].set_title(\"Total counts\")\np2 = sns.histplot(scales_counts.sum(1), bins=100, kde=False, ax=axes[1])\naxes[1].set_title(\"Shifted logarithm\")\nplt.show()\n\n\n\n\n\n\n\n\nWe verify that our normalization is the same as the one implemented in scanpy. You should have a corelation above 0.999999. The reason why we are not reaching a corelation of 1.0 comes from the precision in float number used by scanpy.\n\nfrom scipy.stats import pearsonr\n\npearsonr(np.array(scales_counts).flatten(),\n         np.array(log_shifted_matrix).flatten())\n\nPearsonRResult(statistic=0.9999986936481091, pvalue=0.0)\n\n\nWe then run it again so that the normalized data is set as our default matrix in .X emplacement.\n\n# To directly change the data 'in place', use the following:\nsc.pp.normalize_total(adata, target_sum=None)\nsc.pp.log1p(adata)\n\nOf course, in your own analysis, you can just use the two scanpy commands in the code cell above. However, this exercise aims as giving you a better understanding of the transformation being applied to your data! Some more complex machine learning based algorithms (i.e. batch integration with scVI, coming up in a later exercise during this course) actually work more optimally on the raw, untransformed counts!"
  },
  {
    "objectID": "ipynb/Normalization_And_Scaling_D1B3_Controls.html#variable-feature-selection",
    "href": "ipynb/Normalization_And_Scaling_D1B3_Controls.html#variable-feature-selection",
    "title": "Single-cell transcriptomics with Python",
    "section": "Variable feature selection",
    "text": "Variable feature selection\nNow, we begin with dimensionality reduction, i.e. reducing the number of variables in the data by removing features (genes) with little variability among the cells and by combining highly similar features. This is important because normally you start with tens of thousands of genes and it is difficult to represent their patterns in a two-dimensional visualization.\nOne useful (but optional) step is to perform cell cycle characterization, as the cell cycle signature is often a strong convoluting factor with cell type signatures.\nExercise 2: Perform cell cycle characterization on your dataset. Plot a scatter plot of the S_score and G2M_score metadata created and stored in your AnnData object. Color the points by the assigned cell cycle phase. What percentage of your cells are in a proliferative state (S or G2M phases)?\n\n# your code here\nS_genes_mouse = np.array(['MCM5', 'PCNA', 'TYMS', 'FEN1', 'MCM2', 'MCM4', 'RRM1', 'UNG',\n       'GINS2', 'MCM6', 'CDCA7', 'DTL', 'PRIM1', 'UHRF1', 'CENPU',\n       'HELLS', 'RFC2', 'RPA2', 'NASP', 'RAD51AP1', 'GMNN', 'WDR76',\n       'SLBP', 'CCNE2', 'UBR7', 'POLD3', 'MSH2', 'ATAD2', 'RAD51', 'RRM2',\n       'CDC45', 'CDC6', 'EXO1', 'TIPIN', 'DSCC1', 'BLM', 'CASP8AP2',\n       'USP1', 'CLSPN', 'POLA1', 'CHAF1B', 'BRIP1', 'E2F8'])\nG2M_genes_mouse = np.array(['HMGB2', 'CDK1', 'NUSAP1', 'UBE2C', 'BIRC5', 'TPX2', 'TOP2A',\n       'NDC80', 'CKS2', 'NUF2', 'CKS1B', 'MKI67', 'TMPO', 'CENPF',\n       'TACC3', 'PIMREG', 'SMC4', 'CCNB2', 'CKAP2L', 'CKAP2', 'AURKB',\n       'BUB1', 'KIF11', 'ANP32E', 'TUBB4B', 'GTSE1', 'KIF20B', 'HJURP',\n       'CDCA3', 'JPT1', 'CDC20', 'TTK', 'CDC25C', 'KIF2C', 'RANGAP1',\n       'NCAPD2', 'DLGAP5', 'CDCA2', 'CDCA8', 'ECT2', 'KIF23', 'HMMR',\n       'AURKA', 'PSRC1', 'ANLN', 'LBR', 'CKAP5', 'CENPE', 'CTCF', 'NEK2',\n       'G2E3', 'GAS2L3', 'CBX5', 'CENPA'])\n\nsc.tl.score_genes_cell_cycle(adata, s_genes=S_genes_mouse, g2m_genes=G2M_genes_mouse)\n\nWARNING: genes are not in var_names and ignored: Index(['PIMREG', 'JPT1'], dtype='object')\n\n\nThe gene lists provided here are the “standard” in single cell analysis as come from the two papers: - Buettner et al (2015) - Satija et al (2015)\n\nn2c = {\"G1\":\"red\", \"S\":\"green\", \"G2M\":\"blue\"} # use to assign each cell a color based on phase in the scatter plot\nplt.scatter(adata.obs['S_score'], adata.obs['G2M_score'], c=[n2c[k] for k in adata.obs['phase']])\nplt.xlabel('S score') ; plt.ylabel('G2M score')\nplt.show()\n\n\n\n\n\n\n\n\nExercise 3: How many cells do you have assigned to each of the cell cycle phases? Can you check this using a function you have applied in the previous exercises?\nAnswer: typing adata.obs[\"phase\"].value_counts() should return:\nphase\nG1      3129\nS       1400\nG2M     940\nName: count, dtype: int64\nWe next calculate a subset of features that exhibit high cell-to-cell variation in the dataset (i.e, they are highly expressed in some cells, and lowly expressed in others). Genes that are similarly expressed in all cells will not assist with discriminating different cell types from each other.\nThe procedure in scanpy models the mean-variance relationship inherent in single-cell data, and is implemented in the sc.pp.highly_variable_genes function. By default, 2,000 genes (features) per dataset are returned and these will be used in downstream analysis, like PCA.\n\n# suggestion: start with 3000 highly variable genes\nsc.pp.highly_variable_genes(adata, n_top_genes=3000)\n\n\nadata = adata[:, adata.var[\"highly_variable\"]].copy() # actually do the filtering\n\nWhile correcting for technical covariates may be crucial to uncovering the underlying biological signal, correction for biological covariates serves to single out particular biological signals of interest. The most common biological data correction is to remove the effects of the cell cycle on the transcriptome, the number of raw counts that existed per cell, or the percentage of mitochondrial reads present. This data correction can be performed by a simple linear regression against a cell cycle score as implemented in scanpy.\nExercise 4: Use scanpy’s regress_out function to remove the effect of the cell cycle phase metadata from your downstream analyses steps.\nAnswer: sc.pp.regress_out(adata, 'phase') is the command you want to apply here\n\nsc.pp.regress_out(adata, 'phase') # specify which feature from adata.obs you want to regress out"
  },
  {
    "objectID": "ipynb/Normalization_And_Scaling_D1B3_Controls.html#scaling",
    "href": "ipynb/Normalization_And_Scaling_D1B3_Controls.html#scaling",
    "title": "Single-cell transcriptomics with Python",
    "section": "Scaling",
    "text": "Scaling\nNext, we apply scaling, a linear transformation that is a standard pre-processing step prior to dimensional reduction techniques like PCA. The sc.pp.scale function:\n\nshifts the expression of each gene, so that the mean expression across cells is 0\nscales the expression of each gene, so that the variance across cells is 1\nThis step gives equal weight in downstream analyses, so that highly-expressed genes do not dominate. The results of this are stored as the updated count matrix at adata.X, and the original means and standard deviations for each gene are stored as metadata variables in adata.var[\"mean\"] and adata.var[\"std\"].\n\n\nsc.pp.scale(adata)\n\nExercise 5: Can you use the commands adata.X.mean() to check whether this method is successfully scaling the mean of each gene to be equal to 0, and adata.X.std() to check whether this method is successfully scaling the standard deviation of each gene to be equal to 1? Important: don’t forget to take the mean for each gene by specifying axis=0!\nAnswer: adata.X.mean(axis=0) should return:\narray([ 9.97964231e-17,  2.44618185e-17,  3.11863822e-17, ...,\n    4.98982115e-17, -1.55931911e-17, -7.17286791e-17])\nIn other words, values extremely close to zero.\nadata.X.std(axis=0) should return:\narray([0.99997257, 0.99997257, 0.99997257, ..., 0.99997257, 0.99997257,\n   0.99997257])\nIn other words, values extremely close to one.\n\n# Save the dataset\n\n\nadata.write_h5ad(\"PBMC_analysis_SIB_tutorial2.h5ad\") # feel free to choose whichever file name you prefer!"
  },
  {
    "objectID": "ipynb/Dimensionality_Reduction_Embedding_D2B1_Controls.html",
    "href": "ipynb/Dimensionality_Reduction_Embedding_D2B1_Controls.html",
    "title": "Single-cell transcriptomics with Python",
    "section": "",
    "text": "import scanpy as sc\nimport pandas as pd # for handling data frames (i.e. data tables)\nimport numpy as np # for handling numbers, arrays, and matrices\nimport matplotlib.pyplot as plt # plotting package\nimport seaborn as sns # plotting package\nExercise 0: Before we continue in this notebook with the next steps of the analysis, we need to load our results from the previous notebook using the sc.read_h5ad function and assign them to the variable name adata. Give it a try!\nadata = sc.read_h5ad(\"PBMC_analysis_SIB_tutorial2.h5ad\")"
  },
  {
    "objectID": "ipynb/Dimensionality_Reduction_Embedding_D2B1_Controls.html#principal-component-analysis",
    "href": "ipynb/Dimensionality_Reduction_Embedding_D2B1_Controls.html#principal-component-analysis",
    "title": "Single-cell transcriptomics with Python",
    "section": "Principal component analysis",
    "text": "Principal component analysis\nDimensionality reduction methods seek to take a large set of variables and return a smaller set of components that still contain most of the information in the original dataset. One of the simplest forms of dimensionality reduction is PCA. Principal component analysis (PCA) is a mathematical procedure that transforms a number of possibly correlated (e.g., expression of genes in a network) variables into a (smaller) number of uncorrelated variables called principal components (“PCs”).\nExercise 1: Run PCA analysis using sc.pp.pca in scanpy. Plot a scatter plot of the first principal components and try to identify how many components are needed to explain 50% of the variance in the data.\n\nsc.pp.pca(adata, svd_solver=\"arpack\", n_comps=100)\n\nWe can then visualize the first two components of the PCA as a scatter plot using ss.pl.pca. The first two components capture the largest axes of variability in the data.\n\nsc.pl.pca_scatter(adata, color=\"sample\")\n\n\n\n\n\n\n\n\nExercise 2: We can color the PCA plot according to any factor that is present in adata.obs, or for any gene’s expression. Can you color by the column n_counts and phase? What about by the genes HBA1 (an alpha subunit of hemoglobin) IGKC (one of the most highly variable genes)?\n\n# visualize the first PCs, color by cell cycle phase and n_counts (unnormalized)\nsc.pl.pca_scatter(adata, color=[\"n_counts\", \"phase\"])\n\n\n\n\n\n\n\n\nEach principal component scores the contribution of each gene to that component. Therefore, we can see which genes are more highly correlated to one component compared to the others.\n\nsc.pl.pca_loadings(adata, components = '1,2,3')\n\n\n\n\n\n\n\n\nSince PCA is a geometric form of dimensionality reduction, we can visualize the specific genes that most strongly contribute to each principal component. This allows us to get an idea of which components discriminate which cells from each other.\n\ndef pca_heatmap(adata, component, groupby, use_raw=False, layer=None):\n    attr = 'varm'\n    keys = 'PCs'\n    scores = getattr(adata, attr)[keys][:, component]\n    dd = pd.DataFrame(scores, index=adata.var_names)\n    var_names_pos = dd.sort_values(0, ascending=False).index[:20]\n\n    var_names_neg = dd.sort_values(0, ascending=True).index[:20]\n\n    pd2 = pd.DataFrame(adata.obsm['X_pca'][:, component], index=adata.obs.index)\n\n    bottom_cells = pd2.sort_values(0).index[:300].tolist()\n    top_cells = pd2.sort_values(0, ascending=False).index[:300].tolist()\n\n    sc.pl.heatmap(adata[top_cells+bottom_cells], list(var_names_pos) + list(var_names_neg), \n                        show_gene_labels=False, groupby=groupby,\n                        swap_axes=True, cmap='viridis', \n                        use_raw=use_raw, layer=layer, figsize=(6,4))\n\nExercise 3: Use the pca_heatmap function defined above to plot the 0th and 1st components. Group the top/bottom cells by their sample identifier?\n\npca_heatmap(adata, component=0, groupby=\"sample\")\n\n\n\n\n\n\n\n\n\npca_heatmap(adata, component=1, groupby=\"sample\")\n\n\n\n\n\n\n\n\nExercise 4: Do you see anything strange about these results? Is there something undesirable about the way in which the PCs discriminate cell populations that might not be biological?\nFor further dimensionality reduction, we need to select a number of PCs to use (the rest are excluded). Ideally, we want to capture as much data variance as possible in as few PCs as possible. The plot generated with pca_variance_ratio can help you in determining how many PCs to use for downstream analysis such as UMAP:\n\nsc.pl.pca_variance_ratio(adata, log=False) # see contribution of each PC to variance\n\n\n\n\n\n\n\n\nThis plot ranks principle components based on the percentage of variance explained by each one. Where we observe an “elbow” or flattening curve, the majority of true signal is captured by this number of PCs, eg around 20 PCs for the adata dataset.\nIncluding too many PCs usually does not affect much the result, while including too few PCs can affect the results very much\nAnother - maybe more intuitive - way to see it, is to use a cumulative sum to show the % unexplained variance in function of the number of PCs. If we look at the first PCs, we can see that we passed the elbow with 30 to 50 PCs, which should ensure that we capture the major part of the variability in our dataset to be shown on a 2D representation like UMAP or tSNE.\n\nplt.plot(100 - (np.cumsum(adata.uns[\"pca\"][\"variance_ratio\"])*100)/sum(adata.uns[\"pca\"][\"variance_ratio\"]))\nplt.ylabel('% unexplained variance') ; plt.xlabel('N PCs')\n\nText(0.5, 0, 'N PCs')"
  },
  {
    "objectID": "ipynb/Dimensionality_Reduction_Embedding_D2B1_Controls.html#dimensionality-reduction",
    "href": "ipynb/Dimensionality_Reduction_Embedding_D2B1_Controls.html#dimensionality-reduction",
    "title": "Single-cell transcriptomics with Python",
    "section": "Dimensionality reduction",
    "text": "Dimensionality reduction\nExercise 5: Compute the neighborhood graph of cells using the PCA representation of the data matrix. The purpose of this step is to understand the “distance” between individual cells in the lower-dimensional PCA space, important for creating 2D scatter plot representations of your data. The number of neighbors used will influence how much the data is smoothened, which is a necessary step due to the sparsity (missing values) widely present in scRNA-seq data compared to bulk methods.\n\nsc.pp.neighbors(adata, n_pcs = 30) # specify the number of neighbors and number of PCs you wish to use\n\n/home/alex/anaconda3/envs/sib_course_2024/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\nEmbedding the graph in a 2D representation can be performed using either tSNE or UMAP algorithms.\nExercise 6: Run UMAP algorithm on your data after completing the previous steps with default parameters. We will evaluate the quality of each approach in later exercises.\n\nsc.tl.umap(adata)\n\nTo view the UMAP plot:\n\nsc.pl.umap(adata, color=[\"sample\"])\n\n\n\n\n\n\n\n\nExercise 7: Try to change:\nA. Color the dots in the UMAP according to a variable (e.g. n_counts or HBA1). Any idea where the erythrocytes probably are in the UMAP?\nB. The number of neighbors used for the calculation of the UMAP. Which is the parameter to change and how did it affect the output. What is the default? In which situation would you lower/increase this?\nC. The number of principal components (n_pcs) to extremely few (5) or many (50). How does this it affect the output? In your opinion, it is better with fewer or more PCs? Why does n_pcs=150 not work? When would more precision be needed?\n\nsc.pl.umap(adata, color=[\"n_counts\", \"HBA1\"])\n\n\n\n\n\n\n\n\nSave your results!\n\nadata.write_h5ad(\"PBMC_analysis_SIB_tutorial3.h5ad\")"
  },
  {
    "objectID": "precourse_preparations.html",
    "href": "precourse_preparations.html",
    "title": "Pre-course preparations",
    "section": "",
    "text": "Participants should already have a basic knowledge in Next Generation Sequencing (NGS) techniques, or have already followed the course NGS - Quality control, Alignment, Visualisation. Knowledge in RNA sequencing is a plus. A basic knowledge of the Python programming language is required. Test your Python skills with the quiz here, before registering."
  },
  {
    "objectID": "precourse_preparations.html#required-competences",
    "href": "precourse_preparations.html#required-competences",
    "title": "Pre-course preparations",
    "section": "",
    "text": "Participants should already have a basic knowledge in Next Generation Sequencing (NGS) techniques, or have already followed the course NGS - Quality control, Alignment, Visualisation. Knowledge in RNA sequencing is a plus. A basic knowledge of the Python programming language is required. Test your Python skills with the quiz here, before registering."
  },
  {
    "objectID": "precourse_preparations.html#software",
    "href": "precourse_preparations.html#software",
    "title": "Pre-course preparations",
    "section": "Software",
    "text": "Software\nAttendees should have a Wi-Fi enabled computer. An online Python and Jupyter Notebook environment will be provided. However, in case you wish to perform the practical exercises on your own computer, please take a moment to install the following before the course:\n\nPython version &gt;= 3.8 installed through Anaconda.\nJupyter Notebook or the Anaconda Navigator\nThe Python packages necessary for the course. Find the script to install them here."
  },
  {
    "objectID": "course_schedule.html",
    "href": "course_schedule.html",
    "title": "Course schedule",
    "section": "",
    "text": "Note\n\n\n\nOther than the starting time, the time schedule is an approximation. Because we cannot plan a course by the minute, in practice the time points will deviate."
  },
  {
    "objectID": "course_schedule.html#day-1",
    "href": "course_schedule.html#day-1",
    "title": "Course schedule",
    "section": "Day 1",
    "text": "Day 1\n\n\n\nblock\nstart\nend\nsubject\n\n\n\n\nintroduction\n9:15 AM\n9:30 AM\nIntroduction\n\n\nblock 1\n9:30 AM\n10:15 AM\nIntroduction scRNAseq\n\n\n\n10:15 AM\n10:45 AM\nBREAK\n\n\nblock 2\n10:45 AM\n12:00 PM\nPresentation: Andreia Gouvêa, 10X Field Application Scientist\n\n\n\n12:00 PM\n1:00 PM\nBREAK\n\n\nblock 3\n1:00 PM\n3:00 PM\nCell Ranger and Quality Control\n\n\n\n3:00 PM\n3:30 PM\nBREAK\n\n\nblock 4\n3:30 PM\n5:00 PM\nNormalization and Scaling"
  },
  {
    "objectID": "course_schedule.html#day-2",
    "href": "course_schedule.html#day-2",
    "title": "Course schedule",
    "section": "Day 2",
    "text": "Day 2\n\n\n\nblock\nstart\nend\nsubject\n\n\n\n\nblock 1\n9:15 AM\n10:30 AM\nDimensionality Reduction and Integration\n\n\n\n10:30 AM\n11:00 AM\nBREAK\n\n\nblock 2\n11:00 AM\n12:30 PM\nClustering\n\n\n\n12:30 PM\n1:30 PM\nBREAK\n\n\nblock 3\n1:30 PM\n3:00 PM\nCell annotation\n\n\n\n3:00 PM\n3:30 PM\nBREAK\n\n\nblock 4\n3:30 PM\n5:00 PM\nVisualization & further exercises"
  },
  {
    "objectID": "course_schedule.html#day-3",
    "href": "course_schedule.html#day-3",
    "title": "Course schedule",
    "section": "Day 3",
    "text": "Day 3\n\n\n\nblock\nstart\nend\nsubject\n\n\n\n\nblock 1\n9:15 AM\n10:30 AM\nTranscription factor analysis\n\n\n\n10:30 AM\n11:00 AM\nBREAK\n\n\nblock 2\n11:00 AM\n12:30 PM\nTrajectory Inference\n\n\n\n12:30 PM\n1:30 PM\nBREAK\n\n\nblock 3\n1:30 PM\n3:00 PM\nRNA velocity\n\n\n\n3:00 PM\n3:30 PM\nBREAK\n\n\nblock 4\n3:30 PM\n5:00 PM\nRNA velocity"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Single Cell Transcriptomics with Python",
    "section": "",
    "text": "Alex Russell Lederer ORCiD\nGeert van Geest ORCiD\nTania Wyss ORCiD"
  },
  {
    "objectID": "index.html#teachers",
    "href": "index.html#teachers",
    "title": "Single Cell Transcriptomics with Python",
    "section": "",
    "text": "Alex Russell Lederer ORCiD\nGeert van Geest ORCiD\nTania Wyss ORCiD"
  },
  {
    "objectID": "index.html#attribution",
    "href": "index.html#attribution",
    "title": "Single Cell Transcriptomics with Python",
    "section": "Attribution",
    "text": "Attribution\nParts of this course are inspired by the Single Cell Best Practices Guide, previous R courses from the SIB on Single Cell Transcriptomics, and from the BC2 Conference 2023 Workshop. These previous course materials were prepared by Tania Wass, Rachel Marcone-Jeitziner, Geert van Geert, Patricia Palagi, Alex Lederer, and Alexandre Coudray.\nThe current course material here is still under active development, for questions please contact Alex Lederer"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Single Cell Transcriptomics with Python",
    "section": "Overview",
    "text": "Overview\nSingle-cell RNA sequencing (scRNA-seq) can measure the gene expression of complex biological systems at the level of individual cells, enabling scientists to generate detailed tissue atlases describing the transcriptomic profiles of thousands or even millions of cells. While scRNA-seq has become a popular technique in diverse fields of biological research, the required expertise for handling such datasets has restricted its use among the larger scientific community. The aim of this 3-day course is to empower researchers to start applying the fundamental scRNA-seq analysis pipeline to their own data. We will outline how to design and interpret results of a scRNA-seq dataset and explore the basics of preprocessing and analysis in Python on real data. We will discuss common concerns in the field, including preprocessing choices, dimensionality reduction, cell type clustering and identification, batch effect correction, pseudotime, and RNA velocity methods. The course will be taught in Python."
  },
  {
    "objectID": "index.html#license-copyright",
    "href": "index.html#license-copyright",
    "title": "Single Cell Transcriptomics with Python",
    "section": "License & copyright",
    "text": "License & copyright\nLicense: CC BY-SA 4.0\nCopyright: SIB Swiss Institute of Bioinformatics"
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "Single Cell Transcriptomics with Python",
    "section": "Learning outcomes",
    "text": "Learning outcomes\n\nGeneral learning outcomes\nBy the end of the course, participants will be able to:\n\nRun Cell Ranger\nEvaluate the quality of a scRNA-seq experiment\nPerform scanpy analysis on their own data\nConfidently communicate about how to overcome potential bottlenecks\n\n\n\nLearning outcomes explained\nTo reach the general learning outcomes above, we have set a number of smaller learning outcomes. Each chapter starts with these smaller learning outcomes. Use these at the start of a chapter to get an idea what you will learn. Use them also at the end of a chapter to evaluate whether you have learned what you were expected to learn."
  },
  {
    "objectID": "index.html#learning-experiences",
    "href": "index.html#learning-experiences",
    "title": "Single Cell Transcriptomics with Python",
    "section": "Learning experiences",
    "text": "Learning experiences\nTo reach the learning outcomes we will use lectures, exercises, polls, and group work. During exercises, you are free to discuss with other participants. During lectures, focus on the lecture only.\n\nExercises\nEach block has practical work involved. Some more than others. The practicals are subdivided into chapters, and we’ll have a (short) discussion after each chapter. All answers to the practicals are incorporated, but they are hidden. Do the exercise first by yourself, before checking out the answer. If your answer is different from the answer in the practicals, try to figure out why they are different."
  },
  {
    "objectID": "ipynb/Clustering_Marker_Genes_D2B2_Controls.html",
    "href": "ipynb/Clustering_Marker_Genes_D2B2_Controls.html",
    "title": "Clustering and cell annotation",
    "section": "",
    "text": "import scanpy as sc\nimport pandas as pd # for handling data frames (i.e. data tables)\nimport numpy as np # for handling numbers, arrays, and matrices\nimport matplotlib.pyplot as plt # plotting package\nimport seaborn as sns # plotting package\n\nExercise 0: Before we continue in this notebook with the next steps of the analysis, we need to load our results from the previous notebook using the sc.read_h5ad function and assign them to the variable name adata. Give it a try!\n\nadata = sc.read_h5ad(\"PBMC_analysis_SIB_tutorial4.h5ad\")\n\n\nadata\n\nAnnData object with n_obs × n_vars = 5469 × 3000\n    obs: 'sample', 'n_counts', 'n_genes', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_20_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'log1p_total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'log1p_total_counts_hb', 'pct_counts_hb', 'is_doublet', 'S_score', 'G2M_score', 'phase'\n    var: 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'mean', 'std'\n    uns: 'hvg', 'log1p', 'neighbors', 'pca', 'phase_colors', 'sample_colors', 'umap'\n    obsm: 'X_pca', 'X_pcahm', 'X_umap'\n    varm: 'PCs'\n    obsp: 'connectivities', 'distances'\n\n\nClustering the data helps to identify cells with similar gene expression properties that may belong to the same cell type or cell state. There are two popular clustering methods, both available in scanpy: Leiden clustering.\nExercise 1: Run Leiden clustering algorithm. Visualize the clusters on your UMAP representation. Are the clusters different from each method? Visualize the clusters again, this time on the tSNE embedding instead of the UMAP embedding. Are there differences in which clusters are grouped together?\n\nsc.tl.leiden(adata)\n\n/tmp/ipykernel_22873/3467744858.py:1: FutureWarning: In the future, the default backend for leiden will be igraph instead of leidenalg.\n\n To achieve the future defaults please pass: flavor=\"igraph\" and n_iterations=2.  directed must also be False to work with igraph's implementation.\n  sc.tl.leiden(adata)\n\n\nNext, you can visualize your UMAP and tSNE representations of the scRNA-seq and color by various metadata attributes (including Louvian or Leiden clusters) from the prior steps. For example:\n\nsc.pl.umap(adata, color=\"leiden\",legend_loc=\"on data\")\n\n\n\n\n\n\n\n\nExercise 2: How many cells do you have per cluster? Can you plot a histogram of this?\n\nn_clusters = adata.obs[\"leiden\"].value_counts()\nax = n_clusters.plot(kind=\"bar\")\nplt.show()\n\n\n\n\n\n\n\n\nExercise 3: Visualize some of the other metadata on the UMAP embedding, including the n_counts, sample, n_genes, pct_counts_mt, and phase metadata found in adata.obs.\nDo any clusters seem to have an obvious bias towards particular attributes? This might be a sign that we want to optimize prior steps of the analysis, such as adjusting the number of principal components used in the neighborhood smoothing or regressing out particular variables. As with a pandas dataframe, you can also examine the frequency of various attributes using a command such as: adata.obs[\"phase\"].value_counts().\n\nsc.pl.umap(adata, color=['n_counts', 'sample'])\n\n\n\n\n\n\n\n\n\nsc.pl.umap(adata, color=['n_genes', 'pct_counts_mt', 'phase'])\n\n\n\n\n\n\n\n\nExercise 4: Let’s proceed with Leiden clustering and UMAP embeddings for the time being.\n\nCreate a new metadata attribute for your current clusters, i.e. adata.obs[\"leiden_res1\"] = adata.obs[\"leiden\"].\nRepeat leiden clustering using different values for the resolution parameter: 0.1, 0.5, 2.0.\nSave the clusters in a new metadata column and visualize them on the UMAP representation.\nHow does the number of clusters change with adjustments to the resolution parameter? Using the resolution=1 as a basis, do any clusters divide into two smaller clusters upon changing the resolution parameter? Do any clusters merge together? Can you plot the three different clustering results side-by-side on the UMAP to compare?\n\n\nadata.obs[\"leiden_res1\"] = adata.obs[\"leiden\"]\n\n\nsc.tl.leiden(adata, key_added=\"leiden_res0_1\", resolution=0.1)\nsc.tl.leiden(adata, key_added=\"leiden_res0_5\", resolution=0.5)\nsc.tl.leiden(adata, key_added=\"leiden_res2\", resolution=2)\n\n\nsc.pl.umap(\n    adata,\n    color=[\"leiden_res0_1\", \"leiden_res0_5\", \"leiden_res1\", \"leiden_res2\"],\n    legend_loc=\"on data\",\n    ncols=2,\n)\n\n\n\n\n\n\n\n\nOPTIONAL Exercise 5: Let’s take a few steps back to understand the previous steps a little bit better! For example, the number of principal components used in computing the neighborhood graph will greatly impact the visualizations. Rerun previous code using the following number of PCs and visualize the different UMAPs and number of clusters: 4 PCs, 8 PCs, 15 PCs, 30 PCs. What changes with the different number of PCs used? Choose an “optimal” number of PCs by examining the contribution of each PC to the total variance with the command: sc.pl.pca_variance_ratio(adata, log=True).\n\n# Apply for 4 PCs\nsc.pp.neighbors # you must complete\nsc.tl.umap # you must complete\nsc.tl.leiden # you must complete\nsc.pl.umap(adata_scvi, color=[\"leiden\"])\n\n\n# Apply for 8 PCs\n\n\n# Apply for 15 PCs\n\n\n# Apply for 30 PCs\n\n\nVisualization and cell type annotation\nBefore proceeding with marker gene analysis and cell type annotation, restore the raw version of the data, add the necessary annotations, and normalize the counts:\n\nadata_raw = sc.read_h5ad(\"PBMC_analysis_SIB_tutorial.h5ad\") # raw data before selecting highly variable genes\nadata_raw_norm = adata_raw.copy()\nsc.pp.normalize_total(adata_raw_norm, target_sum=None)\nsc.pp.log1p(adata_raw_norm)\n\n\nadata_raw_norm.obs[\"leiden\"] = adata.obs[\"leiden_res1\"]\nadata_raw_norm.obsm[\"X_pca\"] = adata.obsm[\"X_pcahm\"]\nadata_raw_norm.obsm[\"X_umap\"] = adata.obsm[\"X_umap\"]\n\nLet’s use a simple method implemented by scanpy to find marker genes by the Leiden cluster.\n\nsc.tl.rank_genes_groups(\n    adata_raw_norm, use_raw=False, groupby=\"leiden\", method=\"wilcoxon\", key_added=\"dea_leiden\"\n)\n\n/home/alex/anaconda3/envs/sib_course_2024/lib/python3.9/site-packages/scanpy/tools/_rank_genes_groups.py:429: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, \"names\"] = self.var_names[global_indices]\n/home/alex/anaconda3/envs/sib_course_2024/lib/python3.9/site-packages/scanpy/tools/_rank_genes_groups.py:431: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, \"scores\"] = scores[global_indices]\n/home/alex/anaconda3/envs/sib_course_2024/lib/python3.9/site-packages/scanpy/tools/_rank_genes_groups.py:434: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, \"pvals\"] = pvals[global_indices]\n/home/alex/anaconda3/envs/sib_course_2024/lib/python3.9/site-packages/scanpy/tools/_rank_genes_groups.py:444: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, \"pvals_adj\"] = pvals_adj[global_indices]\n/home/alex/anaconda3/envs/sib_course_2024/lib/python3.9/site-packages/scanpy/tools/_rank_genes_groups.py:455: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, \"logfoldchanges\"] = np.log2(\n/home/alex/anaconda3/envs/sib_course_2024/lib/python3.9/site-packages/scanpy/tools/_rank_genes_groups.py:429: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, \"names\"] = self.var_names[global_indices]\n/home/alex/anaconda3/envs/sib_course_2024/lib/python3.9/site-packages/scanpy/tools/_rank_genes_groups.py:431: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, \"scores\"] = scores[global_indices]\n/home/alex/anaconda3/envs/sib_course_2024/lib/python3.9/site-packages/scanpy/tools/_rank_genes_groups.py:434: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, \"pvals\"] = pvals[global_indices]\n/home/alex/anaconda3/envs/sib_course_2024/lib/python3.9/site-packages/scanpy/tools/_rank_genes_groups.py:444: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, \"pvals_adj\"] = pvals_adj[global_indices]\n/home/alex/anaconda3/envs/sib_course_2024/lib/python3.9/site-packages/scanpy/tools/_rank_genes_groups.py:455: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, \"logfoldchanges\"] = np.log2(\n/home/alex/anaconda3/envs/sib_course_2024/lib/python3.9/site-packages/scanpy/tools/_rank_genes_groups.py:429: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, \"names\"] = self.var_names[global_indices]\n/home/alex/anaconda3/envs/sib_course_2024/lib/python3.9/site-packages/scanpy/tools/_rank_genes_groups.py:431: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, \"scores\"] = scores[global_indices]\n/home/alex/anaconda3/envs/sib_course_2024/lib/python3.9/site-packages/scanpy/tools/_rank_genes_groups.py:434: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, \"pvals\"] = pvals[global_indices]\n/home/alex/anaconda3/envs/sib_course_2024/lib/python3.9/site-packages/scanpy/tools/_rank_genes_groups.py:444: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, \"pvals_adj\"] = pvals_adj[global_indices]\n/home/alex/anaconda3/envs/sib_course_2024/lib/python3.9/site-packages/scanpy/tools/_rank_genes_groups.py:455: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, \"logfoldchanges\"] = np.log2(\n\n\n\nsc.settings.set_figure_params(dpi=50, facecolor='white')\nsc.pl.rank_genes_groups_dotplot(\n    adata_raw_norm, groupby=\"leiden\", standard_scale=\"var\", n_genes=5, key=\"dea_leiden\"\n)\n\nWARNING: dendrogram data not found (using key=dendrogram_leiden). Running `sc.tl.dendrogram` with default parameters. For fine tuning it is recommended to run `sc.tl.dendrogram` independently.\n\n\n\n\n\n\n\n\n\nAs you can see above, a lot of the differentially expressed genes are highly expressed in multiple clusters. We can filter the differentially expressed genes to select for more cluster-specific differentially expressed genes:\n\nsc.tl.filter_rank_genes_groups(\n    adata_raw_norm,\n    min_in_group_fraction=0.2,\n    max_out_group_fraction=0.2,\n    key=\"dea_leiden\",\n    key_added=\"dea_leiden_filtered\",\n)\n\n\nsc.pl.rank_genes_groups_dotplot(\n    adata_raw_norm, groupby=\"leiden\", standard_scale=\"var\", n_genes=5, key=\"dea_leiden_filtered\"\n)\n\n\n\n\n\n\n\n\nExercise 6: Visualize marker genes on the UMAP or tSNE representation. Try to find 3-4 marker genes that are indeed specific to a particular cluster. Are there any clusters that do not seem to have unique marker genes? Are there any clusters containing markers that are only specific to a portion of the cluster? Marker genes should uniformly define cells “everywhere” in a cluster in UMAP space, otherwise the cluster might actually be two!\n\nsc.pl.umap(\n    adata,\n    color=[\"CD74\", \"SSR4\", \"CA2\", \"HBA2\", \"CST3\", \"CD37\", \"IL32\", \"leiden_res0_5\"],\n    vmax=\"p99\",\n    legend_loc=\"on data\",\n    frameon=False,\n    cmap=\"coolwarm\",\n)\n\n\n\n\n\n\n\n\nExercise 7 (optional): Let’s take a few steps back to understand all of the previous steps a little bit better! The number of genes selected by the highly_variable_genes function can significantly impact your ability to cluster. Too few genes and you cannot discriminate between different cell types, too many genes and you capture lots of noisy clusters! Try repeating the previous analysis with either 200 or 5000 highly variable genes, naming the AnnData object differently (i.e. adata_200genes) to avoid overwriting your previous results. Transfer the metadata for the new cluster labels to the original AnnData object’s metadata at adata.obs and compare on the UMAP. Are the clusters different?\nOnce you have settled on the parameters for the dimensionality reduction and clustering steps, it is time to begin annotating your clusters with cell types. This is normally a challenging step! When you are not too familiar with the marker genes for a particular cluster, a good starting point is simply to Google a strong marker gene and understand its function. Other tools that might be useful include EnrichR and GSEAPy. - https://maayanlab.cloud/Enrichr/ - https://gseapy.readthedocs.io/en/latest/gseapy_example.html#2.-Enrichr-Example\nFortunately in our case, we will try automated cell type annotations!\n\n\nAutomated cell type annotation\nExercise 8 The methods discussed here focus on automated data annotation, distinct from manual methods. Unlike the previously detailed approach, these methods automate data annotation. They operate on different principles, using predefined markers or trained on comprehensive scRNA-seq datasets. It’s vital to note that automated annotations can vary in quality. Thus, they should be seen as a starting point rather than a final solution. Pasquini et al., 2021 and Abdelaal et al., 2019 offer extensive discussions on automated annotation methods.\nQuality depends on:\n\nClassifier Choice: Various classifier types perform similarly, with neural networks not necessarily outperforming linear models [1, 2, 3].\nTraining Data Quality: Annotation quality relies on the quality of the training data. Poorly annotated or noisy training data can impact the classifier.\nData Similarity: Similarity between your data and the classifier’s training data matters. Cross-dataset models often provide better annotations. For example, CellTypist, trained on diverse lung datasets, is likely to perform well on new lung data.\n\nWhile classifiers have limitations, they offer advantages like rapid annotation, leveraging previous studies, and promoting standardized terminology. Ensuring robust uncertainty measures to quantify annotation reliability is crucial.\nMany classification methods rely on a limited set of genes, typically just 1 to ~10 marker genes per cell type. An alternative approach utilizes classifiers that consider a more extensive gene set, often several thousands or more. These classifiers are trained on previously annotated datasets or atlases. Notable examples include CellTypist Conde et al., 2022 and Clustifyr Fu et al., 2020.\nLet’s explore CellTypist for our data. Referring to the CellTypist tutorial, we should prepare our data by normalizing counts to 10,000 counts per cell and subsequently applying a log1p transformation. So we need to re-normalize our data, without our logarithm shift approach, but with a more classical ‘Counts per ten-thousand’.\n\nimport re\nimport celltypist\nfrom celltypist import models\n\n\nadata_celltypist = adata_raw.copy()  # make a copy of our adata\nsc.pp.normalize_per_cell(\n    adata_celltypist, counts_per_cell_after=10000.0\n)  # normalize to 10,000 counts per cell\nsc.pp.log1p(adata_celltypist)  # log-transform\n# make .X dense instead of sparse, for compatibility with celltypist:\nadata_celltypist.X = adata_celltypist.X\n\nHere we will load the model directly from our folder on google drive, where we can find the model trained. Alternatively, CellTypist method propose a panel of models that can be download directly from python using models.download_models(force_update = True). The idea is of course to use a model that match our biological context, and for pre-trained model-based method like CellTypist, it is possible that your biological context is not available. In that situation, there is no other options than opting for manual annotations.\nThere are two models that might be relevant for this particular dataset we are working with. Let’s download both of them and try each one for the classification.\n\nmodels.download_models(\n    force_update=True, model=[\"Immune_All_Low.pkl\", \"Immune_All_High.pkl\"]\n)\n\n📜 Retrieving model list from server https://celltypist.cog.sanger.ac.uk/models/models.json\n📚 Total models in list: 46\n📂 Storing models in /home/alex/.celltypist/data/models\n💾 Total models to download: 2\n💾 Downloading model [1/2]: Immune_All_Low.pkl\n💾 Downloading model [2/2]: Immune_All_High.pkl\n\n\n\nmodel_low = models.Model.load(model=\"Immune_All_Low.pkl\")\nmodel_high = models.Model.load(model=\"Immune_All_High.pkl\")\n\nFor each of these, we can see which cell types it includes to see if bone marrow cell types are included:\n\n# We can print all the cell types covererd by the model\nmodel_low.cell_types\n\narray(['Age-associated B cells', 'Alveolar macrophages', 'B cells',\n       'CD16+ NK cells', 'CD16- NK cells', 'CD8a/a', 'CD8a/b(entry)',\n       'CMP', 'CRTAM+ gamma-delta T cells', 'Classical monocytes',\n       'Cycling B cells', 'Cycling DCs', 'Cycling NK cells',\n       'Cycling T cells', 'Cycling gamma-delta T cells',\n       'Cycling monocytes', 'DC', 'DC precursor', 'DC1', 'DC2', 'DC3',\n       'Double-negative thymocytes', 'Double-positive thymocytes', 'ELP',\n       'ETP', 'Early MK', 'Early erythroid', 'Early lymphoid/T lymphoid',\n       'Endothelial cells', 'Epithelial cells', 'Erythrocytes',\n       'Erythrophagocytic macrophages', 'Fibroblasts',\n       'Follicular B cells', 'Follicular helper T cells', 'GMP',\n       'Germinal center B cells', 'Granulocytes', 'HSC/MPP',\n       'Hofbauer cells', 'ILC', 'ILC precursor', 'ILC1', 'ILC2', 'ILC3',\n       'Intermediate macrophages', 'Intestinal macrophages',\n       'Kidney-resident macrophages', 'Kupffer cells',\n       'Large pre-B cells', 'Late erythroid', 'MAIT cells', 'MEMP', 'MNP',\n       'Macrophages', 'Mast cells', 'Megakaryocyte precursor',\n       'Megakaryocyte-erythroid-mast cell progenitor',\n       'Megakaryocytes/platelets', 'Memory B cells',\n       'Memory CD4+ cytotoxic T cells', 'Mid erythroid', 'Migratory DCs',\n       'Mono-mac', 'Monocyte precursor', 'Monocytes', 'Myelocytes',\n       'NK cells', 'NKT cells', 'Naive B cells',\n       'Neutrophil-myeloid progenitor', 'Neutrophils',\n       'Non-classical monocytes', 'Plasma cells', 'Plasmablasts',\n       'Pre-pro-B cells', 'Pro-B cells',\n       'Proliferative germinal center B cells', 'Promyelocytes',\n       'Regulatory T cells', 'Small pre-B cells', 'T(agonist)',\n       'Tcm/Naive cytotoxic T cells', 'Tcm/Naive helper T cells',\n       'Tem/Effector helper T cells', 'Tem/Effector helper T cells PD1+',\n       'Tem/Temra cytotoxic T cells', 'Tem/Trm cytotoxic T cells',\n       'Transitional B cells', 'Transitional DC', 'Transitional NK',\n       'Treg(diff)', 'Trm cytotoxic T cells', 'Type 1 helper T cells',\n       'Type 17 helper T cells', 'gamma-delta T cells', 'pDC',\n       'pDC precursor'], dtype=object)\n\n\n\n# We can print all the cell types covererd by the model\nmodel_high.cell_types\n\narray(['B cells', 'B-cell lineage', 'Cycling cells', 'DC', 'DC precursor',\n       'Double-negative thymocytes', 'Double-positive thymocytes', 'ETP',\n       'Early MK', 'Endothelial cells', 'Epithelial cells',\n       'Erythrocytes', 'Erythroid', 'Fibroblasts', 'Granulocytes',\n       'HSC/MPP', 'ILC', 'ILC precursor', 'MNP', 'Macrophages',\n       'Mast cells', 'Megakaryocyte precursor',\n       'Megakaryocytes/platelets', 'Mono-mac', 'Monocyte precursor',\n       'Monocytes', 'Myelocytes', 'Plasma cells', 'Promyelocytes',\n       'T cells', 'pDC', 'pDC precursor'], dtype=object)\n\n\nThe model_high seems to have fewer cell types, let’s start with that for obtaining broader cell type categories.\n\npredictions_high = celltypist.annotate(\n    adata_celltypist, model=model_high, majority_voting=True\n)\n\n🔬 Input data has 5469 cells and 10841 genes\n🔗 Matching reference genes in the model\n🧬 3731 features used for prediction\n⚖️ Scaling input data\n🖋️ Predicting labels\n✅ Prediction done!\n👀 Can not detect a neighborhood graph, will construct one before the over-clustering\n/home/alex/anaconda3/envs/sib_course_2024/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n⛓️ Over-clustering input data with resolution set to 10\n🗳️ Majority voting the predictions\n✅ Majority voting done!\n\n\n\npredictions_high_adata = predictions_high.to_adata()\npredictions_high_adata.obs[['majority_voting', 'conf_score']]\n\n\n\n\n\n\n\n\n\nmajority_voting\nconf_score\n\n\n\n\nAAACCTGCAGACGCAA-1\nB cells\n0.997790\n\n\nAAACCTGTCATCACCC-1\nT cells\n0.986443\n\n\nAAAGATGCATAAAGGT-1\nMonocytes\n0.961658\n\n\nAAAGCAAAGCAGCGTA-1\nT cells\n0.999908\n\n\nAAAGCAACAATAACGA-1\nT cells\n0.994181\n\n\n...\n...\n...\n\n\nTTTGTCACAATGAAAC-1\nT cells\n0.999788\n\n\nTTTGTCACATCTGGTA-1\nB cells\n0.999942\n\n\nTTTGTCAGTACAGCAG-1\nT cells\n0.828790\n\n\nTTTGTCAGTGTGAATA-1\nB-cell lineage\n0.970384\n\n\nTTTGTCAGTTCTCATT-1\nT cells\n0.976647\n\n\n\n\n5469 rows × 2 columns\n\n\n\n\n\nadata_raw_norm.obs[\"celltypist_annotations_high\"] = predictions_high_adata.obs[\"majority_voting\"]\nadata_raw_norm.obs[\"celltypist_conf_score_high\"] = predictions_high_adata.obs[\"conf_score\"]\n\nNow let’s do the same for the finer-grained annotations\n\npredictions_low = celltypist.annotate(\n    adata_celltypist, model=model_low, majority_voting=True\n)\n\n🔬 Input data has 5469 cells and 10841 genes\n🔗 Matching reference genes in the model\n🧬 3731 features used for prediction\n⚖️ Scaling input data\n🖋️ Predicting labels\n✅ Prediction done!\n👀 Detected a neighborhood graph in the input object, will run over-clustering on the basis of it\n⛓️ Over-clustering input data with resolution set to 10\n🗳️ Majority voting the predictions\n✅ Majority voting done!\n\n\n\npredictions_low_adata = predictions_low.to_adata()\npredictions_low_adata.obs[['majority_voting', 'conf_score']]\n\n\n\n\n\n\n\n\n\nmajority_voting\nconf_score\n\n\n\n\nAAACCTGCAGACGCAA-1\nB cells\n0.731157\n\n\nAAACCTGTCATCACCC-1\nTcm/Naive cytotoxic T cells\n0.975580\n\n\nAAAGATGCATAAAGGT-1\nClassical monocytes\n0.532938\n\n\nAAAGCAAAGCAGCGTA-1\nTcm/Naive helper T cells\n0.998210\n\n\nAAAGCAACAATAACGA-1\nTcm/Naive cytotoxic T cells\n0.809559\n\n\n...\n...\n...\n\n\nTTTGTCACAATGAAAC-1\nTcm/Naive cytotoxic T cells\n0.946592\n\n\nTTTGTCACATCTGGTA-1\nNaive B cells\n0.341540\n\n\nTTTGTCAGTACAGCAG-1\nTcm/Naive cytotoxic T cells\n0.135967\n\n\nTTTGTCAGTGTGAATA-1\nLarge pre-B cells\n0.217258\n\n\nTTTGTCAGTTCTCATT-1\nTcm/Naive helper T cells\n0.090272\n\n\n\n\n5469 rows × 2 columns\n\n\n\n\nAnd we save our predictions to our AnnData object:\n\nadata_raw_norm.obs[\"celltypist_annotations_low\"] = predictions_low_adata.obs[\"majority_voting\"]\nadata_raw_norm.obs[\"celltypist_conf_score_low\"] = predictions_low_adata.obs[\"conf_score\"]\n\nCellTypist annotations can then be visualized on the UMAP embedding:\n\nsc.settings.set_figure_params(dpi=80, facecolor='white')\nsc.pl.umap(\n    adata_raw_norm,\n    color=[\"celltypist_annotations_low\", \"celltypist_annotations_high\"],\n    frameon=False,\n    sort_order=False,\n    wspace=1.2,\n)\n\n\n\n\n\n\n\n\nAlso, each cell gets a prediction score:\n\nsc.pl.umap(\n    adata_raw_norm,\n    color=[\"celltypist_conf_score_low\", \"celltypist_conf_score_high\"],\n    frameon=False,\n    sort_order=False,\n    wspace=1,\n)\n\n\n\n\n\n\n\n\nOne way of getting a feeling for the quality of these annotations is by looking if the observed cell type similarities correspond to our expectations:\n\nsc.pl.dendrogram(adata_raw_norm, groupby=\"celltypist_annotations_low\")\n\nWARNING: dendrogram data not found (using key=dendrogram_celltypist_annotations_low). Running `sc.tl.dendrogram` with default parameters. For fine tuning it is recommended to run `sc.tl.dendrogram` independently.\n\n\n\n\n\n\n\n\n\n\n\nAnother way to annotate: with label transfer from a reference dataset!\n\n# To be added for the diseased conditions...\n\n\ndel adata_raw_norm.uns[\"dea_leiden_filtered\"]\n\n\nadata_raw_norm.write_h5ad(\"PBMC_analysis_SIB_tutorial5.h5ad\")"
  },
  {
    "objectID": "ipynb/Integration_D2B1_Controls.html",
    "href": "ipynb/Integration_D2B1_Controls.html",
    "title": "Single-cell transcriptomics with Python",
    "section": "",
    "text": "import scanpy as sc\nimport pandas as pd # for handling data frames (i.e. data tables)\nimport numpy as np # for handling numbers, arrays, and matrices\nimport matplotlib.pyplot as plt # plotting package\nimport seaborn as sns # plotting package\nExercise 0: Before we continue in this notebook with the next steps of the analysis, we need to load our results from the previous notebook using the sc.read_h5ad function and assign them to the variable name adata. Give it a try!"
  },
  {
    "objectID": "ipynb/Integration_D2B1_Controls.html#integration-and-batch-effect-correction",
    "href": "ipynb/Integration_D2B1_Controls.html#integration-and-batch-effect-correction",
    "title": "Single-cell transcriptomics with Python",
    "section": "Integration and batch effect correction",
    "text": "Integration and batch effect correction\n\nadata = sc.read_h5ad(\"PBMC_analysis_SIB_tutorial3.h5ad\")\n\nFor simple integration tasks, it is worth first trying the algorithm Harmony. The harmonypy package is a port of the original tool in R. Harmony uses a variant of singular value decomposition (SVD) to embed the data, then look for mutual nearest neighborhoods of similar cells across batches in the embedding, which it then uses to correct the batch effect in a locally adaptive (non-linear) manner.\n\nimport harmonypy as hm\n\nExercise 1: Harmonypy operates on the principal components you previous obtained using the command sc.tl.pca. These are stored in your adata object under the .obsm field as X_pca. Can you extract these and store them in a variable named mat_PC in order to provide them directly to harmonypy in the following code cell?\n\nmat_PC = adata.obsm[\"X_pca\"]\n\nNext, we run harmony on the data to correct for any batch effects. We must specify with vars_use the metadata column name in adata.obs that we would like to correct for.\n\nho = hm.run_harmony(data_mat = mat_PC, meta_data=adata.obs, vars_use=[\"sample\"])\n\n2024-04-25 17:41:12,679 - harmonypy - INFO - Computing initial centroids with sklearn.KMeans...\n2024-04-25 17:41:18,426 - harmonypy - INFO - sklearn.KMeans initialization complete.\n2024-04-25 17:41:18,525 - harmonypy - INFO - Iteration 1 of 10\n2024-04-25 17:41:20,283 - harmonypy - INFO - Iteration 2 of 10\n2024-04-25 17:41:22,581 - harmonypy - INFO - Iteration 3 of 10\n2024-04-25 17:41:24,569 - harmonypy - INFO - Iteration 4 of 10\n2024-04-25 17:41:25,812 - harmonypy - INFO - Iteration 5 of 10\n2024-04-25 17:41:27,935 - harmonypy - INFO - Converged after 5 iterations\n\n\nWe can then visualize the extent to which each of the principal components was rescaled by harmonypy to correct for batch effects.\n\npc_std = np.std(ho.Z_corr, axis=1).tolist()\n\n\nsns.scatterplot(x=range(0, len(pc_std)), y=sorted(pc_std, reverse=True))\n\n\n\n\n\n\n\n\nFinally, we store the results in our adata as X_pcahm variable.\n\nadata.obsm[\"X_pcahm\"] = ho.Z_corr.transpose()\n\nExercise 2: Now that we have used harmony to correct for batch effects in our principal components, we want to rerun the sc.pp.neighbors and sc.tl.umap steps from our previous exercises, to obtain a new UMAP representation. Write those commands out below to obtain your new UMAP. Important: when running sc.pp.neighbors, you must specify that you would like to use the harmony-adjusted PCs and not the original, unadjusted ones. To do this, specify the optional argument use_rep=X_pcahm!\n\nsc.pp.neighbors(adata, n_pcs=30, use_rep=\"X_pcahm\")\nsc.tl.umap(adata)\n\n/home/alex/anaconda3/envs/sib_course_2024/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\nExercise 3: Finally, visualize your UMAP with sc.pl.umap. How does the embedding compare to the one obtained prior to running harmony?\n\nsc.pl.umap(adata, color=[\"sample\"])\n\n\n\n\n\n\n\n\n\nadata.write_h5ad(\"PBMC_analysis_SIB_tutorial4.h5ad\") # save your results!"
  },
  {
    "objectID": "ipynb/Quality_Control_D1B2_Controls.html",
    "href": "ipynb/Quality_Control_D1B2_Controls.html",
    "title": "Analysis tools and quality control (QC)",
    "section": "",
    "text": "Today, you will begin to learn about the standard workflow for analyzing scRNA-seq count data in Python. As single cell data is complex and often tailored to the particular experimental design, so there is not one “correct” approach to analyzing these data. However, certain steps have become accepted as a sort of standard “best practice.”\nA useful overview on the current best practices is found in the articles below, which we also borrow from in this tutorial. We thank the authors for compiling such handy resources!\nCurrent best practices in single-cell RNA-seq analysis are explained in a recent Nature Review: https://www.nature.com/articles/s41576-023-00586-w\nAccompanying this review is an online webpage, which is still under development but can be quite handy nonetheless: https://www.sc-best-practices.org/preamble.html\nAfter having completed this chapter you will be able to:"
  },
  {
    "objectID": "ipynb/Quality_Control_D1B2_Controls.html#loading-scrnaseq-data",
    "href": "ipynb/Quality_Control_D1B2_Controls.html#loading-scrnaseq-data",
    "title": "Analysis tools and quality control (QC)",
    "section": "Loading scRNAseq data",
    "text": "Loading scRNAseq data\nAfter the generation of the count matrices with CellRanger, the next step is the data analysis. The scanpy package is currently the most popular software in Python to do this. To start working with scanpy, you must import the package into your Jupyter notebook as follows:\n\nimport scanpy as sc\n\nAn excellent resource for documentation on scanpy can be found on the software page at the following link: https://scanpy.readthedocs.io/en/stable/\nThere are some supplemental packages for data handling and visualization that are also very useful to import into your notebook as well.\n\nimport pandas as pd # for handling data frames (i.e. data tables)\nimport numpy as np # for handling numbers, arrays, and matrices\nimport matplotlib.pyplot as plt # plotting package\n\nFirst, we will load a file specifying the different samples, and create a dictionary “datadirs” specifying the location of the count data:\n\nsample_info = pd.read_csv(\"course_data/sample_info_course.csv\")\n\ndatadirs = {}\nfor sample_name in sample_info[\"SampleName\"]:\n    if \"PBMMC\" in sample_name:\n        datadirs[sample_name] = \"course_data/count_matrices/\" + sample_name + \"/outs/filtered_feature_bc_matrix\"\n\nTo run through a typical scanpy analysis, we will use the files that are in the directory outs/filtered_feature_bc_matrix. This directory is part of the output generated by CellRanger.\nWe will use the list of file paths generated in the previous step to load each sample into a separate AnnData object. We will then store all six of those samples in a list called adatas, and combine them into a single AnnData object for our analysis.\n\nadatas = []\nfor sample in datadirs.keys():\n    print(\"Loading: \", sample)\n    curr_adata = sc.read_10x_mtx(datadirs[sample]) # load file into an AnnData object\n    curr_adata.obs[\"sample\"] = sample\n    curr_adata.X = curr_adata.X.toarray()\n    adatas.append(curr_adata)\n    \nadata = sc.concat(adatas) # combine all samples into a single AnnData object\nadata.obs_names_make_unique() # make sure each cell barcode has a unique identifier\n\nLoading:  PBMMC_1\nLoading:  PBMMC_2\nLoading:  PBMMC_3\n\n\n/home/alex/anaconda3/envs/sib_course_2024/lib/python3.9/site-packages/anndata/_core/anndata.py:1818: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n  utils.warn_names_duplicates(\"obs\")\n\n\nThe AnnData object is similar to a detailed spreadsheet! Some basic commands to view the object are shown below. For a new dataset, there will be little to no metdata other than Cell IDs and gene names, but as you perform analyses, the metadata fields will be populated with more detail.\nExercise 1: Check what’s in the adata object, by typing adata in the Python console. How many gene features are in there? And how many cells?\nAnswer: typing adata should return:\nAnnData object with n_obs × n_vars = 6946 × 33694\nobs: 'sample'\nYou can also confirm the number of observations (cells) and variables (genes/features) using the commands below:\nadata.n_obs # number of cells\nadata.n_vars # number of genes"
  },
  {
    "objectID": "ipynb/Quality_Control_D1B2_Controls.html#the-anndata-object",
    "href": "ipynb/Quality_Control_D1B2_Controls.html#the-anndata-object",
    "title": "Analysis tools and quality control (QC)",
    "section": "The AnnData object",
    "text": "The AnnData object\nThe adata object we have created has the class AnnData. The object contains the single-cell count matrix, accessible with the command adata.X as well as various slots that specify sample metadata. This metadata is pretty limited when first loading the output from cellranger, but we will populate it with more useful information (i.e., cluster information) in later steps of the analysis.\nTo access the metadata corresponding to the cells (i.e. cell barcode, batch), you can enter the command adata.obs. To access the metadata corresponding to the genes (i.e, gene names, chromosome, etc), you can enter the command adata.var. These commands return a pandas.dataframe object. This data frame can be manipulated using any functions from the pandas package, including sum(), mean(), groupby(), and value_counts().\n\nadata.X # view the count matrix (rows x columns, cells x genes)\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)\n\n\n\nadata.obs.head() # view a pandas data frame containing metadata on the cells\n\n\n\n\n\n\n\n\n\nsample\n\n\n\n\nAAACCTGCAGACGCAA-1\nPBMMC_1\n\n\nAAACCTGTCATCACCC-1\nPBMMC_1\n\n\nAAAGATGCATAAAGGT-1\nPBMMC_1\n\n\nAAAGCAAAGCAGCGTA-1\nPBMMC_1\n\n\nAAAGCAACAATAACGA-1\nPBMMC_1\n\n\n\n\n\n\n\n\n\nadata.var.head() # view a pandas data frame containing metadata on the cells\n\n\n\n\n\n\n\n\n\n\n\n\n\nRP11-34P13.3\n\n\nFAM138A\n\n\nOR4F5\n\n\nRP11-34P13.7\n\n\nRP11-34P13.8\n\n\n\n\n\n\n\n\nExercise 2: Use the pandas value_counts() function to determine how many cells were collected for each of the six samples saved into your adata object? Keep in mind, since this is cell-specific metadata, we will want to work with the data frame returned by typing adata.obs.\nAnswer: You can run the command adata.obs[\"sample\"].value_counts() to view the number of cells per sample.\nThe output should be:\nsample\nPBMMC_2         3105\nPBMMC_3         2229\nPBMMC_1         1612\nName: count, dtype: int64"
  },
  {
    "objectID": "ipynb/Quality_Control_D1B2_Controls.html#quality-control",
    "href": "ipynb/Quality_Control_D1B2_Controls.html#quality-control",
    "title": "Analysis tools and quality control (QC)",
    "section": "Quality control",
    "text": "Quality control\nIn general, quality control (QC) should be done before any downstream analysis is performed. How the data is cleaned will likely have huge effects on downstream results, so it’s imperative to invest the time in choosing QC methods that you think are appropriate for your data! There are some “best practices” but these are by no means strict standards and also have certain limitations: https://www.sc-best-practices.org/preprocessing_visualization/quality_control.html\nGoals: - Filter the data to only include cells that are of high quality. This includes empty droplets, cells with a low total number of UMIs, doublets (two cells that got the same cell barcode), and dying cells (with a high fraction of mitochondrial reads).\nChallenges: - Delineating cells that are poor quality from less complex cell types - Choosing appropriate thresholds for filtering, so as to keep high quality cells without removing biologically relevant cell types or cell states.\nBefore analyzing the scRNA-seq gene expression data, we should ensure that all cellular barcode data corresponds to viable cells. Cell QC is commonly performed based on three QC covariates: - Library size: the number of counts per barcode (count depth) - Detected genes: the number of genes per barcode - Mitochondrial reads: the fraction of counts from mitochondrial genes per barcode.\nLibrary size: First, consider the total number of reads (UMIs) detected per cell. Cells with few reads are likely to have been broken or failed to capture a cell, and should thus be removed. Cells with many reads above the average for a sample are likely to be doublets, or two cells encapsulated in the gel bead during the protocol.\nExercise 3: Using the scanpy and matplotlib packages, visualize a histogram of the distribution of total counts per cell in the dataset. Save this information as metadata to the adata.obs dataframe using a command such as: adata.obs[\"n_counts\"] = n_counts_array. Choose lower and upper boundaries to filter out poor-quality cells and doublets.\nThe histogram function is: plt.hist() https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html\nTip: each function, such as plt.hist() has a set of required arguments. To view those required arguments, as well as an optional arguments, from your jupyter notebook, simply click with your cursor between the () parenthesis of the function, and tab tab+shift on your keyboard.\n\nn_counts_array = adata.X.sum(axis=1) # axis=1 to sum over genes, axis=0 to sum over cells\nadata.obs['n_counts'] = n_counts_array\n\n\nplt.hist(adata.obs['n_counts'], bins=100)\nplt.xlabel(\"Number of UMIs\")\nplt.ylabel(\"Number of cells\")\nplt.axvline(2000, c=\"r\") # specify the lower cutoff for total UMIs\nplt.axvline(12500, c=\"r\") # specify the upper cutoff for total UMIs\nplt.xlim(0, 20000)\nplt.show()\n\n\n\n\n\n\n\n\nExercise 4: Using the scanpy and matplotlib packages, visualize a histogram of the distribution of total genes expressed per cell in the dataset. Save this information as metadata to the adata.obs dataframe using a command such as: adata.obs[\"n_genes\"] = n_genes_array. Choose lower and upper boundaries to filter out low-diversity cells.\n\nexpressed_genes = np.sum(adata.X &gt; 0, 1)\nadata.obs['n_genes'] = expressed_genes\n\nplt.hist(adata.obs['n_genes'], bins=100)\nplt.axvline(500, c=\"r\") # specify the lower cutoff for number of detected genes\nplt.axvline(4000, c=\"r\") # specify the upper cutoff for number of detected genes\nplt.xlabel(\"Number of Genes\")\nplt.ylabel(\"Number of Cells\")\nplt.show()\n\n\n\n\n\n\n\n\nNext, we want to consider filtering cells with high levels of certain classes of genes, namely mitochondrial, ribosomal, and/or hemoglobin genes. There is a different rationale for filtering cells with high levels of these gene classes:\n\nMitochondrial genes: If a cell membrane is damaged, it looses free RNA quicker compared to mitochondrial RNA, because the latter is part of the mitochondrion. A high relative amount of mitochondrial counts can therefore point to damaged cells (Lun et al. 2016).\nRibosomal genes: Are not rRNA (ribosomal RNA) but is mRNA that code for ribosomal proteins. They do not point to specific issues, but it can be good to have a look at their relative abundance. They can have biological relevance (e.g. Caron et al. 2020).\nHemoglobin genes: these transcripts are very abundant in erythrocytes. Depending on your application, you can expect ‘contamination’ of erythrocytes and select against it.\n\nIn order to have an idea about the relative counts of these type of genes in our dataset, we can calculate their expression as relative counts in each cell. We do that by selecting genes based on patterns (e.g. ^MT- matches with all gene names starting with MT, i.e. mitochondrial genes):\n\n# mitochondrial genes\nadata.var[\"mt\"] = adata.var_names.str.startswith(\"MT-\")\n\n# ribosomal genes\nadata.var[\"ribo\"] = adata.var_names.str.startswith((\"RPS\", \"RPL\"))\n\n# hemoglobin genes.\nadata.var[\"hb\"] = adata.var_names.str.contains((\"^HB[^(P)]\"))\n\n\nsc.pp.calculate_qc_metrics(adata, qc_vars=[\"mt\", \"ribo\", \"hb\"], inplace=True, percent_top=[20]) # this step can be a little slow to run\n\nExercise 5: Run the commands and check out the metadata data frame at adata.obs. What has changed?\nAnswer: If we type adata.obs, a lot more metadata is present compared to before! This should include the following columns:\npct_counts_mt\ntotal_counts_mt\npct_counts_ribo\ntotal_counts_ribo\npct_counts_hb\ntotal_counts_hb\nExercise 6: Using scanpy’s sc.pl.violin function, create a violin plot of the percent of reads corresponding to mitochondrial, ribosomal, and hemoglobin genes per cell. Choose an upper boundary to filter out poor quality cells with high mitochondrial reads. Note that we might want to view the results as a separate violin plot for each of our six samples. To do this, please use the optional groupby=\"sample\" argument.\nPlease note that depending on your experimental setup, it might not make sense to filter on all these criteria.\n\n# Violin plots for all samples together\nsc.pl.violin(adata, [\"pct_counts_mt\", \"pct_counts_ribo\", \"pct_counts_hb\"])\n\n\n\n\n\n\n\n\n\n# Violin plots for each sample separately\nsc.pl.violin(adata, [\"pct_counts_mt\", \"pct_counts_ribo\", \"pct_counts_hb\"], groupby=\"sample\")\n\n\n\n\n\n\n\n\nYou can see that PBMMC-2 is quite different from the two others, it has a group of cells with very low ribosomal counts and one with very high globin counts. Maybe these two percentages are negatively correlated? Let’s have a look, by plotting the two percentages against each other:\n\nsc.pl.scatter(adata, x=\"pct_counts_hb\", y=\"pct_counts_ribo\", color='sample')\n\n\n\n\n\n\n\n\nExercise 7: Are they correlated? What kind of cells might have a high abundance of hemoglobin transcripts and low ribosomal transcripts?\nAnswer: Yes there is a negative correlation. Erythrocytes (red blood cells) have a high abundance of hemoglobin transcripts and low abundance of ribosomal transcripts. These are most likely erythroid cells, i.e. the precursor cells for erythrocytes in the bone marrow.\nWe can also evaluate the relative expression of other genes in our dataset, for example, the ones that are most highly expressed. Some very highly expressed genes might point to a technical cause, and we might consider to remove them. Below you will find a simple function to generate a boxplot of relative counts per gene per cell:\n\nsc.pl.highest_expr_genes(adata, n_top=20)"
  },
  {
    "objectID": "ipynb/Quality_Control_D1B2_Controls.html#doublet-detection",
    "href": "ipynb/Quality_Control_D1B2_Controls.html#doublet-detection",
    "title": "Analysis tools and quality control (QC)",
    "section": "Doublet detection",
    "text": "Doublet detection\nThere are several tools for identifying doublets (i.e. two cells that were encapsulated with the same gel bead, obtaining the same cell barcode). Recently a benchmarking study was conducted comparing approaches: https://www.sciencedirect.com/science/article/pii/S2405471220304592\nHere, we suggest you use Scrublet for doublet detection, as it is fully incorporated into the scanpy framework: https://www.cell.com/cell-systems/pdfExtended/S2405-4712(18)30474-5\nA tutorial with scanpy is described below: https://github.com/swolock/scrublet\nIf you want to compare doublet detection methods, another method is DoubletDetection: https://doubletdetection.readthedocs.io/en/latest/tutorial.html\n\nimport scrublet as scr\n\n\nscrub = scr.Scrublet(adata.X)\n\n\ndoublet_scores, predicted_doublets = scrub.scrub_doublets()\n\nPreprocessing...\nSimulating doublets...\nEmbedding transcriptomes using PCA...\nCalculating doublet scores...\nAutomatically set threshold at doublet score = 0.45\nDetected doublet rate = 1.0%\nEstimated detectable doublet fraction = 37.1%\nOverall doublet rate:\n    Expected   = 10.0%\n    Estimated  = 2.6%\nElapsed time: 17.1 seconds\n\n\nExercise 8: Run the above steps. How many doublets are predicted? Can you assign this predicted_doublets metadata to your adata object with adata.obs[\"is_doublet\"] and then use value_counts() to see the number of doublets?\n\nadata.obs[\"is_doublet\"] = predicted_doublets\n\n\nadata.obs[\"is_doublet\"].value_counts()\n\nis_doublet\nFalse    6879\nTrue       67\nName: count, dtype: int64\n\n\nFinally, let’s filter out the doublet cells!\n\nadata = adata[~adata.obs[\"is_doublet\"]].copy()\n\n\nadata # is now populated with some more metadata from the QC steps above\n\nAnnData object with n_obs × n_vars = 6879 × 33694\n    obs: 'sample', 'n_counts', 'n_genes', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_20_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'log1p_total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'log1p_total_counts_hb', 'pct_counts_hb', 'is_doublet'\n    var: 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts'\n    uns: 'sample_colors'"
  },
  {
    "objectID": "ipynb/Quality_Control_D1B2_Controls.html#cell-filtering",
    "href": "ipynb/Quality_Control_D1B2_Controls.html#cell-filtering",
    "title": "Analysis tools and quality control (QC)",
    "section": "Cell Filtering",
    "text": "Cell Filtering\nExercise 9: Once you have selected your QC filtering criteria, you need to actually do the filtering! You can do this either (1) using the QC thresholds you selected above or (2) obtaining automated thresholds using scanpy quality control metrics. Whether you choose the (1) or (2) approach is up to you. Unfortunately, there is not much automation in the quality control steps at this stage, although there are ongoing efforts by the single cell community to create a more unbiased QC approaches.\nUse the following scanpy quality control checks to filter out poor quality cells based either (1) your semi-subjective criteria OR (2) the somewhat automated approach offered described here: https://www.sc-best-practices.org/preprocessing_visualization/quality_control.html\nHint: you can also filter an AnnData object using indexing approaches, as with numpy arrays and pandas data frames. For instance, the follow command filters genes (columns) on a qc metric for percent mitochondrial reads: adata = adata[adata.obs['percent_mito'] &lt; 0.08, :].copy()\n\nsc.pp.filter_cells(adata, min_counts=2000) # apply threshold from above to actually do the filtering\nsc.pp.filter_cells(adata, max_counts=12500) # apply threshold from above to actually do the filtering\n\n\nsc.pp.filter_cells(adata, min_genes=200) # apply threshold from above to actually do the filtering\nsc.pp.filter_cells(adata, max_genes=5000) # apply threshold from above to actually do the filtering\n\n\nadata = adata[adata.obs['pct_counts_mt'] &lt;= 8, :].copy() # apply threshold from above to actually do the filtering\n\n\nadata\n\nAnnData object with n_obs × n_vars = 5469 × 33694\n    obs: 'sample', 'n_counts', 'n_genes', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_20_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'log1p_total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'log1p_total_counts_hb', 'pct_counts_hb', 'is_doublet'\n    var: 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts'\n    uns: 'sample_colors'\n\n\nExercise 10: We have been discussing cell filtering, but you may also want to filter out genes that are not detected in your data! For this, you can use the function sc.pp.filter_genes. Try doing this to filter out genes expressed in fewer than 1% of your total cells. How many genes are removed (you can check the value of adata.n_vars before and after filtering with sc.pp.filter_genes.\n\nn_cells = adata.n_obs\nsc.pp.filter_genes(adata, min_cells=int(n_cells*0.01)) # specify min cells equal to 1% of your total cell count\n\n\nadata\n\nAnnData object with n_obs × n_vars = 5469 × 10841\n    obs: 'sample', 'n_counts', 'n_genes', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_20_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'log1p_total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'log1p_total_counts_hb', 'pct_counts_hb', 'is_doublet'\n    var: 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts', 'n_cells'\n    uns: 'sample_colors'\n\n\nExercise 11: You have finished this set of exercises! One important final step: in case you want to save your results at any time, you can use the command adata.write_h5ad() to save your AnnData object for later use. Try doing this here, so that you can load the adata object into the next Jupyter notebook tutorial on Normalization and Scaling.\n\nadata.write_h5ad(\"PBMC_analysis_SIB_tutorial.h5ad\") # feel free to choose whichever file name you prefer!"
  },
  {
    "objectID": "ipynb/day1-1_setup.html",
    "href": "ipynb/day1-1_setup.html",
    "title": "Setup",
    "section": "",
    "text": "If you are enrolled in the course, log on the server with the provided link, username, and password. The environment on the server contains all the necessary software pre-installed."
  },
  {
    "objectID": "ipynb/day1-1_setup.html#downloading-the-course-data",
    "href": "ipynb/day1-1_setup.html#downloading-the-course-data",
    "title": "Setup",
    "section": "Downloading the course data",
    "text": "Downloading the course data\nTo download and extract the dataset, copy-paste these commands inside the terminal tab:\nwget https://single-cell-transcriptomics-python.s3.eu-central-1.amazonaws.com/course_data.tar.gz\ntar -xvf course_data.tar.gz\nrm course_data.tar.gz\n\nIf on Windows\nIf you’re using Windows, you can directly open the link in your browser, and downloading will start automatically. Unpack the tar.gz file in the directory where you want to work in during the course.\nHave a look at the data directory you have downloaded. It should contain the following:\ncourse_data\n├── count_matrices\n│   ├── ETV6-RUNX1_1\n│   │   └── outs\n│   │       └── filtered_feature_bc_matrix\n│   │           ├── barcodes.tsv.gz\n│   │           ├── features.tsv.gz\n│   │           └── matrix.mtx.gz\n│   ├── ETV6-RUNX1_2\n│   │   └── outs\n│   │       └── filtered_feature_bc_matrix\n│   │           ├── barcodes.tsv.gz\n│   │           ├── features.tsv.gz\n│   │           └── matrix.mtx.gz\n│   ├── ETV6-RUNX1_3\n│   │   └── outs\n│   │       └── filtered_feature_bc_matrix\n│   │           ├── barcodes.tsv.gz\n│   │           ├── features.tsv.gz\n│   │           └── matrix.mtx.gz\n│   ├── PBMMC_1\n│   │   └── outs\n│   │       └── filtered_feature_bc_matrix\n│   │           ├── barcodes.tsv.gz\n│   │           ├── features.tsv.gz\n│   │           └── matrix.mtx.gz\n│   ├── PBMMC_2\n│   │   └── outs\n│   │       └── filtered_feature_bc_matrix\n│   │           ├── barcodes.tsv.gz\n│   │           ├── features.tsv.gz\n│   │           └── matrix.mtx.gz\n│   └── PBMMC_3\n│       └── outs\n│           └── filtered_feature_bc_matrix\n│               ├── barcodes.tsv.gz\n│               ├── features.tsv.gz\n│               └── matrix.mtx.gz\n└── reads\n    ├── ETV6-RUNX1_1_S1_L001_I1_001.fastq.gz\n    ├── ETV6-RUNX1_1_S1_L001_R1_001.fastq.gz\n    └── ETV6-RUNX1_1_S1_L001_R2_001.fastq.gz\n\n20 directories, 21 files\nThis data comes from:\nCaron M, St-Onge P, Sontag T, Wang YC, Richer C, Ragoussis I, et al. Single-cell analysis of childhood leukemia reveals a link between developmental states and ribosomal protein expression as a source of intra-individual heterogeneity. Scientific Reports. 2020;10:1–12. Available from: http://dx.doi.org/10.1038/s41598-020-64929-x\nWe will use the reads to showcase the use of cellranger count. The directory contains only reads from chromosome 21 and 22 of one sample (ETV6-RUNX1_1). The count matrices are output of cellranger count, and we will use those for the other exercises in R."
  },
  {
    "objectID": "ipynb/day1-3_normalization_scaling.html",
    "href": "ipynb/day1-3_normalization_scaling.html",
    "title": "Normalization",
    "section": "",
    "text": "After having completed this chapter you will be able to:\n\nPerform size normalization and log transformation of single-cell data.\nIntuitively understand what “normalization” does to your data\nIdentifying highly variable gene features\nPerform data scaling\nAssign categorical cell cycle phases to single cells\n\n\nimport scanpy as sc\nimport pandas as pd # for handling data frames (i.e. data tables)\nimport numpy as np # for handling numbers, arrays, and matrices\nimport matplotlib.pyplot as plt # plotting package\nimport seaborn as sns # plotting package\n\nExercise 0: Before we continue in this notebook with the next steps of the analysis, we need to load our results from the previous notebook using the sc.read_h5ad function and assign them to the variable name adata. Give it a try!\n\nClick for Answer\n\n\nAnswer: Load your data from the previous notebook into an h5ad object with sc.read_h5ad\n    adata = sc.read_h5ad(\"PBMC_analysis_SIB_tutorial.h5ad\")\n\n\nEach count in a count matrix represents the successful capture, reverse transcription and sequencing of a molecule of cellular mRNA. Count depths for identical cells can differ due to the variability inherent in each of these steps. Thus, when gene expression is compared between cells based on count data, any difference may have arisen solely due to sampling effects. Normalization addresses this issue by e.g. scaling count data to obtain correct relative gene expression abundances between cells.\nA recent benchmark published by Ahlmann-Eltze and Huber (2023) compared 22 different transformations for single-cell data, which surprisingly showed that a seemingly simple shifted logarithm transformation outperformed gold-standard SCTransform (the default method in analysis package’s such as Seurat in R).\n\n\nWe will thus use the shifted logarithm for normalizing our data, which is based on the delta method v\n\\[f(y) = \\log \\left( \\frac{y}{s} + y_0 \\right) \\]\nwith \\(y\\) being the raw counts, \\(s\\) being a so-called size factor and \\(y_0\\) describing a pseudo-count. The size factors are determined for each cell to account for variations in sampling effects and different cell sizes. The size factor for a cell \\(c\\) can be calculated by:\n\\[ s_c = \\frac{\\sum_g y_{gc}}{L} \\]\nwith \\(g\\) indexing different genes and \\(L\\) describing a target sum. There are different approaches to determine the size factors from the data. We will leverage the scanpy default in this section with \\(L\\) being the median raw count depth in the dataset. Many analysis templates use fixed values for \\(L\\), for example \\(L = 10^6\\), resulting in “counts per million” metric (CPM). For a beginner, these values may seem arbitrary, but it can lead to an overestimation of the variance. Indeed, Ahlmann-Eltze and Huber (2023) showed that log(CPM+1) performed poorly on a variety of tasks.\nThe shifted logarithm is a fast normalization technique, outperforms other methods for uncovering the latent structure of the dataset (especially when followed by principal component analysis) and works beneficial for stabilizing variance for subsequent dimensionality reduction and identification of differentially expressed genes.\nExercise 1: Perform normalization using the logarithm shift, following the above formula. Without relying on scanpy implemented function, you should be able to compute the log-normalized data, and store the resulting matrix in the object log_shifted_matrix below. We then provide code to display the distribution of the raw counts versus the normalized data (exercise courtesy of Alexandre Coudray).\n\nClick for Answer\n\n\nAnswer:\n    median_raw_counts = np.median(adata.obs['n_counts'])\n    size_factors = np.array(adata.obs['n_counts'] / median_raw_counts)\n    log_shifted_matrix = np.log(adata.X / size_factors[:,None] + 1) # we use a pseudo-count of 1\n\n\n\nmedian_raw_counts = np.median(adata.obs['n_counts'])\n\nsize_factors = np.array(adata.obs['n_counts'] / median_raw_counts)\n\nlog_shifted_matrix = np.log(adata.X / size_factors[:,None] + 1) # we use a pseudo-count of 1\n\nOnce you have performed the log-normalization shift, you can visualize your results here to see how the counts at adata.X have changed:\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\np1 = sns.histplot(adata.obs[\"n_counts\"], bins=100, kde=False, ax=axes[0])\naxes[0].set_title(\"Total counts\")\np2 = sns.histplot(log_shifted_matrix.sum(1), bins=100, kde=False, ax=axes[1])\naxes[1].set_title(\"Shifted logarithm\")\nplt.show()\n\n\n\n\n\n\n\n\nThe shifted logarithm can be conveniently called with scanpy by running pp.normalized_total with target_sum=None. We then apply a log transformation with a pseudo-count of 1, which can be easily done with the function sc.pp.log1p.\n\n# This can be easily done with scanpy normalize_total and log1p functions\nscales_counts = sc.pp.normalize_total(adata, target_sum=None, inplace=False)\n# log1p transform - log the data and adds a pseudo-count of 1\nscales_counts = sc.pp.log1p(scales_counts[\"X\"], copy=True)\n\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\np1 = sns.histplot(adata.obs[\"n_counts\"], bins=100, kde=False, ax=axes[0])\naxes[0].set_title(\"Total counts\")\np2 = sns.histplot(scales_counts.sum(1), bins=100, kde=False, ax=axes[1])\naxes[1].set_title(\"Shifted logarithm\")\nplt.show()\n\n\n\n\n\n\n\n\nWe verify that our normalization is the same as the one implemented in scanpy. You should have a corelation above 0.999999. The reason why we are not reaching a corelation of 1.0 comes from the precision in float number used by scanpy.\n\nfrom scipy.stats import pearsonr\n\npearsonr(np.array(scales_counts).flatten(),\n         np.array(log_shifted_matrix).flatten())\n\nPearsonRResult(statistic=0.999998708385195, pvalue=0.0)\n\n\nWe then run the normalized_total and log1p functions it again so that the normalized data is set as our default matrix in .X emplacement.\nExercise 2: Perform normalization and log transformation using the built in scanpy functions, as hinted at above.\n\nClick for Answer\n\n\nAnswer:\n    # To directly change the data 'in place', use the following:\n    sc.pp.normalize_total(adata, target_sum=None)\n    sc.pp.log1p(adata)\n\n\nOf course, in your own analysis, you can just use the two scanpy commands in the code cell above. However, this exercise aims as giving you a better understanding of the transformation being applied to your data! Some more complex machine learning based algorithms (i.e. batch integration with scVI, coming up in a later exercise during this course) actually work more optimally on the raw, untransformed counts!"
  },
  {
    "objectID": "ipynb/day1-3_normalization_scaling.html#learning-outcomes",
    "href": "ipynb/day1-3_normalization_scaling.html#learning-outcomes",
    "title": "Normalization",
    "section": "",
    "text": "After having completed this chapter you will be able to:\n\nPerform size normalization and log transformation of single-cell data.\nIntuitively understand what “normalization” does to your data\nIdentifying highly variable gene features\nPerform data scaling\nAssign categorical cell cycle phases to single cells\n\n\nimport scanpy as sc\nimport pandas as pd # for handling data frames (i.e. data tables)\nimport numpy as np # for handling numbers, arrays, and matrices\nimport matplotlib.pyplot as plt # plotting package\nimport seaborn as sns # plotting package\n\nExercise 0: Before we continue in this notebook with the next steps of the analysis, we need to load our results from the previous notebook using the sc.read_h5ad function and assign them to the variable name adata. Give it a try!\n\nClick for Answer\n\n\nAnswer: Load your data from the previous notebook into an h5ad object with sc.read_h5ad\n    adata = sc.read_h5ad(\"PBMC_analysis_SIB_tutorial.h5ad\")\n\n\nEach count in a count matrix represents the successful capture, reverse transcription and sequencing of a molecule of cellular mRNA. Count depths for identical cells can differ due to the variability inherent in each of these steps. Thus, when gene expression is compared between cells based on count data, any difference may have arisen solely due to sampling effects. Normalization addresses this issue by e.g. scaling count data to obtain correct relative gene expression abundances between cells.\nA recent benchmark published by Ahlmann-Eltze and Huber (2023) compared 22 different transformations for single-cell data, which surprisingly showed that a seemingly simple shifted logarithm transformation outperformed gold-standard SCTransform (the default method in analysis package’s such as Seurat in R).\n\n\nWe will thus use the shifted logarithm for normalizing our data, which is based on the delta method v\n\\[f(y) = \\log \\left( \\frac{y}{s} + y_0 \\right) \\]\nwith \\(y\\) being the raw counts, \\(s\\) being a so-called size factor and \\(y_0\\) describing a pseudo-count. The size factors are determined for each cell to account for variations in sampling effects and different cell sizes. The size factor for a cell \\(c\\) can be calculated by:\n\\[ s_c = \\frac{\\sum_g y_{gc}}{L} \\]\nwith \\(g\\) indexing different genes and \\(L\\) describing a target sum. There are different approaches to determine the size factors from the data. We will leverage the scanpy default in this section with \\(L\\) being the median raw count depth in the dataset. Many analysis templates use fixed values for \\(L\\), for example \\(L = 10^6\\), resulting in “counts per million” metric (CPM). For a beginner, these values may seem arbitrary, but it can lead to an overestimation of the variance. Indeed, Ahlmann-Eltze and Huber (2023) showed that log(CPM+1) performed poorly on a variety of tasks.\nThe shifted logarithm is a fast normalization technique, outperforms other methods for uncovering the latent structure of the dataset (especially when followed by principal component analysis) and works beneficial for stabilizing variance for subsequent dimensionality reduction and identification of differentially expressed genes.\nExercise 1: Perform normalization using the logarithm shift, following the above formula. Without relying on scanpy implemented function, you should be able to compute the log-normalized data, and store the resulting matrix in the object log_shifted_matrix below. We then provide code to display the distribution of the raw counts versus the normalized data (exercise courtesy of Alexandre Coudray).\n\nClick for Answer\n\n\nAnswer:\n    median_raw_counts = np.median(adata.obs['n_counts'])\n    size_factors = np.array(adata.obs['n_counts'] / median_raw_counts)\n    log_shifted_matrix = np.log(adata.X / size_factors[:,None] + 1) # we use a pseudo-count of 1\n\n\n\nmedian_raw_counts = np.median(adata.obs['n_counts'])\n\nsize_factors = np.array(adata.obs['n_counts'] / median_raw_counts)\n\nlog_shifted_matrix = np.log(adata.X / size_factors[:,None] + 1) # we use a pseudo-count of 1\n\nOnce you have performed the log-normalization shift, you can visualize your results here to see how the counts at adata.X have changed:\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\np1 = sns.histplot(adata.obs[\"n_counts\"], bins=100, kde=False, ax=axes[0])\naxes[0].set_title(\"Total counts\")\np2 = sns.histplot(log_shifted_matrix.sum(1), bins=100, kde=False, ax=axes[1])\naxes[1].set_title(\"Shifted logarithm\")\nplt.show()\n\n\n\n\n\n\n\n\nThe shifted logarithm can be conveniently called with scanpy by running pp.normalized_total with target_sum=None. We then apply a log transformation with a pseudo-count of 1, which can be easily done with the function sc.pp.log1p.\n\n# This can be easily done with scanpy normalize_total and log1p functions\nscales_counts = sc.pp.normalize_total(adata, target_sum=None, inplace=False)\n# log1p transform - log the data and adds a pseudo-count of 1\nscales_counts = sc.pp.log1p(scales_counts[\"X\"], copy=True)\n\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\np1 = sns.histplot(adata.obs[\"n_counts\"], bins=100, kde=False, ax=axes[0])\naxes[0].set_title(\"Total counts\")\np2 = sns.histplot(scales_counts.sum(1), bins=100, kde=False, ax=axes[1])\naxes[1].set_title(\"Shifted logarithm\")\nplt.show()\n\n\n\n\n\n\n\n\nWe verify that our normalization is the same as the one implemented in scanpy. You should have a corelation above 0.999999. The reason why we are not reaching a corelation of 1.0 comes from the precision in float number used by scanpy.\n\nfrom scipy.stats import pearsonr\n\npearsonr(np.array(scales_counts).flatten(),\n         np.array(log_shifted_matrix).flatten())\n\nPearsonRResult(statistic=0.999998708385195, pvalue=0.0)\n\n\nWe then run the normalized_total and log1p functions it again so that the normalized data is set as our default matrix in .X emplacement.\nExercise 2: Perform normalization and log transformation using the built in scanpy functions, as hinted at above.\n\nClick for Answer\n\n\nAnswer:\n    # To directly change the data 'in place', use the following:\n    sc.pp.normalize_total(adata, target_sum=None)\n    sc.pp.log1p(adata)\n\n\nOf course, in your own analysis, you can just use the two scanpy commands in the code cell above. However, this exercise aims as giving you a better understanding of the transformation being applied to your data! Some more complex machine learning based algorithms (i.e. batch integration with scVI, coming up in a later exercise during this course) actually work more optimally on the raw, untransformed counts!"
  },
  {
    "objectID": "ipynb/day1-3_normalization_scaling.html#variable-feature-selection",
    "href": "ipynb/day1-3_normalization_scaling.html#variable-feature-selection",
    "title": "Normalization",
    "section": "Variable feature selection",
    "text": "Variable feature selection\nNow, we begin with dimensionality reduction, i.e. reducing the number of variables in the data by removing features (genes) with little variability among the cells and by combining highly similar features. This is important because normally you start with tens of thousands of genes and it is difficult to represent their patterns in a two-dimensional visualization.\nOne useful (but optional) step is to perform cell cycle characterization, as the cell cycle signature is often a strong convoluting factor with cell type signatures.\nExercise 3: Perform cell cycle characterization on your dataset. Plot a scatter plot of the S_score and G2M_score metadata created and stored in your AnnData object. Color the points by the assigned cell cycle phase. What percentage of your cells are in a proliferative state (S or G2M phases)?\n\n# your code here\nS_genes_mouse = np.array(['MCM5', 'PCNA', 'TYMS', 'FEN1', 'MCM2', 'MCM4', 'RRM1', 'UNG',\n       'GINS2', 'MCM6', 'CDCA7', 'DTL', 'PRIM1', 'UHRF1', 'CENPU',\n       'HELLS', 'RFC2', 'RPA2', 'NASP', 'RAD51AP1', 'GMNN', 'WDR76',\n       'SLBP', 'CCNE2', 'UBR7', 'POLD3', 'MSH2', 'ATAD2', 'RAD51', 'RRM2',\n       'CDC45', 'CDC6', 'EXO1', 'TIPIN', 'DSCC1', 'BLM', 'CASP8AP2',\n       'USP1', 'CLSPN', 'POLA1', 'CHAF1B', 'BRIP1', 'E2F8'])\nG2M_genes_mouse = np.array(['HMGB2', 'CDK1', 'NUSAP1', 'UBE2C', 'BIRC5', 'TPX2', 'TOP2A',\n       'NDC80', 'CKS2', 'NUF2', 'CKS1B', 'MKI67', 'TMPO', 'CENPF',\n       'TACC3', 'PIMREG', 'SMC4', 'CCNB2', 'CKAP2L', 'CKAP2', 'AURKB',\n       'BUB1', 'KIF11', 'ANP32E', 'TUBB4B', 'GTSE1', 'KIF20B', 'HJURP',\n       'CDCA3', 'JPT1', 'CDC20', 'TTK', 'CDC25C', 'KIF2C', 'RANGAP1',\n       'NCAPD2', 'DLGAP5', 'CDCA2', 'CDCA8', 'ECT2', 'KIF23', 'HMMR',\n       'AURKA', 'PSRC1', 'ANLN', 'LBR', 'CKAP5', 'CENPE', 'CTCF', 'NEK2',\n       'G2E3', 'GAS2L3', 'CBX5', 'CENPA'])\n\nsc.tl.score_genes_cell_cycle(adata, s_genes=S_genes_mouse, g2m_genes=G2M_genes_mouse)\n\nWARNING: genes are not in var_names and ignored: ['PIMREG', 'JPT1']\n\n\nThe gene lists provided here are the “standard” in single cell analysis as come from the two papers: - Buettner et al (2015) - Satija et al (2015)\n\nn2c = {\"G1\":\"red\", \"S\":\"green\", \"G2M\":\"blue\"} # use to assign each cell a color based on phase in the scatter plot\nplt.scatter(adata.obs['S_score'], adata.obs['G2M_score'], c=[n2c[k] for k in adata.obs['phase']])\nplt.xlabel('S score') ; plt.ylabel('G2M score')\nplt.show()\n\n\n\n\n\n\n\n\nExercise 4: How many cells do you have assigned to each of the cell cycle phases? Can you check this using a function you have applied in the previous exercises?\n\nClick for Answer\n\n\nAnswer: typing adata.obs[“phase”].value_counts() should return:\n    phase\n    G1      3129\n    S       1400\n    G2M     940\n    Name: count, dtype: int64\n\n\nWe next calculate a subset of features that exhibit high cell-to-cell variation in the dataset (i.e, they are highly expressed in some cells, and lowly expressed in others). Genes that are similarly expressed in all cells will not assist with discriminating different cell types from each other.\nThe procedure in scanpy models the mean-variance relationship inherent in single-cell data, and is implemented in the sc.pp.highly_variable_genes function. By default, 2,000 genes (features) per dataset are returned and these will be used in downstream analysis, like PCA.\n\n# suggestion: start with 3000 highly variable genes\nsc.pp.highly_variable_genes(adata, n_top_genes=3000)\n\n\nadata = adata[:, adata.var[\"highly_variable\"]].copy() # actually do the filtering\n\nWhile correcting for technical covariates may be crucial to uncovering the underlying biological signal, correction for biological covariates serves to single out particular biological signals of interest. The most common biological data correction is to remove the effects of the cell cycle on the transcriptome, the number of raw counts that existed per cell, or the percentage of mitochondrial reads present. This data correction can be performed by a simple linear regression against a cell cycle score as implemented in scanpy.\nExercise 5: Use scanpy’s regress_out function to remove the effect of the cell cycle phase metadata from your downstream analyses steps.\n\nClick for Answer\n\n\nAnswer: sc.pp.regress_out(adata, ‘phase’) is the command you want to apply here.\n\n\nsc.pp.regress_out(adata, 'phase') # specify which feature from adata.obs you want to regress out"
  },
  {
    "objectID": "ipynb/day1-3_normalization_scaling.html#scaling",
    "href": "ipynb/day1-3_normalization_scaling.html#scaling",
    "title": "Normalization",
    "section": "Scaling",
    "text": "Scaling\nNext, we apply scaling, a linear transformation that is a standard pre-processing step prior to dimensional reduction techniques like PCA. The sc.pp.scale function:\n\nshifts the expression of each gene, so that the mean expression across cells is 0\nscales the expression of each gene, so that the variance across cells is 1\nThis step gives equal weight in downstream analyses, so that highly-expressed genes do not dominate. The results of this are stored as the updated count matrix at adata.X, and the original means and standard deviations for each gene are stored as metadata variables in adata.var[\"mean\"] and adata.var[\"std\"].\n\n\nsc.pp.scale(adata)\n\nExercise 6: Can you use the commands adata.X.mean() to check whether this method is successfully scaling the mean of each gene to be equal to 0, and adata.X.std() to check whether this method is successfully scaling the standard deviation of each gene to be equal to 1? Important: don’t forget to take the mean for each gene by specifying axis=0!\n\nClick for Answer\n\n\nAnswer: typing adata.X.mean(axis=0) should return:\n    array([ 9.97964231e-17,  2.44618185e-17,  3.11863822e-17, ...,\n        4.98982115e-17, -1.55931911e-17, -7.17286791e-17])\n\nIn other words, values extremely close to zero.\nadata.X.std(axis=0) should return:\n    array([0.99997257, 0.99997257, 0.99997257, ..., 0.99997257, 0.99997257,\n       0.99997257])\n\nIn other words, values extremely close to one.\n\n\n# Save the dataset\n\n\nadata.write_h5ad(\"PBMC_analysis_SIB_tutorial2.h5ad\") # feel free to choose whichever file name you prefer!"
  },
  {
    "objectID": "ipynb/day2-2_integration.html",
    "href": "ipynb/day2-2_integration.html",
    "title": "Data Integration",
    "section": "",
    "text": "import scanpy as sc\nimport pandas as pd # for handling data frames (i.e. data tables)\nimport numpy as np # for handling numbers, arrays, and matrices\nimport matplotlib.pyplot as plt # plotting package\nimport seaborn as sns # plotting package\n\nExercise 0: Before we continue in this notebook with the next steps of the analysis, we need to load our results from the previous notebook using the sc.read_h5ad function and assign them to the variable name adata. Give it a try!\n\nClick for Answer\n\n\nAnswer: Load your data from the previous notebook into an h5ad object with sc.read_h5ad\n    adata = sc.read_h5ad(\"PBMC_analysis_SIB_tutorial3.h5ad\")\n\n\nFor simple integration tasks, it is worth first trying the algorithm Harmony. The harmonypy package is a port of the original tool in R. Harmony uses a variant of singular value decomposition (SVD) to embed the data, then look for mutual nearest neighborhoods of similar cells across batches in the embedding, which it then uses to correct the batch effect in a locally adaptive (non-linear) manner.\n\nimport harmonypy as hm\n\nExercise 1: Harmonypy operates on the principal components you previous obtained using the command sc.tl.pca. These are stored in your adata object under the .obsm field as X_pca. Can you extract these and store them in a variable named mat_PC in order to provide them directly to harmonypy in the following code cell?\n\nmat_PC = adata.obsm[\"X_pca\"]\n\nNext, we run harmony on the data to correct for any batch effects. We must specify with vars_use the metadata column name in adata.obs that we would like to correct for.\n\nho = hm.run_harmony(data_mat = mat_PC, meta_data=adata.obs, vars_use=[\"sample\"])\n\n2024-05-20 14:39:55,993 - harmonypy - INFO - Computing initial centroids with sklearn.KMeans...\n2024-05-20 14:39:58,408 - harmonypy - INFO - sklearn.KMeans initialization complete.\n2024-05-20 14:39:58,445 - harmonypy - INFO - Iteration 1 of 10\n2024-05-20 14:39:59,630 - harmonypy - INFO - Iteration 2 of 10\n2024-05-20 14:40:00,821 - harmonypy - INFO - Iteration 3 of 10\n2024-05-20 14:40:01,552 - harmonypy - INFO - Iteration 4 of 10\n2024-05-20 14:40:02,182 - harmonypy - INFO - Iteration 5 of 10\n2024-05-20 14:40:02,729 - harmonypy - INFO - Iteration 6 of 10\n2024-05-20 14:40:03,304 - harmonypy - INFO - Iteration 7 of 10\n2024-05-20 14:40:03,904 - harmonypy - INFO - Iteration 8 of 10\n2024-05-20 14:40:04,484 - harmonypy - INFO - Iteration 9 of 10\n2024-05-20 14:40:05,013 - harmonypy - INFO - Iteration 10 of 10\n2024-05-20 14:40:05,611 - harmonypy - INFO - Converged after 10 iterations\n\n\nWe can then visualize the extent to which each of the principal components was rescaled by harmonypy to correct for batch effects.\n\npc_std = np.std(ho.Z_corr, axis=1).tolist()\n\n\nsns.scatterplot(x=range(0, len(pc_std)), y=sorted(pc_std, reverse=True))\n\n\n\n\n\n\n\n\nFinally, we store the results in our adata as X_pcahm variable.\n\nadata.obsm[\"X_pcahm\"] = ho.Z_corr.transpose()\n\nExercise 2: Now that we have used harmony to correct for batch effects in our principal components, we want to rerun the sc.pp.neighbors and sc.tl.umap steps from our previous exercises, to obtain a new UMAP representation. Write those commands out below to obtain your new UMAP. Important: when running sc.pp.neighbors, you must specify that you would like to use the harmony-adjusted PCs and not the original, unadjusted ones. To do this, specify the optional argument use_rep=X_pcahm!\n\nsc.pp.neighbors(adata, n_pcs=30, use_rep=\"X_pcahm\")\nsc.tl.umap(adata)\n\nExercise 3: Finally, visualize your UMAP with sc.pl.umap. How does the embedding compare to the one obtained prior to running harmony?\n\nsc.pl.umap(adata, color=[\"sample\"])\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py:394: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  cax = scatter(\n\n\n\n\n\n\n\n\n\n\nadata.write_h5ad(\"PBMC_analysis_SIB_tutorial4.h5ad\") # save your results!"
  },
  {
    "objectID": "ipynb/day2-4_visualization.html",
    "href": "ipynb/day2-4_visualization.html",
    "title": "Cell type annotation and visualization",
    "section": "",
    "text": "import scanpy as sc\nimport pandas as pd # for handling data frames (i.e. data tables)\nimport numpy as np # for handling numbers, arrays, and matrices\nimport matplotlib.pyplot as plt # plotting package\nimport seaborn as sns # plotting package\n\nExercise 0: Before we continue in this notebook with the next steps of the analysis, we need to load our results from the previous notebook using the sc.read_h5ad function and assign them to the variable name adata. Give it a try!\n\nadata = sc.read_h5ad(\"PBMC_analysis_SIB_tutorial5.h5ad\")\n\n\nadata\n\nAnnData object with n_obs × n_vars = 5465 × 3000\n    obs: 'sample', 'n_counts', 'n_genes', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_20_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'log1p_total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'log1p_total_counts_hb', 'pct_counts_hb', 'is_doublet', 'S_score', 'G2M_score', 'phase', 'leiden', 'leiden_res1', 'leiden_res0_1', 'leiden_res0_5', 'leiden_res2'\n    var: 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'mean', 'std'\n    uns: 'hvg', 'leiden', 'leiden_colors', 'leiden_res0_1_colors', 'leiden_res0_5_colors', 'leiden_res1_colors', 'leiden_res2_colors', 'log1p', 'neighbors', 'pca', 'phase_colors', 'sample_colors', 'umap'\n    obsm: 'X_pca', 'X_pcahm', 'X_umap'\n    varm: 'PCs'\n    obsp: 'connectivities', 'distances'\n\n\nBefore proceeding with marker gene analysis and cell type annotation, restore the raw version of the data, add the necessary annotations, and normalize the counts:\n\nadata_raw = sc.read_h5ad(\"PBMC_analysis_SIB_tutorial.h5ad\") # raw data before selecting highly variable genes\nshared_bcs = list(set(adata.obs.index) & set(adata_raw.obs.index))\nadata_raw = adata_raw[shared_bcs].copy()\nadata = adata[shared_bcs].copy()\nadata_raw_norm = adata_raw.copy()\nsc.pp.normalize_total(adata_raw_norm, target_sum=None)\nsc.pp.log1p(adata_raw_norm)\n\n\nadata_raw_norm.obs[\"leiden\"] = adata.obs[\"leiden_res1\"]\nadata_raw_norm.obsm[\"X_pca\"] = adata.obsm[\"X_pcahm\"]\nadata_raw_norm.obsm[\"X_umap\"] = adata.obsm[\"X_umap\"]\n\nLet’s use a simple method implemented by scanpy to find marker genes by the Leiden cluster.\n\nsc.tl.rank_genes_groups(\n    adata_raw_norm, use_raw=False, groupby=\"leiden\", method=\"wilcoxon\", key_added=\"dea_leiden\"\n)\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:396: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'names'] = self.var_names[global_indices]\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:398: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'scores'] = scores[global_indices]\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:401: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'pvals'] = pvals[global_indices]\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:411: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'pvals_adj'] = pvals_adj[global_indices]\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:422: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'logfoldchanges'] = np.log2(\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:396: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'names'] = self.var_names[global_indices]\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:398: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'scores'] = scores[global_indices]\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:401: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'pvals'] = pvals[global_indices]\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:411: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'pvals_adj'] = pvals_adj[global_indices]\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:422: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'logfoldchanges'] = np.log2(\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:396: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'names'] = self.var_names[global_indices]\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:398: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'scores'] = scores[global_indices]\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:401: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'pvals'] = pvals[global_indices]\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:411: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'pvals_adj'] = pvals_adj[global_indices]\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:422: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'logfoldchanges'] = np.log2(\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:396: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'names'] = self.var_names[global_indices]\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:398: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'scores'] = scores[global_indices]\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:401: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'pvals'] = pvals[global_indices]\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:411: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'pvals_adj'] = pvals_adj[global_indices]\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:422: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'logfoldchanges'] = np.log2(\n\n\n\nsc.settings.set_figure_params(dpi=50, facecolor='white')\nsc.pl.rank_genes_groups_dotplot(\n    adata_raw_norm, groupby=\"leiden\", standard_scale=\"var\", n_genes=5, key=\"dea_leiden\"\n)\n\nWARNING: dendrogram data not found (using key=dendrogram_leiden). Running `sc.tl.dendrogram` with default parameters. For fine tuning it is recommended to run `sc.tl.dendrogram` independently.\n\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_dotplot.py:747: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap', 'norm' will be ignored\n  dot_ax.scatter(x, y, **kwds)\n\n\n\n\n\n\n\n\n\nAs you can see above, a lot of the differentially expressed genes are highly expressed in multiple clusters. We can filter the differentially expressed genes to select for more cluster-specific differentially expressed genes:\n\nsc.tl.filter_rank_genes_groups(\n    adata_raw_norm,\n    min_in_group_fraction=0.2,\n    max_out_group_fraction=0.2,\n    key=\"dea_leiden\",\n    key_added=\"dea_leiden_filtered\",\n)\n\n\nsc.pl.rank_genes_groups_dotplot(\n    adata_raw_norm, groupby=\"leiden\", standard_scale=\"var\", n_genes=5, key=\"dea_leiden_filtered\"\n)\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_dotplot.py:747: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap', 'norm' will be ignored\n  dot_ax.scatter(x, y, **kwds)\n\n\n\n\n\n\n\n\n\nExercise 6: Visualize marker genes on the UMAP or tSNE representation. Try to find 3-4 marker genes that are indeed specific to a particular cluster. Are there any clusters that do not seem to have unique marker genes? Are there any clusters containing markers that are only specific to a portion of the cluster? Marker genes should uniformly define cells “everywhere” in a cluster in UMAP space, otherwise the cluster might actually be two!\n\nsc.pl.umap(\n    adata,\n    color=[\"CD74\", \"SSR4\", \"CA2\", \"HBA2\", \"CST3\", \"CD37\", \"IL32\", \"leiden_res0_5\"],\n    vmax=\"p99\",\n    legend_loc=\"on data\",\n    frameon=False,\n    cmap=\"coolwarm\",\n)\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py:394: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  cax = scatter(\n\n\n\n\n\n\n\n\n\nExercise 7 (optional): Let’s take a few steps back to understand all of the previous steps a little bit better! The number of genes selected by the highly_variable_genes function can significantly impact your ability to cluster. Too few genes and you cannot discriminate between different cell types, too many genes and you capture lots of noisy clusters! Try repeating the previous analysis with either 200 or 5000 highly variable genes, naming the AnnData object differently (i.e. adata_200genes) to avoid overwriting your previous results. Transfer the metadata for the new cluster labels to the original AnnData object’s metadata at adata.obs and compare on the UMAP. Are the clusters different?\nOnce you have settled on the parameters for the dimensionality reduction and clustering steps, it is time to begin annotating your clusters with cell types. This is normally a challenging step! When you are not too familiar with the marker genes for a particular cluster, a good starting point is simply to Google a strong marker gene and understand its function. Other tools that might be useful include EnrichR and GSEAPy. - https://maayanlab.cloud/Enrichr/ - https://gseapy.readthedocs.io/en/latest/gseapy_example.html#2.-Enrichr-Example\nFortunately in our case, we will try automated cell type annotations!\n\nAutomated cell type annotation\nExercise 8 The methods discussed here focus on automated data annotation, distinct from manual methods. Unlike the previously detailed approach, these methods automate data annotation. They operate on different principles, using predefined markers or trained on comprehensive scRNA-seq datasets. It’s vital to note that automated annotations can vary in quality. Thus, they should be seen as a starting point rather than a final solution. Pasquini et al., 2021 and Abdelaal et al., 2019 offer extensive discussions on automated annotation methods.\nQuality depends on:\n\nClassifier Choice: Various classifier types perform similarly, with neural networks not necessarily outperforming linear models [1, 2, 3].\nTraining Data Quality: Annotation quality relies on the quality of the training data. Poorly annotated or noisy training data can impact the classifier.\nData Similarity: Similarity between your data and the classifier’s training data matters. Cross-dataset models often provide better annotations. For example, CellTypist, trained on diverse lung datasets, is likely to perform well on new lung data.\n\nWhile classifiers have limitations, they offer advantages like rapid annotation, leveraging previous studies, and promoting standardized terminology. Ensuring robust uncertainty measures to quantify annotation reliability is crucial.\nMany classification methods rely on a limited set of genes, typically just 1 to ~10 marker genes per cell type. An alternative approach utilizes classifiers that consider a more extensive gene set, often several thousands or more. These classifiers are trained on previously annotated datasets or atlases. Notable examples include CellTypist Conde et al., 2022 and Clustifyr Fu et al., 2020.\nLet’s explore CellTypist for our data. Referring to the CellTypist tutorial, we should prepare our data by normalizing counts to 10,000 counts per cell and subsequently applying a log1p transformation. So we need to re-normalize our data, without our logarithm shift approach, but with a more classical ‘Counts per ten-thousand’.\n\nimport re\nimport celltypist\nfrom celltypist import models\n\n\nadata_celltypist = adata_raw.copy()  # make a copy of our adata\nsc.pp.normalize_per_cell(\n    adata_celltypist, counts_per_cell_after=10000.0\n)  # normalize to 10,000 counts per cell\nsc.pp.log1p(adata_celltypist)  # log-transform\n# make .X dense instead of sparse, for compatibility with celltypist:\nadata_celltypist.X = adata_celltypist.X\n\nHere we will load the model directly from our folder on google drive, where we can find the model trained. Alternatively, CellTypist method propose a panel of models that can be download directly from python using models.download_models(force_update = True). The idea is of course to use a model that match our biological context, and for pre-trained model-based method like CellTypist, it is possible that your biological context is not available. In that situation, there is no other options than opting for manual annotations.\nThere are two models that might be relevant for this particular dataset we are working with. Let’s download both of them and try each one for the classification.\n\nmodels.download_models(\n    force_update=True, model=[\"Immune_All_Low.pkl\", \"Immune_All_High.pkl\"]\n)\n\n📜 Retrieving model list from server https://celltypist.cog.sanger.ac.uk/models/models.json\n📚 Total models in list: 48\n📂 Storing models in /home/alex/.celltypist/data/models\n💾 Total models to download: 2\n💾 Downloading model [1/2]: Immune_All_Low.pkl\n💾 Downloading model [2/2]: Immune_All_High.pkl\n\n\n\nmodel_low = models.Model.load(model=\"Immune_All_Low.pkl\")\nmodel_high = models.Model.load(model=\"Immune_All_High.pkl\")\n\nFor each of these, we can see which cell types it includes to see if bone marrow cell types are included:\n\n# We can print all the cell types covererd by the model\nmodel_low.cell_types\n\narray(['Age-associated B cells', 'Alveolar macrophages', 'B cells',\n       'CD16+ NK cells', 'CD16- NK cells', 'CD8a/a', 'CD8a/b(entry)',\n       'CMP', 'CRTAM+ gamma-delta T cells', 'Classical monocytes',\n       'Cycling B cells', 'Cycling DCs', 'Cycling NK cells',\n       'Cycling T cells', 'Cycling gamma-delta T cells',\n       'Cycling monocytes', 'DC', 'DC precursor', 'DC1', 'DC2', 'DC3',\n       'Double-negative thymocytes', 'Double-positive thymocytes', 'ELP',\n       'ETP', 'Early MK', 'Early erythroid', 'Early lymphoid/T lymphoid',\n       'Endothelial cells', 'Epithelial cells', 'Erythrocytes',\n       'Erythrophagocytic macrophages', 'Fibroblasts',\n       'Follicular B cells', 'Follicular helper T cells', 'GMP',\n       'Germinal center B cells', 'Granulocytes', 'HSC/MPP',\n       'Hofbauer cells', 'ILC', 'ILC precursor', 'ILC1', 'ILC2', 'ILC3',\n       'Intermediate macrophages', 'Intestinal macrophages',\n       'Kidney-resident macrophages', 'Kupffer cells',\n       'Large pre-B cells', 'Late erythroid', 'MAIT cells', 'MEMP', 'MNP',\n       'Macrophages', 'Mast cells', 'Megakaryocyte precursor',\n       'Megakaryocyte-erythroid-mast cell progenitor',\n       'Megakaryocytes/platelets', 'Memory B cells',\n       'Memory CD4+ cytotoxic T cells', 'Mid erythroid', 'Migratory DCs',\n       'Mono-mac', 'Monocyte precursor', 'Monocytes', 'Myelocytes',\n       'NK cells', 'NKT cells', 'Naive B cells',\n       'Neutrophil-myeloid progenitor', 'Neutrophils',\n       'Non-classical monocytes', 'Plasma cells', 'Plasmablasts',\n       'Pre-pro-B cells', 'Pro-B cells',\n       'Proliferative germinal center B cells', 'Promyelocytes',\n       'Regulatory T cells', 'Small pre-B cells', 'T(agonist)',\n       'Tcm/Naive cytotoxic T cells', 'Tcm/Naive helper T cells',\n       'Tem/Effector helper T cells', 'Tem/Effector helper T cells PD1+',\n       'Tem/Temra cytotoxic T cells', 'Tem/Trm cytotoxic T cells',\n       'Transitional B cells', 'Transitional DC', 'Transitional NK',\n       'Treg(diff)', 'Trm cytotoxic T cells', 'Type 1 helper T cells',\n       'Type 17 helper T cells', 'gamma-delta T cells', 'pDC',\n       'pDC precursor'], dtype=object)\n\n\n\n# We can print all the cell types covererd by the model\nmodel_high.cell_types\n\narray(['B cells', 'B-cell lineage', 'Cycling cells', 'DC', 'DC precursor',\n       'Double-negative thymocytes', 'Double-positive thymocytes', 'ETP',\n       'Early MK', 'Endothelial cells', 'Epithelial cells',\n       'Erythrocytes', 'Erythroid', 'Fibroblasts', 'Granulocytes',\n       'HSC/MPP', 'ILC', 'ILC precursor', 'MNP', 'Macrophages',\n       'Mast cells', 'Megakaryocyte precursor',\n       'Megakaryocytes/platelets', 'Mono-mac', 'Monocyte precursor',\n       'Monocytes', 'Myelocytes', 'Plasma cells', 'Promyelocytes',\n       'T cells', 'pDC', 'pDC precursor'], dtype=object)\n\n\nThe model_high seems to have fewer cell types, let’s start with that for obtaining broader cell type categories.\n\npredictions_high = celltypist.annotate(\n    adata_celltypist, model=model_high, majority_voting=True\n)\n\n🔬 Input data has 5459 cells and 10839 genes\n🔗 Matching reference genes in the model\n🧬 3729 features used for prediction\n⚖️ Scaling input data\n🖋️ Predicting labels\n✅ Prediction done!\n👀 Can not detect a neighborhood graph, will construct one before the over-clustering\n⛓️ Over-clustering input data with resolution set to 10\n🗳️ Majority voting the predictions\n✅ Majority voting done!\n\n\n\npredictions_high_adata = predictions_high.to_adata()\npredictions_high_adata.obs[['majority_voting', 'conf_score']]\n\n\n\n\n\n\n\n\n\nmajority_voting\nconf_score\n\n\n\n\nCAACCTCCATACTACG-1\nErythroid\n0.923852\n\n\nCGTCCATTCTGAAAGA-1\nErythroid\n0.981308\n\n\nCGGACTGTCTGTCCGT-1\nErythroid\n0.986510\n\n\nCTGATCCGTCTAGCCG-1\nT cells\n0.999813\n\n\nTGTGGTAAGTGATCGG-1\nT cells\n0.954788\n\n\n...\n...\n...\n\n\nGACTGCGGTAGAAGGA-1\nT cells\n0.953285\n\n\nCGACTTCAGTTGAGAT-1\nDC\n0.959309\n\n\nAACCATGCAATCTACG-1\nT cells\n0.992116\n\n\nAACTCAGTCGTCTGCT-1\nMonocytes\n0.966532\n\n\nATTGGTGTCTACTCAT-1\nMonocytes\n0.999224\n\n\n\n\n5459 rows × 2 columns\n\n\n\n\n\nadata_raw_norm.obs[\"celltypist_annotations_high\"] = predictions_high_adata.obs[\"majority_voting\"]\nadata_raw_norm.obs[\"celltypist_conf_score_high\"] = predictions_high_adata.obs[\"conf_score\"]\n\nNow let’s do the same for the finer-grained annotations\n\npredictions_low = celltypist.annotate(\n    adata_celltypist, model=model_low, majority_voting=True\n)\n\n🔬 Input data has 5459 cells and 10839 genes\n🔗 Matching reference genes in the model\n🧬 3729 features used for prediction\n⚖️ Scaling input data\n🖋️ Predicting labels\n✅ Prediction done!\n👀 Detected a neighborhood graph in the input object, will run over-clustering on the basis of it\n⛓️ Over-clustering input data with resolution set to 10\n🗳️ Majority voting the predictions\n✅ Majority voting done!\n\n\n\npredictions_low_adata = predictions_low.to_adata()\npredictions_low_adata.obs[['majority_voting', 'conf_score']]\n\n\n\n\n\n\n\n\n\nmajority_voting\nconf_score\n\n\n\n\nCAACCTCCATACTACG-1\nLate erythroid\n0.969943\n\n\nCGTCCATTCTGAAAGA-1\nLate erythroid\n0.981763\n\n\nCGGACTGTCTGTCCGT-1\nLate erythroid\n0.993632\n\n\nCTGATCCGTCTAGCCG-1\nTcm/Naive cytotoxic T cells\n0.725759\n\n\nTGTGGTAAGTGATCGG-1\nTcm/Naive helper T cells\n0.878998\n\n\n...\n...\n...\n\n\nGACTGCGGTAGAAGGA-1\nTcm/Naive cytotoxic T cells\n0.827536\n\n\nCGACTTCAGTTGAGAT-1\nDC2\n0.125286\n\n\nAACCATGCAATCTACG-1\nTcm/Naive cytotoxic T cells\n0.270089\n\n\nAACTCAGTCGTCTGCT-1\nMonocytes\n0.882287\n\n\nATTGGTGTCTACTCAT-1\nClassical monocytes\n0.986496\n\n\n\n\n5459 rows × 2 columns\n\n\n\n\nAnd we save our predictions to our AnnData object:\n\nadata_raw_norm.obs[\"celltypist_annotations_low\"] = predictions_low_adata.obs[\"majority_voting\"]\nadata_raw_norm.obs[\"celltypist_conf_score_low\"] = predictions_low_adata.obs[\"conf_score\"]\n\nCellTypist annotations can then be visualized on the UMAP embedding:\n\nsc.settings.set_figure_params(dpi=80, facecolor='white')\nsc.pl.umap(\n    adata_raw_norm,\n    color=[\"celltypist_annotations_low\", \"celltypist_annotations_high\"],\n    frameon=False,\n    sort_order=False,\n    wspace=1.2,\n)\n\n\n\n\n\n\n\n\nAlso, each cell gets a prediction score:\n\nsc.pl.umap(\n    adata_raw_norm,\n    color=[\"celltypist_conf_score_low\", \"celltypist_conf_score_high\"],\n    frameon=False,\n    sort_order=False,\n    wspace=1,\n)\n\n\n\n\n\n\n\n\nOne way of getting a feeling for the quality of these annotations is by looking if the observed cell type similarities correspond to our expectations:\n\nsc.pl.dendrogram(adata_raw_norm, groupby=\"celltypist_annotations_low\")\n\nWARNING: dendrogram data not found (using key=dendrogram_celltypist_annotations_low). Running `sc.tl.dendrogram` with default parameters. For fine tuning it is recommended to run `sc.tl.dendrogram` independently.\n\n\n\n\n\n\n\n\n\n\n\nAnother way to annotate: with label transfer from a reference dataset!\n\n# To be added for the diseased conditions...\n\n\ndel adata_raw_norm.uns[\"dea_leiden_filtered\"]\n\n\nadata_raw_norm.write_h5ad(\"PBMC_analysis_SIB_tutorial6.h5ad\")"
  },
  {
    "objectID": "ipynb/day3-4_velocity1.html",
    "href": "ipynb/day3-4_velocity1.html",
    "title": "RNA velocity with scvelo",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport scanpy as sc\nimport scvelo as scv\nRNA velocity is a method for estimating the rate of change in gene expression in scRNA-seq dataset. Differently from pseudotime trajectory inference, where you need to specify a “root” cell, RNA velocity tells you the direction along which cells are evolving in gene expression space. RNA velocity achieves this by examining the ratio of unspliced (intronic-containing) and spliced (exonic-only) reads in a dataset.\nIn the previous notebook, we walked through the theoretical foundations behind RNA velocity. Here, we will demonstrate how to practically apply it to a dataset using the scvelo package, which is nicely integrated with the scanpy framework we have been working with during the past two days. This exercise is partially adapted from several tutorials on the scvelo documentation page: https://scvelo.readthedocs.io/\nFirst, we will load a dataset on pancreatic endocrinogenesis\nadata = scv.datasets.pancreas()\nadata\n\n\n\n\nAnnData object with n_obs × n_vars = 3696 × 27998\n    obs: 'clusters_coarse', 'clusters', 'S_score', 'G2M_score'\n    var: 'highly_variable_genes'\n    uns: 'clusters_coarse_colors', 'clusters_colors', 'day_colors', 'neighbors', 'pca'\n    obsm: 'X_pca', 'X_umap'\n    layers: 'spliced', 'unspliced'\n    obsp: 'distances', 'connectivities'\nExercise 1: As we can see from the metadata, this dataset already contains a low dimensional UMAP embedding with cluster annotations. Can you plot this UMAP embedding, coloring the cells by clusters?\nsc.pl.umap(adata, color='clusters')\nNext, we display the proportions of spliced/unspliced counts. Depending on the protocol used , we typically have between 10%-25% of unspliced molecules containing intronic sequences. For single-nuclei data, you will have many more intronic reads, approximately 60%-70%. We also advice you to examine the variations on cluster level to verify consistency in splicing efficiency. Here, we find variations as expected, with slightly lower unspliced proportions at cycling ductal cells, then higher proportion at cell fate commitment in Ngn3-high and Pre-endocrine cells where many genes start to be transcribed.\nscv.pl.proportions(adata, groupby=\"clusters\")\nNext, as with our standard scRNA-seq analysis pipeline, we need to preprocess the data! This requires performing the following steps, which you have studied in previous exercises: - Gene filtering (with a minimum number of counts per cell) - Normalization - Log transformation\nIn scvelo, these steps are combined into a single function, called scv.pp.filter_and_normalize. We will run that command below with two parameters specified: - min_shared_counts requires a minimum number of counts (both spliced and unspliced) for all genes; any other genes are filtered out - n_top_genes is similar to sc.pp.highly_variable_genes from scanpy, finding the top variable genes and filtering out the others\nscv.pp.filter_and_normalize(adata, min_shared_counts=20, n_top_genes=2000)\n\nFiltered out 20801 genes that are detected 20 counts (shared).\nNormalized count data: X, spliced, unspliced.\nExtracted 2000 highly variable genes.\nLogarithmized X.\n\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scvelo/preprocessing/utils.py:705: DeprecationWarning: `log1p` is deprecated since scVelo v0.3.0 and will be removed in a future version. Please use `log1p` from `scanpy.pp` instead.\n  log1p(adata)\nExercise 2: As we mentioned, scv.pp.filter_and_normalize combines several scanpy functions into a single command. However, if you want full control over the filtering, normalization, and log-transformation steps, you can run each command individually. Can you write the five lines of code needed to achieve the above steps?\nAnswer\nNext, we need to compute a PCA and neighborhood graph, as we have done previously.\nExercise 3: Write the commands to compute the PCA and then the neighborhood graph (using n_pcs=30 and n_neighbors=30)\nsc.pp.pca(adata)\nsc.pp.neighbors(adata, n_pcs=30, n_neighbors=30)\nNext, we need to compute the first and second order moments (means and uncentered variances) computed among nearest neighbors in PCA space, summarized in scv.pp.moments.\nscv.pp.moments(adata, n_pcs=None, n_neighbors=None)\n\ncomputing moments based on connectivities\n    finished (0:00:00) --&gt; added \n    'Ms' and 'Mu', moments of un/spliced abundances (adata.layers)\nVelocities are vectors in gene expression space and represent the direction and speed of movement of the individual cells. The velocities are obtained by modeling transcriptional dynamics of splicing kinetics, either stochastically (default) or deterministically (by setting mode=‘deterministic’). For each gene, a steady-state-ratio of pre-mature (unspliced) and mature (spliced) mRNA counts is fitted, which constitutes a constant transcriptional state. Velocities are then obtained as residuals from this ratio. Positive velocity indicates that a gene is up-regulated, which occurs for cells that show higher abundance of unspliced mRNA for that gene than expected in steady state. Conversely, negative velocity indicates that a gene is down-regulated.\nscv.tl.velocity(adata)\n\ncomputing velocities\n    finished (0:00:01) --&gt; added \n    'velocity', velocity vectors for each individual cell (adata.layers)\nThe combination of velocities across genes can then be used to estimate the future state of an individual cell. In order to project the velocities into a lower-dimensional embedding, transition probabilities of cell-to-cell transitions are estimated. That is, for each velocity vector we find the likely cell transitions that are accordance with that direction. The transition probabilities are computed using cosine correlation between the potential cell-to-cell transitions and the velocity vector, and are stored in a matrix denoted as velocity graph. The resulting velocity graph has dimension 𝑛𝑜𝑏𝑠×𝑛𝑜𝑏𝑠 and summarizes the possible cell state changes that are well explained through the velocity vectors (for runtime speedup it can also be computed on reduced PCA space by setting approx=True).\nscv.tl.velocity_graph(adata)\n\ncomputing velocity graph (using 1/160 cores)\n    finished (0:00:08) --&gt; added \n    'velocity_graph', sparse matrix with cosine correlations (adata.uns)\nFinally, the velocities are projected onto any embedding, specified by basis, and visualized in one of these ways: - on cellular level with scv.pl.velocity_embedding, - as gridlines with scv.pl.velocity_embedding_grid, - or as streamlines with scv.pl.velocity_embedding_stream.\nNote, that the data has an already pre-computed UMAP embedding, and annotated clusters. When applying to your own data, these can be obtained with scv.tl.umap and scv.tl.louvain.\nThe most fine-grained resolution of the velocity vector field we get at single-cell level, with each arrow showing the direction and speed of movement of an individual cell. That reveals, e.g., the early endocrine commitment of Ngn3-cells (yellow) and a clear-cut difference between near-terminal α-cells (blue) and transient β-cells (green).\nscv.pl.velocity_embedding(adata, arrow_length=3, arrow_size=2, color='clusters')\n\ncomputing velocity embedding\n    finished (0:00:00) --&gt; added\n    'velocity_umap', embedded velocity vectors (adata.obsm)\nThe velocity vector field displayed as streamlines yields fine-grained insights into the developmental processes. It accurately delineates the cycling population of ductal cells and endocrine progenitors. Further, it illuminates cell states of lineage commitment, cell-cycle exit, and endocrine cell differentiation.\nscv.pl.velocity_embedding_stream(adata, basis='umap', color='clusters')\nThis is perhaps the most important part as we advise the user not to limit biological conclusions to the projected velocities, but to examine individual gene dynamics via phase portraits to understand how inferred directions are supported by particular genes.\nAs we discussed in our previous exercise into the theoretical foundations of RNA velocity: Gene activity is orchestrated by transcriptional regulation. Transcriptional induction for a particular gene results in an increase of (newly transcribed) precursor unspliced mRNAs while, conversely, repression or absence of transcription results in a decrease of unspliced mRNAs. Spliced mRNAs is produced from unspliced mRNA and follows the same trend with a time lag. Time is a hidden/latent variable. Thus, the dynamics needs to be inferred from what is actually measured: spliced and unspliced mRNAs as displayed in the phase portrait.\nNow, let us examine the phase portraits of some marker genes, visualized with scv.pl.velocity(adata, gene_names) or scv.pl.scatter(adata, gene_names).\nscv.pl.velocity(adata, ['Cpe',  'Gnao1', 'Ins2', 'Adk'], ncols=2)\nExercise 4: Describe the plots above: what can you tell me about the patterns of the four genes, Cpe, Gnao1, Ins2, Adk, along the differentiation trajectory. Transitioning from ductal cells to mature alpha and beta cells, which genes are being upregulated? Which genes are being downregulated along the same trajectory? Are any of the genes limited to a particular cell type or lineage?\nAnswer: Cpe explains the directionality in the up-regulated Ngn3 (yellow) to Pre-endocrine (orange) to β-cells (green), while Adk explains the directionality in the down-regulated Ductal (dark green) to Ngn3 (yellow) to the remaining endocrine cells.\nscv.pl.scatter(adata, 'Cpe', color=['clusters', 'velocity'],\n               add_outline='Ngn3 high EP, Pre-endocrine, Beta')\nExercise 5: What does the black dashed line represent in the phase portrait plots, and how does this relate to the way in which RNA velocity is determined?\nAnswer: The black line corresponds to the estimated ‘steady-state’ ratio, i.e. the ratio of unspliced to spliced mRNA abundance which is in a constant transcriptional state. RNA velocity for a particular gene is determined as the residual, i.e. how much an observation deviates from that steady-state line.\nWe need a systematic way to identify genes that may help explain the resulting vector field and inferred lineages. To do so, we can test which genes have cluster-specific differential velocity expression, being siginificantly higher/lower compared to the remaining population. The module scv.tl.rank_velocity_genes runs a differential velocity t-test and outpus a gene ranking for each cluster. Thresholds can be set (e.g. min_corr) to restrict the test on a selection of gene candidates.\nscv.tl.rank_velocity_genes(adata, groupby='clusters', min_corr=.3)\n\ndf = pd.DataFrame(adata.uns['rank_velocity_genes']['names'])\ndf.head()\n\nranking velocity genes\n    finished (0:00:03) --&gt; added \n    'rank_velocity_genes', sorted scores by group ids (adata.uns) \n    'spearmans_score', spearmans correlation scores (adata.var)\n\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scvelo/tools/utils.py:463: DeprecationWarning: Please use `rankdata` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n  from scipy.stats.stats import rankdata\n\n\n\n\n\n\n\n\n\n\nDuctal\nNgn3 low EP\nNgn3 high EP\nPre-endocrine\nBeta\nAlpha\nDelta\nEpsilon\n\n\n\n\n0\nNotch2\nPtpn3\nPde1c\nBaiap3\nPax6\nZcchc16\nZdbf2\nHeg1\n\n\n1\nSox5\nHspa8\nPclo\nPam\nUnc5c\nNell1\nPtprt\nIca1\n\n\n2\nHspa8\nDcbld1\nPtprs\nSdk1\nNnat\nKsr2\nAkr1c19\nTmcc3\n\n\n3\nKrt19\nGrb10\nRap1gap2\nAbcc8\nKcnmb2\nPrune2\nAnk2\nGpr179\n\n\n4\nNr2f6\nHacd1\nKcnb2\nPtprn2\nScg3\nNdst4\nSpock3\nMamld1\nkwargs = dict(frameon=False, size=10, linewidth=1.5,\n              add_outline='Ngn3 high EP, Pre-endocrine, Beta')\n\nscv.pl.scatter(adata, df['Ngn3 high EP'][:5], ylabel='Ngn3 high EP', **kwargs)\nscv.pl.scatter(adata, df['Pre-endocrine'][:5], ylabel='Pre-endocrine', **kwargs)\nThe genes Ptprs, Pclo, Pam, Abcc8, Gnas, for instance, support the directionality from Ngn3 high EP (yellow) to Pre-endocrine (orange) to Beta (green)."
  },
  {
    "objectID": "ipynb/day3-4_velocity1.html#dynamical-modeling-of-rna-velocity",
    "href": "ipynb/day3-4_velocity1.html#dynamical-modeling-of-rna-velocity",
    "title": "RNA velocity with scvelo",
    "section": "Dynamical modeling of RNA velocity",
    "text": "Dynamical modeling of RNA velocity\nSince RNA velocity yields insights into the directionality of gene expression change, we can use the approach to infer a trajectory. One way this is acheives is by recovering estimates of the full transcriptional dynamics (i.e., the transcription rate, the splicing rate, and the degradation rate) instead of using the steady-state asusmption and linear fits. This is particularly useful when you have a dataset without a cluster of cells representing the “steady-state”.\nDynamical modeling of RNA velocity is possible with scvelo and allows for: - Estimation of a latent time - Identification of possible driver genes\nWe run the dynamical model to learn the full transcriptional dynamics of splicing kinetics.\nIt is solved in a likelihood-based expectation-maximization framework, by iteratively estimating the parameters of reaction rates and latent cell-specific variables, i.e. transcriptional state and cell-internal latent time. It thereby aims to learn the unspliced/spliced phase trajectory for each gene.\n\nscv.tl.recover_dynamics(adata)\n\nrecovering dynamics (using 1/160 cores)\n    finished (0:06:26) --&gt; added \n    'fit_pars', fitted parameters for splicing dynamics (adata.var)\n\n\n\n\n\nThen, we before we need to estiamte the velocity and compute the velocity graph, specifying this time the “dynamical” mode.\n\nscv.tl.velocity(adata, mode='dynamical')\nscv.tl.velocity_graph(adata)\n\ncomputing velocities\n    finished (0:00:06) --&gt; added \n    'velocity', velocity vectors for each individual cell (adata.layers)\ncomputing velocity graph (using 1/160 cores)\n    finished (0:00:07) --&gt; added \n    'velocity_graph', sparse matrix with cosine correlations (adata.uns)\n\n\n\n\n\nRunning the dynamical model can take a while (up to 10 mins for this dataset)\n\nscv.pl.velocity_embedding_stream(adata, basis='umap')\n\ncomputing velocity embedding\n    finished (0:00:00) --&gt; added\n    'velocity_umap', embedded velocity vectors (adata.obsm)\n\n\n\n\n\n\n\n\n\nThe rates of RNA transcription, splicing and degradation are estimated without the need of any experimental data.\nThey can be useful to better understand the cell identity and phenotypic heterogeneity.\n\ndf = adata.var\ndf = df[(df['fit_likelihood'] &gt; .1) & df['velocity_genes'] == True]\n\nkwargs = dict(xscale='log', fontsize=16)\nwith scv.GridSpec(ncols=3) as pl:\n    pl.hist(df['fit_alpha'], xlabel='transcription rate', **kwargs)\n    pl.hist(df['fit_beta'] * df['fit_scaling'], xlabel='splicing rate', xticks=[.1, .4, 1], **kwargs)\n    pl.hist(df['fit_gamma'], xlabel='degradation rate', xticks=[.1, .4, 1], **kwargs)\n\nscv.get_df(adata, 'fit*', dropna=True).head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfit_alpha\nfit_beta\nfit_gamma\nfit_t_\nfit_scaling\nfit_std_u\nfit_std_s\nfit_likelihood\nfit_u0\nfit_s0\nfit_pval_steady\nfit_steady_u\nfit_steady_s\nfit_variance\nfit_alignment_scaling\nfit_r2\n\n\nindex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSntg1\n0.011041\n0.004043\n0.080231\n25.527206\n52.120241\n1.021111\n0.023619\n0.367464\n0.0\n0.0\n0.008509\n2.507244\n0.075472\n0.259419\n6.120456\n0.466323\n\n\nSbspon\n0.225797\n1.682173\n0.302747\n6.147519\n0.463775\n0.057540\n0.175868\n0.242276\n0.0\n0.0\n0.215998\n0.162373\n0.481733\n0.794976\n1.816960\n0.651425\n\n\nMcm3\n4.098365\n59.257791\n1.203317\n1.986274\n0.012154\n0.015394\n0.687152\n0.125647\n0.0\n0.0\n0.480542\n0.060191\n1.991981\n0.985103\n0.717393\n0.282687\n\n\nFam135a\n0.169396\n0.113233\n0.185499\n11.025427\n1.057786\n0.350119\n0.153911\n0.271898\n0.0\n0.0\n0.414863\n1.239550\n0.397954\n0.730658\n3.569145\n0.362124\n\n\nAdgrb3\n0.039118\n0.007863\n0.195750\n7.880056\n118.323538\n2.063335\n0.028773\n0.362395\n0.0\n0.0\n0.072837\n4.881226\n0.095819\n0.380088\n1.773425\n0.376199\n\n\n\n\n\n\n\n\nThe dynamical model recovers the latent time of the underlying cellular processes. This latent time represents the cell’s internal clock and approximates the real time experienced by cells as they differentiate, based only on its transcriptional dynamics. This offers advantages over traditional pseudotime trajectory inference approaches.\n\nscv.tl.latent_time(adata)\nscv.pl.scatter(adata, color='latent_time', color_map='gnuplot', size=80)\n\ncomputing terminal states\n    identified 1 region of root cells and 1 region of end points .\n    finished (0:00:00) --&gt; added\n    'root_cells', root cells of Markov diffusion process (adata.obs)\n    'end_points', end points of Markov diffusion process (adata.obs)\ncomputing latent time using root_cells as prior\n    finished (0:00:01) --&gt; added \n    'latent_time', shared time (adata.obs)\n\n\n\n\n\n\n\n\n\nDriver genes display pronounced dynamic behavior and are systematically detected via their characterization by high likelihoods in the dynamic model. We can plot a heatmap of the top 300 genes expressed along the pseudotime.\n\ntop_genes = adata.var['fit_likelihood'].sort_values(ascending=False).index[:300]\nscv.pl.heatmap(adata, var_names=top_genes, sortby='latent_time', col_color='clusters', n_convolve=100)\n\n\n\n\n\n\n\n\nFor any top candidates that you might want to validate biologically, it is always essential to examine the phase portraits, to ensure that the gene is not too noisy.\n\ntop_genes = adata.var['fit_likelihood'].sort_values(ascending=False).index\nscv.pl.scatter(adata, basis=top_genes[:15], ncols=5, frameon=False)\n\n\n\n\n\n\n\n\n\nvar_names = ['Actn4', 'Ppp3ca', 'Cpe', 'Nnat']\nscv.pl.scatter(adata, var_names, frameon=False)\nscv.pl.scatter(adata, x='latent_time', y=var_names, frameon=False)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nscv.tl.rank_dynamical_genes(adata, groupby='clusters')\ndf = scv.get_df(adata, 'rank_dynamical_genes/names')\ndf.head(5)\n\nranking genes by cluster-specific likelihoods\n    finished (0:00:03) --&gt; added \n    'rank_dynamical_genes', sorted scores by group ids (adata.uns)\n\n\n\n\n\n\n\n\n\n\nDuctal\nNgn3 low EP\nNgn3 high EP\nPre-endocrine\nBeta\nAlpha\nDelta\nEpsilon\n\n\n\n\n0\nNfib\nTop2a\nGnas\nAbcc8\nPcsk2\nPak3\nPcsk2\nTox3\n\n\n1\nDcdc2a\nDcdc2a\nRbfox3\nPpp3ca\nAnk\nGnao1\nPak3\nMeis2\n\n\n2\nTop2a\nAdk\nBtbd17\nRap1b\nScgn\nCpe\nRap1b\nRnf130\n\n\n3\nIncenp\nRap1gap2\nTcp11\nGnas\nTspan7\nRph3al\nMeis2\nAdk\n\n\n4\nShank2\nTpx2\nMapre3\nTox3\nMap1b\nRap1b\nMap1b\nRap1b\n\n\n\n\n\n\n\n\n\nfor cluster in ['Ductal', 'Ngn3 high EP', 'Pre-endocrine', 'Beta']:\n    scv.pl.scatter(adata, df[cluster][:5], ylabel=cluster, frameon=False)"
  },
  {
    "objectID": "ipynb/day3-4_velocity1.html#velocities-in-cycling-progenitors",
    "href": "ipynb/day3-4_velocity1.html#velocities-in-cycling-progenitors",
    "title": "RNA velocity with scvelo",
    "section": "Velocities in cycling progenitors",
    "text": "Velocities in cycling progenitors\nThe cell cycle detected by RNA velocity, and it is biologically affirmed by cell cycle scores (standardized scores of mean expression levels of phase marker genes).\n\nscv.tl.score_genes_cell_cycle(adata)\nscv.pl.scatter(adata, color_gradients=['S_score', 'G2M_score'], smooth=True, perc=[5, 95])\n\ncalculating cell cycle phase\n--&gt;     'S_score' and 'G2M_score', scores of cell cycle phases (adata.obs)\n\n\n\n\n\n\n\n\n\nFor the cycling Ductal cells, we may screen through S and G2M phase markers. The previous module also computed a spearmans correlation score, which we can use to rank/sort the phase marker genes to then display their phase portraits.\n\ns_genes, g2m_genes = scv.utils.get_phase_marker_genes(adata)\ns_genes = scv.get_df(adata[:, s_genes], 'spearmans_score', sort_values=True).index\ng2m_genes = scv.get_df(adata[:, g2m_genes], 'spearmans_score', sort_values=True).index\n\nkwargs = dict(frameon=False, ylabel='cell cycle genes')\nscv.pl.scatter(adata, list(s_genes[:2]) + list(g2m_genes[:3]), **kwargs)\n\n\n\n\n\n\n\n\nParticularly Hells and Top2a are well-suited to explain the vector field in the cycling progenitors. Top2a gets assigned a high velocity shortly before it actually peaks in the G2M phase. There, the negative velocity then perfectly matches the immediately following down-regulation.\n\nscv.pl.velocity(adata, ['Hells', 'Top2a'], ncols=2, add_outline=True)\n\n\n\n\n\n\n\n\nThe cell cycle is an interesting case for RNA velocity estimation, as pseudotime methods along often fail as estimations of cyclical processes. Moreover, RNA velocity corresponds roughly to cell cycle speed, which is both experimentally verifiable. The cell cycle also unfolds on a timescale of less than 24 hours, which is well suited for studying cell dynamics using RNA lifecycle kinetics, such as with RNA velocity."
  }
]