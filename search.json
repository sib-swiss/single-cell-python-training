[
  {
    "objectID": "ipynb/day3-4_velocity2.html",
    "href": "ipynb/day3-4_velocity2.html",
    "title": "RNA velocity with VeloCycle",
    "section": "",
    "text": "Download Presentation: VeloCycle\nThe cell cycle is an interesting case for RNA velocity estimation, as pseudotime methods along often fail as estimations of cyclical processes. Moreover, RNA velocity corresponds roughly to cell cycle speed, which is both experimentally verifiable. The cell cycle also unfolds on a timescale of less than 24 hours, which is well suited for studying cell dynamics using RNA lifecycle kinetics, such as with RNA velocity.\nA recent method has been developed called VeloCycle to estimate RNA velocity of the cell cycle on the real time scale. This method offers several advantages over existing approaches: - The ability to estimate uncertainty of velocity estimates (i.e. velocity confidence). - The ability to estimate both the low dimensional manifold and the velocity jointly. - The ability to perform statistical tests of velocity between conditions. - The ability to convert velocity estimates to a “real” time scale.\nComparing cell cycle velocities might be useful in a variable of scientific contexts: - Do two cancer subtumors proliferate as similar speeds? - Does a particular gene knockout or mutant impact the cell cycle speed? - Do progenitor cells in different regions of an organ (i.e., brain) or at different developmental stages divide equally quickly?\nHere, we will offer a short tutorial into VeloCycle, using the ductal cells from the pancreas dataset above. This will also offer insight into probabilistic modeling in Pyro, which is an advanced method used by many tools for modeling complex biological data.\nimport velocycle as vcy\nimport scvelo as scv\nimport scanpy as sc\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom velocycle import *\nimport anndata\nimport pyro\nimport torch\nimport copy\nadata_raw = scv.datasets.pancreas()\nadata_cycling = adata_raw[adata_raw.obs[\"clusters\"].isin([\"Ductal\"])].copy()\nadata_cycling\n\nAnnData object with n_obs × n_vars = 916 × 27998\n    obs: 'clusters_coarse', 'clusters', 'S_score', 'G2M_score'\n    var: 'highly_variable_genes'\n    uns: 'clusters_coarse_colors', 'clusters_colors', 'day_colors', 'neighbors', 'pca'\n    obsm: 'X_pca', 'X_umap'\n    layers: 'spliced', 'unspliced'\n    obsp: 'distances', 'connectivities'"
  },
  {
    "objectID": "ipynb/day3-4_velocity2.html#load-and-filter-dataset",
    "href": "ipynb/day3-4_velocity2.html#load-and-filter-dataset",
    "title": "RNA velocity with VeloCycle",
    "section": "Load and filter dataset",
    "text": "Load and filter dataset\n\nadata = adata_cycling.copy()\n\n\nsc.pl.umap(adata, color='clusters')\n\n\n\n\n\n\n\n\n\nfull_adatas = {\"pancreas_ductal\":adata[adata.obs[\"clusters\"].isin([\"Ductal\"])].copy()}\n\n\n# Filter lowly-expressed genes and concatenate all datasets\nfor a in full_adatas.keys(): \n    print(full_adatas[a].shape)\n    sc.pp.filter_genes(full_adatas[a], min_cells=int((full_adatas[a].n_obs)*0.10))\n    \ndata = anndata.concat(full_adatas, label=\"batch\", join =\"outer\")\n\n(916, 27998)\n\n\n\n# Perform some very basic gene filtering by unspliced counts\ndata = data[:, (data.layers[\"unspliced\"].toarray().mean(0) &gt; 0.1)].copy()\n\n# Perform some very basic gene filtering by spliced counts\ndata = data[:, data.layers[\"spliced\"].toarray().mean(0) &gt; 0.2].copy()\n\n\ndata.var.index = [i.upper() for i in data.var.index]\ndata\n\nAnnData object with n_obs × n_vars = 916 × 1394\n    obs: 'clusters_coarse', 'clusters', 'S_score', 'G2M_score', 'batch'\n    obsm: 'X_pca', 'X_umap'\n    layers: 'spliced', 'unspliced'\n\n\n\n# Create design matrix for dataset with a single batch\nbatch_design_matrix = preprocessing.make_design_matrix(data, ids=\"batch\")\n\n\n# Rough approximation of the cell cycle phase using categorical approaches \nsc.tl.score_genes_cell_cycle(data, s_genes=utils.S_genes_human, g2m_genes=utils.G2M_genes_human)\n\nWARNING: genes are not in var_names and ignored: ['BLM', 'BRIP1', 'CASP8AP2', 'CCNE2', 'CDC45', 'CDC6', 'CDCA7', 'CHAF1B', 'CLSPN', 'DSCC1', 'DTL', 'E2F8', 'EXO1', 'FEN1', 'GINS2', 'GMNN', 'MCM2', 'MCM4', 'MCM5', 'MCM6', 'MLF1IP', 'MSH2', 'PCNA', 'POLD3', 'RAD51', 'RAD51AP1', 'RPA2', 'RRM1', 'RRM2', 'SLBP', 'UBR7', 'UHRF1', 'UNG', 'USP1', 'WDR76']\nWARNING: genes are not in var_names and ignored: ['ANLN', 'AURKA', 'AURKB', 'BIRC5', 'BUB1', 'CCNB2', 'CDC20', 'CDC25C', 'CDCA3', 'CDCA8', 'CENPA', 'CENPF', 'CKAP2L', 'CKS1B', 'CTCF', 'DLGAP5', 'ECT2', 'FAM64A', 'G2E3', 'GAS2L3', 'GTSE1', 'HJURP', 'HMGB2', 'HMMR', 'HN1', 'KIF20B', 'KIF2C', 'LBR', 'MKI67', 'NDC80', 'NEK2', 'NUF2', 'PSRC1', 'TACC3', 'TMPO', 'TTK', 'TUBB4B', 'UBE2C']\n\n\n\n# Create size-normalized data layers\npreprocessing.normalize_total(data)\n\n\n# Get biologically-relevant gene set to use for velocity estimation\nfull_keep_genes = utils.get_cycling_gene_set(size=\"Medium\", species=\"Human\")"
  },
  {
    "objectID": "ipynb/day3-4_velocity2.html#initialize-cycle-and-phase-objects-with-priors",
    "href": "ipynb/day3-4_velocity2.html#initialize-cycle-and-phase-objects-with-priors",
    "title": "RNA velocity with VeloCycle",
    "section": "Initialize cycle and phase objects with priors",
    "text": "Initialize cycle and phase objects with priors\n\nn_harm = 1\ncycle_prior = cycle.Cycle.trivial_prior(gene_names=full_keep_genes, harmonics=n_harm)\ncycle_prior, data_to_fit = preprocessing.filter_shared_genes(cycle_prior, data, filter_type=\"intersection\")\n\n\n# Update the priors for gene harmonics\n# to gene-specific means and stds\nS = data_to_fit.layers['spliced'].toarray()\nS_means = S.mean(axis=0) #sum over cells\nnu0 = np.log(S_means)\nnu0std = np.std(np.log(S+1), axis=0)/2\n\nS_frac_means=np.vstack((nu0, 0*nu0, 0*nu0))\ncycle_prior.set_means(S_frac_means)\n\nS_frac_stds=np.vstack((nu0std, 0.5*nu0std, 0.5*nu0std))\ncycle_prior.set_stds(S_frac_stds)\n\n\n# Obtain a PCA prior for individual cell phases\nphase_prior = phases.Phases.from_pca_heuristic(data_to_fit, \n                                               concentration=5.0, \n                                               plot=True, \n                                               small_count=1)\n\n\n\n\n\n\n\n\n\n# Shift the phase prior to have maximum correlation with the total raw UMI counts\n(shift, maxcor, allcor) = phase_prior.max_corr(data_to_fit.obs.n_scounts)\nphase_prior.rotate(angle=-shift)\nplt.plot(phase_prior.phis, data_to_fit.obs.n_scounts, '.', c='black')\nplt.xlim(0, np.pi*2)\nplt.xticks([0, np.pi, 2*np.pi],[\"0\", \"π\", \"2π\"])\nplt.xlabel(\"PCA Phase Prior\")\nplt.ylabel(\"Raw Spliced UMIs\")\nplt.show()"
  },
  {
    "objectID": "ipynb/day3-4_velocity2.html#run-the-manifold-learning-module",
    "href": "ipynb/day3-4_velocity2.html#run-the-manifold-learning-module",
    "title": "RNA velocity with VeloCycle",
    "section": "Run the manifold-learning module",
    "text": "Run the manifold-learning module\n\npyro.clear_param_store()\n\n\n# Set batch effect to zero because there is only a single dataset/batch\nΔν = torch.zeros((batch_design_matrix.shape[1], S.shape[1], 1)).float()\ncondition_on_dict = {\"Δν\": Δν}\n\n\nmetapar = preprocessing.preprocess_for_phase_estimation(anndata=data_to_fit, \n                                          cycle_obj=cycle_prior, \n                                          phase_obj=phase_prior, \n                                          design_mtx=batch_design_matrix,\n                                          n_harmonics=n_harm,\n                                          condition_on=condition_on_dict)\n\n\nphase_fit = phase_inference_model.PhaseFitModel(metaparams=metapar, \n                                                condition_on=condition_on_dict)\nphase_fit.check_model()\n\n Trace Shapes:                          \n  Param Sites:                          \n Sample Sites:                          \n    cells dist            |             \n         value        916 |             \n    genes dist            |             \n         value         61 |             \n  batches dist            |             \n         value          1 |             \n        ν dist     61   1 |   3         \n         value     61   1 |   3         \n       Δν dist   1 61   1 |             \n         value   1 61   1 |             \n      ϕxy dist        916 |   2         \n         value        916 |   2         \n        ϕ dist            | 916         \n         value            | 916         \n        ζ dist            | 916 3       \n         value            | 916 3       \n    ElogS dist            |   1 1 61 916\n         value            |   1 1 61 916\nshape_inv dist     61   1 |             \n         value     61   1 |             \n        S dist 1 1 61 916 |             \n         value     61 916 |             \n\n\n\nnum_steps = 1000\ninitial_lr = 0.03\nfinal_lr = 0.005\ngamma = final_lr / initial_lr\nlrd = gamma ** (1 / num_steps)\nadam = pyro.optim.ClippedAdam({'lr': initial_lr, 'lrd': lrd, 'betas': (0.80, 0.99)})\n\nphase_fit.fit(optimizer=adam, num_steps=num_steps)"
  },
  {
    "objectID": "ipynb/day3-4_velocity2.html#visualize-the-results",
    "href": "ipynb/day3-4_velocity2.html#visualize-the-results",
    "title": "RNA velocity with VeloCycle",
    "section": "Visualize the results",
    "text": "Visualize the results\n\n# Put estimations in new objects\ncycle_pyro = phase_fit.cycle_pyro\nphase_pyro = phase_fit.phase_pyro\n\n\nfit_ElogS = phase_fit.posterior[\"ElogS\"].squeeze().numpy()\nfit_ElogS2 = phase_fit.posterior[\"ElogS2\"].squeeze().numpy()\n\n\nname2color = {'G1':\"tab:blue\", 'S':\"tab:orange\", 'G2M':\"tab:green\"}\ngene_list = [\"CDK1\", \"HELLS\", \"SON\", \"TOP2A\", \"HAT1\"]\ngene_names = np.array(data_to_fit.var.index)\nplt.figure(None,(24, 4))\nix = 1\nfor i in range(0, len(gene_list)):\n    g = gene_list[i]\n    plt.subplot(1, len(gene_list), ix)\n    plt.scatter(phase_pyro.phis, \n                metapar.S[np.where(gene_names==g)[0][0], :].squeeze().cpu().numpy(), \n                s=10, alpha=0.5, c=[name2color[x] for x in data_to_fit.obs[\"phase\"]])\n    plt.scatter(phase_pyro.phis, \n                np.exp(fit_ElogS2[np.where(gene_names==g)[0][0], :]), \n                s=10, c=\"black\")\n    plt.title(g)\n    plt.xlabel(\"ϕ\")\n    plt.ylabel(\"counts\")\n    ix+=1\n    plt.xticks([0, np.pi/2, np.pi, 3*np.pi/2, 2*np.pi],[\"0\", \"π/2\", \"π\", \"3π/2\", \"2π\"])\n    plt.xlim(0, 2*np.pi)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nxs = phase_fit.fourier_coef[1]\nys = phase_fit.fourier_coef[2]\nr = np.log10( np.sqrt(xs**2+ys**2) / phase_fit.fourier_coef_sd[1:, :].sum(0) )\nangle = np.arctan2(xs, ys)\nangle = (angle)%(2*np.pi)\n\nphis_df = pd.DataFrame([angle, r])\nphis_df.columns = data_to_fit.var.index\n\nphase_data_frame = pd.concat([phase_fit.cycle_pyro.means, phase_fit.cycle_pyro.stds, phis_df]).T\nphase_data_frame.columns = [\"nu0 mean\", \"nu1sin mean\", \"nu1cos mean\",\n                            \"nu0 std\", \"nu1sin std\", \"nu1cos std\", \"peak_phase\", \"amplitude\"]\nphase_data_frame[\"is_seurat_marker\"] = [True if i in list(utils.S_genes_human)+list(utils.G2M_genes_human) else False for i in phase_data_frame.index]\nphase_data_frame.head()\n\nphis_df = pd.DataFrame(phase_fit.phase_pyro.phis.numpy())\nphis_df.index = data_to_fit.obs.index\nphis_df.columns = [\"cell_cycle_phi\"]\nphase_data_frame_cells = data_to_fit.obs.merge(phis_df, left_index=True, right_index=True)\n\n\n# Define the number of bins\nnum_bins = 10\nbin_width = 2 * np.pi / num_bins\n\n# Calculate the bin index for each gene\nphase_data_frame['bin_index'] = ((phase_data_frame['peak_phase'] + 2 * np.pi) % (2 * np.pi) / bin_width).astype(int)\n\n# Group genes by bin index and find top 10 genes in each bin\ntop_genes_per_bin = phase_data_frame.groupby('bin_index', group_keys=False).apply(lambda group: group.nlargest(5, 'amplitude'))\n\n\nkeep_genes = [a.upper() for a in cycle_prior.means.columns]\n\ngene_names = np.array(keep_genes)\n\nfrom matplotlib.colors import ListedColormap, LinearSegmentedColormap\nimport matplotlib.transforms as mtransforms\nimport seaborn as sns\n\nkeep_genes = [a.upper() for a in cycle_prior.means.columns]\ngene_names = np.array(keep_genes)\nS_genes_human = list(utils.S_genes_human)\nG2M_genes_human = list(utils.G2M_genes_human)\nphases_list = [S_genes_human, G2M_genes_human, [i.upper() for i in gene_names if i.upper() not in S_genes_human+G2M_genes_human]]\n\ng = []\ngradient = []\nfor i in range(len(phases_list)):\n    for j in range(len(phases_list[i])):\n        g.append(phases_list[i][j])\n        gradient.append(i)\n\ncolor_gradient_map = pd.DataFrame({'Gene': g,  'Color': gradient}).set_index('Gene').to_dict()['Color']\ncolored_gradient = pd.Series(gene_names).map(color_gradient_map)\n\nxs = phase_fit.fourier_coef[1]\nys = phase_fit.fourier_coef[2]\nr = np.log10( np.sqrt(xs**2+ys**2) / phase_fit.fourier_coef_sd[1:, :].sum(0) )\nangle = np.arctan2(xs, ys)\nangle = (angle)%(2*np.pi)\n\nN=50\nwidth = (2*np.pi) / N\n\nfig = plt.figure(figsize = (6, 6))\nax = fig.add_subplot(projection='polar')\n\n# First: only plot dots with a color assignment\nangle_subset = angle[~np.isnan(colored_gradient.values)]\nr_subset = r[~np.isnan(colored_gradient.values)]\ncolor_subset = colored_gradient.values[~np.isnan(colored_gradient.values)]\n\n# Remove genes with very low expression\nangle_subset = angle_subset[r_subset&gt;=-12]\ncolor_subset = color_subset[r_subset&gt;=-12]\ngene_names_subset = gene_names[r_subset&gt;=-12]\nr_subset = r_subset[r_subset&gt;=-12]\n\nx=100\n# Take a subset of most highly expressing genes to print the names \nangle_subset_best = angle_subset[r_subset&gt;np.percentile(r_subset, x)]\ncolor_subset_best = color_subset[r_subset&gt;=np.percentile(r_subset, x)]\ngene_names_subset_best = gene_names_subset[r_subset&gt;=np.percentile(r_subset, x)]\nr_subset_best = r_subset[r_subset&gt;=np.percentile(r_subset, x)]\n\n# Plot all genes in phases list\nnum2color = {0:\"tab:orange\", 1:\"tab:green\", 2:\"tab:grey\", 3:\"tab:blue\"}\nax.scatter(angle_subset, r_subset, c=[num2color[i] for i in color_subset], s=50, alpha=0.3, edgecolor='none', rasterized=True)\n\n# Select and plot on top the genes marking S and G2M traditionally\nangle_subset = angle_subset[color_subset!=2]\nr_subset = r_subset[color_subset!=2]\ngene_names_subset = gene_names_subset[color_subset!=2]\ncolor_subset = color_subset[color_subset!=2]\n\nax.scatter(angle_subset, r_subset, c=[num2color[i] for i in color_subset], s=50, alpha=1, edgecolor='none',rasterized=True)\n\n# Annotate genes\nfor (i, txt), c in zip(enumerate(gene_names), colored_gradient.values):\n    if txt in top_genes_per_bin.index:\n        ix = np.where(np.array(gene_names)==txt)[0][0]\n        ax.annotate(txt[0]+txt[1:].upper(), (angle[ix], r[ix]+0.02))\n\nplt.xlim(0, 2*np.pi)\nplt.ylim(-1, )\nplt.yticks([-1, -0.5, 0, 0.5, 1], size=15)\nplt.xticks([0, np.pi/2, np.pi, 3*np.pi/2, 2*np.pi],[\"0\", \"π/2\", \"π\", \"3π/2\", \"2π\"], size=15)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "ipynb/day3-4_velocity2.html#run-the-velocity-learning-module",
    "href": "ipynb/day3-4_velocity2.html#run-the-velocity-learning-module",
    "title": "RNA velocity with VeloCycle",
    "section": "Run the velocity-learning module",
    "text": "Run the velocity-learning module\n\npyro.clear_param_store()\n\n\ncondition_design_matrix = copy.deepcopy(batch_design_matrix)\n\n\nn_velo_harmonics = 0\nspeed_prior = angularspeed.AngularSpeed.trivial_prior(condition_names=[\"pancreas_ductal\"], \n                                                      harmonics=n_velo_harmonics)\n\n\ncondition_on_dict = {\"ϕxy\":phase_pyro.phi_xy_tensor.T,\n                     \"ν\": cycle_pyro.means_tensor.T.unsqueeze(-2),\n                     \"Δν\": torch.tensor(phase_fit.delta_nus),\n                     \"shape_inv\": torch.tensor(phase_fit.disp_pyro).unsqueeze(-1)}\n\n\nmetaparameters_velocity = preprocessing.preprocess_for_velocity_estimation(data_to_fit, \n                                                             cycle_pyro, \n                                                             phase_pyro, \n                                                             speed_prior,\n                                                             condition_design_matrix.float(), \n                                                             batch_design_matrix.float(), \n                                                             n_harmonics=n_harm,\n                                                             count_factor=metapar.count_factor,\n                                                             ω_n_harmonics=n_velo_harmonics, \n                                                             μγ=torch.tensor(0.0).detach().clone().float(),\n                                                             σγ=torch.tensor(0.5).detach().clone().float(),\n                                                             μβ=torch.tensor(2.0).detach().clone().float(),\n                                                             σβ=torch.tensor(3.0).detach().clone().float(),\n                                                             model_type=\"lrmn\",\n                                                             condition_on=condition_on_dict)\n\n\nvelocity_fit = velocity_inference_model.VelocityFitModel(metaparams=metaparameters_velocity, \n                                                         condition_on=condition_on_dict)\n\n\nvelocity_fit.check_model()\n\n  Trace Shapes:                              \n   Param Sites:                              \n  Sample Sites:                              \n     cells dist              |               \n          value          916 |               \n     genes dist              |               \n          value           61 |               \n harmonics dist              |               \n          value            1 |               \nconditions dist              |               \n          value            1 |               \n   batches dist              |               \n          value            1 |               \n     logγg dist       61   1 |               \n          value       61   1 |               \n     logβg dist       61   1 |               \n          value       61   1 |               \n  rho_real dist       61   1 |               \n          value       61   1 |               \n        γg dist       61   1 |  61   1       \n          value              |  61   1       \n         ν dist       61   1 |   3           \n          value       61   1 |   3           \n        Δν dist 1 1 1 61   1 |               \n          value 1 1 1 61   1 |               \n       ϕxy dist          916 |   2           \n          value          916 |   2           \n         ϕ dist              | 916           \n          value              | 916           \n         ζ dist              | 916   3       \n          value              | 916   3       \n      ζ_dϕ dist              | 916   3       \n          value              | 916   3       \n        νω dist   1 1  1   1 |               \n          value   1 1  1   1 |               \n        ζω dist              |   1 916       \n          value              |   1 916       \n     ElogS dist              |   1   1 61 916\n          value              |   1   1 61 916\n         ω dist              |   1 916       \n          value              |   1 916       \n     ElogU dist              |   1   1 61 916\n          value              |   1   1 61 916\n shape_inv dist       61   1 |               \n          value       61   1 |               \n         S dist   1 1 61 916 |               \n          value       61 916 |               \n         U dist   1 1 61 916 |               \n          value       61 916 |               \n\n\n\nvelocity_fit.check_guide()\n\n  Trace Shapes:              \n   Param Sites:              \n         ν_locs     61   1  3\n       ν_scales     61   1  3\n        Δν_locs 1 1  1  61  1\n       ϕxy_locs        916  2\n     logβg_locs         61  1\n   logβg_scales         61  1\n            loc            62\n     cov_factor         62  5\n       cov_diag            62\n   rho_real_loc            61\n shape_inv_locs         61  1\n  Sample Sites:              \n     cells dist             |\n          value        916  |\n     genes dist             |\n          value         61  |\n harmonics dist             |\n          value          1  |\nconditions dist             |\n          value          1  |\n   batches dist             |\n          value          1  |\n     logγg dist     61   1  |\n          value     61   1  |\n  rho_real dist     61   1  |\n          value     61   1  |\n     logβg dist     61   1  |\n          value     61   1  |\n        νω dist 1 1  1   1  |\n          value 1 1  1   1  |\n\n\n\nnum_steps = 3000\ninitial_lr = 0.03\nfinal_lr = 0.005\ngamma = final_lr / initial_lr\nlrd = gamma ** (1 / num_steps)\nadam = pyro.optim.ClippedAdam({'lr': initial_lr, 'lrd': lrd, 'betas': (0.80, 0.99)})\n\nvelocity_fit.fit(optimizer=adam, num_steps=num_steps)\n\n\n\n\n\n\n\n\n\n# Put estimations in new objects\ncycle_pyro = velocity_fit.cycle_pyro\nphase_pyro = velocity_fit.phase_pyro\nspeed_pyro = velocity_fit.speed_pyro\n\nfit_ElogS = velocity_fit.posterior[\"ElogS\"].squeeze()\nfit_ElogU = velocity_fit.posterior[\"ElogU\"].squeeze()\n\nfit_ElogS2 = velocity_fit.posterior[\"ElogS2\"].squeeze()\nfit_ElogU2 = velocity_fit.posterior[\"ElogU2\"].squeeze()\n\nlog_gammas = velocity_fit.log_gammas\nlog_betas = velocity_fit.log_betas\n\n\n# Store entire posterior sampling into an object\nfull_pps_velo = velocity_fit.posterior\n\n\nvelocity = full_pps_velo[\"ω\"].squeeze().numpy() / torch.exp(torch.mean(full_pps_velo[\"logγg\"].squeeze().mean(0).detach())).numpy()\n\n\nplt.hist(velocity.mean(1))\nplt.xlabel(\"Velocity Estimate\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Periodic Model Velocity Posterior\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Cell cycle time in hours\nprint(2*np.pi/velocity.mean())\n\n16.528259838181253\n\n\n\nvelocity_fit0 = velocity_fit\n\n\npyro.clear_param_store()\n\n\ncondition_design_matrix = copy.deepcopy(batch_design_matrix)\n\n\nn_velo_harmonics = 1\nspeed_prior = angularspeed.AngularSpeed.trivial_prior(condition_names=[\"pancreas_ductal\"], harmonics=n_velo_harmonics, \n                                                means=0.0, stds=3.0)\n\nspeed_prior.stds.loc[\"nu1_cos\"] = 0.01\nspeed_prior.stds.loc[\"nu1_sin\"] = 0.01\n\n\ncondition_on_dict = {\"ϕxy\":phase_pyro.phi_xy_tensor.T,\n                     \"ν\": cycle_pyro.means_tensor.T.unsqueeze(-2),\n                     \"Δν\": torch.tensor(phase_fit.delta_nus),\n                     \"shape_inv\": torch.tensor(phase_fit.disp_pyro).unsqueeze(-1)}\n\n\nmetaparameters_velocity = preprocessing.preprocess_for_velocity_estimation(data_to_fit, \n                                                             cycle_pyro, \n                                                             phase_pyro, \n                                                             speed_prior,\n                                                             condition_design_matrix.float(), \n                                                             batch_design_matrix.float(), \n                                                             n_harmonics=n_harm,\n                                                             count_factor=metapar.count_factor,\n                                                             ω_n_harmonics=n_velo_harmonics,\n                                                             μγ=torch.tensor(0.0).detach().clone().float(),\n                                                             σγ=torch.tensor(0.5).detach().clone().float(),\n                                                             μβ=torch.tensor(2.0).detach().clone().float(),\n                                                             σβ=torch.tensor(3.0).detach().clone().float(),\n                                                             model_type=\"lrmn\",\n                                                             condition_on=condition_on_dict)\n\n\nvelocity_fit = velocity_inference_model.VelocityFitModel(metaparams=metaparameters_velocity, \n                                                         condition_on=condition_on_dict, early_exit=False,\n                                                        num_samples=500, n_per_bin=50)\n\n\nvelocity_fit.check_model()\n\n  Trace Shapes:                              \n   Param Sites:                              \n  Sample Sites:                              \n     cells dist              |               \n          value          916 |               \n     genes dist              |               \n          value           61 |               \n harmonics dist              |               \n          value            3 |               \nconditions dist              |               \n          value            1 |               \n   batches dist              |               \n          value            1 |               \n     logγg dist       61   1 |               \n          value       61   1 |               \n     logβg dist       61   1 |               \n          value       61   1 |               \n  rho_real dist       61   1 |               \n          value       61   1 |               \n        γg dist       61   1 |  61   1       \n          value              |  61   1       \n         ν dist       61   1 |   3           \n          value       61   1 |   3           \n        Δν dist 1 1 1 61   1 |               \n          value 1 1 1 61   1 |               \n       ϕxy dist          916 |   2           \n          value          916 |   2           \n         ϕ dist              | 916           \n          value              | 916           \n         ζ dist              | 916   3       \n          value              | 916   3       \n      ζ_dϕ dist              | 916   3       \n          value              | 916   3       \n        νω dist   1 3  1   1 |               \n          value   1 3  1   1 |               \n        ζω dist              |   3 916       \n          value              |   3 916       \n     ElogS dist              |   1   1 61 916\n          value              |   1   1 61 916\n         ω dist              |   1 916       \n          value              |   1 916       \n     ElogU dist              |   1   1 61 916\n          value              |   1   1 61 916\n shape_inv dist       61   1 |               \n          value       61   1 |               \n         S dist   1 1 61 916 |               \n          value       61 916 |               \n         U dist   1 1 61 916 |               \n          value       61 916 |               \n\n\n\nnum_steps = 3000\ninitial_lr = 0.03\nfinal_lr = 0.005\ngamma = final_lr / initial_lr\nlrd = gamma ** (1 / num_steps)\nadam = pyro.optim.ClippedAdam({'lr': initial_lr, 'lrd': lrd, 'betas': (0.80, 0.99)})\n\nvelocity_fit.fit(optimizer=adam, num_steps=num_steps)\n\n\n\n\n\n\n\n\n\n# Put estimations in new objects\ncycle_pyro = velocity_fit.cycle_pyro\nphase_pyro = velocity_fit.phase_pyro\nspeed_pyro = velocity_fit.speed_pyro\n\nfit_ElogS = velocity_fit.posterior[\"ElogS\"].squeeze()\nfit_ElogU = velocity_fit.posterior[\"ElogU\"].squeeze()\n\nfit_ElogS2 = velocity_fit.posterior[\"ElogS2\"].squeeze()\nfit_ElogU2 = velocity_fit.posterior[\"ElogU2\"].squeeze()\n\nlog_gammas = velocity_fit.log_gammas\nlog_betas = velocity_fit.log_betas\n\n\n# Store entire posterior sampling into an object\nfull_pps_velo = velocity_fit.posterior\n\n\n# See the value of the mean gamma\ntorch.exp(torch.mean(full_pps_velo[\"logγg\"].squeeze().mean(0).detach())).numpy()\n\narray(0.9531931, dtype=float32)\n\n\n\nomega = full_pps_velo[\"ω\"].squeeze().numpy() / torch.exp(torch.mean(full_pps_velo[\"logγg\"].squeeze().mean(0).detach())).numpy()\nphi = phase_pyro.phis\nomegas = []\nphis = []\nn2n = {\"pancreas_ductal\":0}\nids = np.array([n2n[i] for i in np.array(data_to_fit.obs[\"batch\"])])\nfor i in range(len(data_to_fit.obs[\"batch\"].unique())):\n    omega1 = omega[:,np.where(ids == i)]\n    phi1 = phi[np.where(ids == i)]\n    omegas.append(omega1)\n    phis.append(phi1)\n\nlabels = np.array(data_to_fit.obs[\"batch\"].unique()) #list(adatas.keys())\n\ncolors = [\"tab:blue\"]\nfor i in range(len(omegas)):\n    plt.plot(phis[i][np.argsort(phis[i])], omegas[i].mean(0)[0][np.argsort(phis[i])], c=\"black\", linestyle='dashed')\n    \n    tmp5 = np.percentile(omega[:, ids==i], 5, axis=0)\n    tmp95 = np.percentile(omega[:, ids==i], 95, axis=0)\n    print(((2*np.pi)/omega[:, ids==i]).mean(), ((2*np.pi)/omega[:, ids==i]).std())\n    phi_i = phi[ids==i] \n    plt.fill_between(x=phi_i[np.argsort(phi_i)],\n                     y1=tmp5[np.argsort(phi_i)], \n                     y2=tmp95[np.argsort(phi_i)], \n                     alpha=0.6, color=colors[i], label = labels[i])\nplt.xlabel(\"Cell Cycle Phase\")\nplt.xlim(0, 2*np.pi)\nplt.ylabel(\"Scaled Velocity\")\nplt.legend()\nplt.show()\n\n17.340261 1.869181"
  },
  {
    "objectID": "ipynb/day3-3_trajectory_analysis.html",
    "href": "ipynb/day3-3_trajectory_analysis.html",
    "title": "Trajectory Inference and Pseudotime",
    "section": "",
    "text": "Download Presentation: Pseudotime Trajectory Inference\nThis notebook is partially adapted from the PAGA tutorial here: tutorial"
  },
  {
    "objectID": "ipynb/day3-3_trajectory_analysis.html#loading-libraries",
    "href": "ipynb/day3-3_trajectory_analysis.html#loading-libraries",
    "title": "Trajectory Inference and Pseudotime",
    "section": "Loading libraries",
    "text": "Loading libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as pl\nfrom matplotlib import rcParams\nimport scanpy as sc\nimport scvelo as scv\n\nimport scipy\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.simplefilter(action=\"ignore\", category=Warning)"
  },
  {
    "objectID": "ipynb/day3-3_trajectory_analysis.html#reading-data",
    "href": "ipynb/day3-3_trajectory_analysis.html#reading-data",
    "title": "Trajectory Inference and Pseudotime",
    "section": "Reading data",
    "text": "Reading data\nFirst, we will load a dataset on pancreatic endocrinogenesis from a recent study:\nBastidas-Ponce et al. “Comprehensive single cell mRNA profiling reveals a detailed roadmap for pancreatic endocrinogenesis.” Development 2019.\n\nadata = scv.datasets.pancreas()\nadata\n\nAnnData object with n_obs × n_vars = 3696 × 27998\n    obs: 'clusters_coarse', 'clusters', 'S_score', 'G2M_score'\n    var: 'highly_variable_genes'\n    uns: 'clusters_coarse_colors', 'clusters_colors', 'day_colors', 'neighbors', 'pca'\n    obsm: 'X_pca', 'X_umap'\n    layers: 'spliced', 'unspliced'\n    obsp: 'distances', 'connectivities'\n\n\nThis dataset contains single cells from very early in mouse development at multiple cell stages. Given that we know the exact temporal ordering of the cells, this dataset is ideal for demonstrating the purpose of trajectory inference and pseudotemporal ordering of single cells.\n\nadata.obs[\"clusters\"].value_counts()\n\nclusters\nDuctal           916\nNgn3 high EP     642\nPre-endocrine    592\nBeta             591\nAlpha            481\nNgn3 low EP      262\nEpsilon          142\nDelta             70\nName: count, dtype: int64\n\n\nExercise 1: We must first quickly perform the standard steps of normalization, log-transformation, and PCA on the dataset. Can you do this using the functions you have learned to use in the previous exercises, and then plot the first two PCs, colored by the clusters metadata?\n\nClick for Answer\n\n\nAnswer:\n    sc.pp.normalize_total(adata)\n    sc.pp.log1p(adata)\n    sc.pp.highly_variable_genes(adata, n_top_genes=3000, subset=True)\n    adata = adata[:, adata.var[\"highly_variable\"]].copy()\n    sc.tl.pca(adata)\n    sc.pl.pca(adata, color='clusters')\n\n\n\n\n# your code here\n\n\n\n\n\n\n\n\nNumerous trajectory inference methods, including PAGA, are graph-based. To obtain such a needed graph, we need to examine the nearest neighbors of each cell. Let’s compute the neighborhood using 50 nearest neighbors and then embed the results on a UMAP.\n\nsc.pp.neighbors(adata, n_pcs = 30, n_neighbors = 50)\nsc.tl.umap(adata, min_dist=0.4, spread=3)\n\n\nsc.pl.umap(adata, color = ['clusters'],\n           legend_loc = 'on data', edges = True)"
  },
  {
    "objectID": "ipynb/day3-3_trajectory_analysis.html#run-paga",
    "href": "ipynb/day3-3_trajectory_analysis.html#run-paga",
    "title": "Trajectory Inference and Pseudotime",
    "section": "Run PAGA",
    "text": "Run PAGA\nUse the ground truth cell types to run PAGA. First we create the graph and initialize the positions using the umap.\n\n# use the umap to initialize the graph layout.\nsc.tl.draw_graph(adata, init_pos='X_umap')\nsc.pl.draw_graph(adata, color='clusters', legend_loc='on data', legend_fontsize = 'xx-small')\n\nWARNING: Package 'fa2' is not installed, falling back to layout 'fr'.To use the faster and better ForceAtlas2 layout, install package 'fa2' (`pip install fa2`).\n\n\n\n\n\n\n\n\n\n\nsc.tl.paga(adata, groups='clusters')\nsc.pl.paga(adata, color='clusters', edge_width_scale = 0.3)"
  },
  {
    "objectID": "ipynb/day3-3_trajectory_analysis.html#embedding-using-paga-initialization",
    "href": "ipynb/day3-3_trajectory_analysis.html#embedding-using-paga-initialization",
    "title": "Trajectory Inference and Pseudotime",
    "section": "Embedding using PAGA-initialization",
    "text": "Embedding using PAGA-initialization\nWe can now redraw the graph using another starting position from the paga layout. The following is just as well possible for a UMAP.\n\nsc.tl.draw_graph(adata, init_pos='paga')\n\nWARNING: Package 'fa2' is not installed, falling back to layout 'fr'.To use the faster and better ForceAtlas2 layout, install package 'fa2' (`pip install fa2`).\n\n\n\nsc.pl.draw_graph(adata, color=['clusters'], legend_loc='on data')"
  },
  {
    "objectID": "ipynb/day3-3_trajectory_analysis.html#gene-changes",
    "href": "ipynb/day3-3_trajectory_analysis.html#gene-changes",
    "title": "Trajectory Inference and Pseudotime",
    "section": "Gene changes",
    "text": "Gene changes\nWe can reconstruct gene changes along PAGA paths for a given set of genes\nBy looking at the different know lineages and the layout of the graph we define manually some paths to the graph that corresponds to specific lineages.\n\n# Define paths\npaths = [('beta', ['Ductal', 'Ngn3 low EP', 'Ngn3 high EP', 'Pre-endocrine', 'Beta']),\n        ('alpha', ['Ductal', 'Ngn3 low EP', 'Ngn3 high EP', 'Pre-endocrine', 'Alpha']),\n        ('delta', ['Ductal', 'Ngn3 low EP', 'Ngn3 high EP', 'Pre-endocrine', 'Delta']),\n        ('epsilon', ['Ductal', 'Ngn3 low EP', 'Ngn3 high EP', 'Pre-endocrine', 'Epsilon'])]\n\nadata.obs['distance'] = adata.obs['dpt_pseudotime']\n\nThen we select some genes that can vary in the lineages and plot onto the paths.\n\nsc.tl.rank_genes_groups(adata, \"clusters\", method=\"t-test\", n_genes=10)\n\n\npd.DataFrame(adata.uns[\"rank_genes_groups\"][\"names\"])\n\n\n\n\n\n\n\n\nDuctal\nNgn3 low EP\nNgn3 high EP\nPre-endocrine\nBeta\nAlpha\nDelta\nEpsilon\n\n\n\n\n0\nSpp1\nSpp1\nNeurog3\nMap1b\nPcsk2\nCpe\nRbp4\nGhrl\n\n\n1\nDbi\nDbi\nBtbd17\nFev\nRbp4\nTmem27\nHhex\nIsl1\n\n\n2\nCldn3\nSparc\nSox4\nHmgn3\nMafb\nPcsk1n\nHmgn3\nRbp4\n\n\n3\nMgst1\nMgst1\nMdk\nBex2\nSec61b\nTspan7\nIsl1\nBex2\n\n\n4\nAnxa2\n1700011H14Rik\nGadd45a\nYpel3\nCpe\nMeis2\nFam183b\nFam183b\n\n\n5\nBicc1\nCldn3\nSmarcd2\nChga\nGng12\nGpx3\nHadh\nMaged2\n\n\n6\nKrt18\nAnxa2\nBtg2\nEmb\nPcsk1n\nFam183b\nArg1\nCck\n\n\n7\nMt1\nClu\nTmsb4x\nCpe\nRap1b\nSlc38a5\nSst\nAnpep\n\n\n8\nClu\nVim\nHes6\nCryba2\nTuba1a\nSlc25a5\nGpx3\nCard19\n\n\n9\n1700011H14Rik\nMt1\nCd63\nGlud1\n1700086L19Rik\nHmgn3\nDlk1\nArg1\n\n\n\n\n\n\n\n\ngene_names = [\"Spp1\", \"Dbi\", \"Cldn3\", \"Sparc\", \"Mgst1\", \"Cldn3\", \"Neurog3\", \"Btbd17\", \"Sox4\",\n              \"Map1b\", \"Fev\", \"Hmgn3\", \"Bex2\", \"Sec61b\", \"Tuba1a\", \"Meis2\", \"Pcsk1n\", \"Sst\", \"Arg1\",\n              \"Rbp4\", \"Hhex\", \"Ghrl\", \"Isl1\", \"Rbp4\", \"Bex2\"]\n\n\n_, axs = pl.subplots(ncols=4, figsize=(16, 8), gridspec_kw={\n                     'wspace': 0.05, 'left': 0.12})\npl.subplots_adjust(left=0.05, right=0.98, top=0.82, bottom=0.2)\nfor ipath, (descr, path) in enumerate(paths):\n    _, data = sc.pl.paga_path(\n        adata, path, gene_names,\n        show_node_names=False,\n        ax=axs[ipath],\n        ytick_fontsize=12,\n        left_margin=0.15,\n        n_avg=50,\n        annotations=['distance'],\n        show_yticks=True if ipath == 0 else False,\n        show_colorbar=False,\n        color_map='Greys',\n        groups_key='clusters',\n        color_maps_annotations={'distance': 'viridis'},\n        title='{} path'.format(descr),\n        return_data=True,\n        use_raw=False,\n        show=False)\npl.show()"
  },
  {
    "objectID": "ipynb/day2-4_visualization.html",
    "href": "ipynb/day2-4_visualization.html",
    "title": "Cell type annotation and visualization",
    "section": "",
    "text": "Download Presentation: Cell type annotation and visualization\n\nimport scanpy as sc\nimport pandas as pd # for handling data frames (i.e. data tables)\nimport numpy as np # for handling numbers, arrays, and matrices\nimport matplotlib.pyplot as plt # plotting package\nimport seaborn as sns # plotting package\n\nExercise 0: Before we continue in this notebook with the next steps of the analysis, we need to load our results from the previous notebook using the sc.read_h5ad function and assign them to the variable name adata. Give it a try!\n\nadata = sc.read_h5ad(\"PBMC_analysis_SIB_tutorial5.h5ad\")\n\n\nadata\n\nAnnData object with n_obs × n_vars = 5465 × 3000\n    obs: 'sample', 'n_counts', 'n_genes', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_20_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'log1p_total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'log1p_total_counts_hb', 'pct_counts_hb', 'is_doublet', 'S_score', 'G2M_score', 'phase', 'leiden', 'leiden_res1', 'leiden_res0_1', 'leiden_res0_5', 'leiden_res2'\n    var: 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'mean', 'std'\n    uns: 'hvg', 'leiden', 'leiden_colors', 'leiden_res0_1_colors', 'leiden_res0_5_colors', 'leiden_res1_colors', 'leiden_res2_colors', 'log1p', 'neighbors', 'pca', 'phase_colors', 'sample_colors', 'umap'\n    obsm: 'X_pca', 'X_pcahm', 'X_umap'\n    varm: 'PCs'\n    obsp: 'connectivities', 'distances'\n\n\nBefore proceeding with marker gene analysis and cell type annotation, restore the raw version of the data, add the necessary annotations, and normalize the counts:\n\nadata_raw = sc.read_h5ad(\"PBMC_analysis_SIB_tutorial.h5ad\") # raw data before selecting highly variable genes\nshared_bcs = list(set(adata.obs.index) & set(adata_raw.obs.index))\nadata_raw = adata_raw[shared_bcs].copy()\nadata = adata[shared_bcs].copy()\nadata_raw_norm = adata_raw.copy()\nsc.pp.normalize_total(adata_raw_norm, target_sum=None)\nsc.pp.log1p(adata_raw_norm)\n\n\nadata_raw_norm.obs[\"leiden\"] = adata.obs[\"leiden_res1\"]\nadata_raw_norm.obsm[\"X_pca\"] = adata.obsm[\"X_pca\"]\nadata_raw_norm.obsm[\"X_pcahm\"] = adata.obsm[\"X_pcahm\"]\nadata_raw_norm.obsm[\"X_umap\"] = adata.obsm[\"X_umap\"]\nadata_raw_norm.obsp[\"connectivities\"] = adata.obsp[\"connectivities\"]\nadata_raw_norm.obsp[\"distances\"] = adata.obsp[\"distances\"]\nadata_raw_norm.uns[\"neighbors\"] = adata.uns[\"neighbors\"]\n\nLet’s use a simple method implemented by scanpy to find marker genes by the Leiden cluster.\n\nsc.tl.rank_genes_groups(\n    adata_raw_norm, use_raw=False, groupby=\"leiden\", method=\"wilcoxon\", key_added=\"dea_leiden\"\n)\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:396: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'names'] = self.var_names[global_indices]\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:398: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'scores'] = scores[global_indices]\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:401: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'pvals'] = pvals[global_indices]\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:411: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'pvals_adj'] = pvals_adj[global_indices]\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:422: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'logfoldchanges'] = np.log2(\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:396: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'names'] = self.var_names[global_indices]\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:398: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'scores'] = scores[global_indices]\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:401: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'pvals'] = pvals[global_indices]\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:411: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'pvals_adj'] = pvals_adj[global_indices]\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:422: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'logfoldchanges'] = np.log2(\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:396: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'names'] = self.var_names[global_indices]\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:398: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'scores'] = scores[global_indices]\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:401: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'pvals'] = pvals[global_indices]\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:411: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'pvals_adj'] = pvals_adj[global_indices]\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:422: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'logfoldchanges'] = np.log2(\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:396: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'names'] = self.var_names[global_indices]\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:398: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'scores'] = scores[global_indices]\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:401: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'pvals'] = pvals[global_indices]\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:411: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'pvals_adj'] = pvals_adj[global_indices]\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/tools/_rank_genes_groups.py:422: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  self.stats[group_name, 'logfoldchanges'] = np.log2(\n\n\n\nadata_raw_norm.uns[\"dea_leiden\"].keys()\n\ndict_keys(['params', 'names', 'scores', 'pvals', 'pvals_adj', 'logfoldchanges'])\n\n\n\nmarker_genes = pd.DataFrame(adata_raw_norm.uns[\"dea_leiden\"][\"names\"])\nmarker_genes.columns = [\"Cluster\" + str(x) for x in range(0, len(adata_raw_norm.obs[\"leiden\"].unique()))]\nmarker_genes.head()\n\n\n\n\n\n\n\n\nCluster0\nCluster1\nCluster2\nCluster3\nCluster4\nCluster5\nCluster6\nCluster7\nCluster8\nCluster9\n...\nCluster14\nCluster15\nCluster16\nCluster17\nCluster18\nCluster19\nCluster20\nCluster21\nCluster22\nCluster23\n\n\n\n\n0\nRPL21\nHBA2\nS100A8\nSLC25A37\nCD74\nHBB\nFTH1\nDNTT\nRPS4X\nCD74\n...\nRPS27\nCD52\nCD79B\nNKG7\nPRDX2\nATPIF1\nMPO\nHBD\nSLC25A37\nSSR4\n\n\n1\nRPS27\nHBB\nS100A9\nHBD\nCST3\nHBA2\nFTL\nIGLL1\nRPS18\nMS4A1\n...\nRPL21\nTMSB4X\nIGHM\nB2M\nGYPB\nAPOC1\nAZU1\nGYPA\nHBD\nMZB1\n\n\n2\nRPS29\nHBA1\nLYZ\nHMBS\nHLA-DRA\nHBA1\nTYROBP\nVPREB1\nRPS8\nCD79A\n...\nRPS29\nTRAC\nTCL1A\nHLA-B\nHEMGN\nNME4\nSRGN\nHMBS\nSLC4A1\nHSP90B1\n\n\n3\nRPL13\nSNCA\nS100A6\nSLC4A1\nHLA-DPB1\nBPGM\nCTSS\nPTMA\nRPLP0\nHLA-DRA\n...\nRPL13\nLTB\nCD24\nCCL5\nAHSP\nTMEM14C\nELANE\nSLC25A37\nBNIP3L\nFKBP11\n\n\n4\nRPLP2\nBNIP3L\nS100A4\nSLC2A1\nHLA-DRB1\nUBB\nFCER1G\nHMGB1\nRPS24\nCD37\n...\nRPLP2\nB2M\nRCSD1\nHLA-C\nCA2\nFAM178B\nCFD\nSLC4A1\nAHSP\nSEC11C\n\n\n\n\n5 rows × 24 columns\n\n\n\n\nmarker_genes_pvals = pd.DataFrame(adata_raw_norm.uns[\"dea_leiden\"][\"logfoldchanges\"])\nmarker_genes_pvals.columns = [\"Cluster\" + str(x) for x in range(0, len(adata_raw_norm.obs[\"leiden\"].unique()))]\nmarker_genes_pvals.head()\n\n\n\n\n\n\n\n\nCluster0\nCluster1\nCluster2\nCluster3\nCluster4\nCluster5\nCluster6\nCluster7\nCluster8\nCluster9\n...\nCluster14\nCluster15\nCluster16\nCluster17\nCluster18\nCluster19\nCluster20\nCluster21\nCluster22\nCluster23\n\n\n\n\n0\n1.696161\n4.536685\n6.127852\n3.926013\n3.412972\n4.632360\n2.845425\n4.403666\n1.796766\n3.350841\n...\n1.718245\n2.538189\n4.155026\n5.839576\n3.842825\n2.919723\n7.224152\n4.162687\n3.580635\n3.880045\n\n\n1\n1.909575\n4.749588\n5.971000\n4.183972\n3.933990\n4.355470\n3.283564\n4.142626\n1.714990\n5.916070\n...\n1.482474\n1.988572\n3.510608\n1.826526\n3.914904\n5.641148\n6.555845\n3.788034\n3.828166\n4.938936\n\n\n2\n1.893222\n4.461418\n6.040055\n3.383904\n3.215807\n4.279613\n4.806640\n3.918157\n1.856143\n3.856486\n...\n1.645742\n3.124133\n4.694790\n2.050864\n3.797841\n3.448345\n4.227748\n3.277330\n2.918315\n3.337664\n\n\n3\n1.828848\n2.532328\n3.790318\n3.315197\n3.314304\n2.452660\n4.474485\n2.222489\n1.779858\n2.956043\n...\n1.580002\n2.921369\n4.179593\n6.373088\n3.689547\n3.167665\n6.572220\n3.602345\n2.388110\n4.894440\n\n\n4\n1.628970\n2.246099\n3.793867\n3.430458\n3.418615\n1.447062\n4.434198\n2.761628\n1.619545\n2.928020\n...\n1.443913\n1.548262\n3.441401\n2.135884\n4.070135\n6.136931\n3.634571\n3.219230\n3.156719\n4.655691\n\n\n\n\n5 rows × 24 columns\n\n\n\n\nsc.settings.set_figure_params(dpi=50, facecolor='white')\nsc.pl.rank_genes_groups_dotplot(\n    adata_raw_norm, groupby=\"leiden\", standard_scale=\"var\", n_genes=5, key=\"dea_leiden\"\n)\n\nWARNING: dendrogram data not found (using key=dendrogram_leiden). Running `sc.tl.dendrogram` with default parameters. For fine tuning it is recommended to run `sc.tl.dendrogram` independently.\n\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_dotplot.py:747: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap', 'norm' will be ignored\n  dot_ax.scatter(x, y, **kwds)\n\n\n\n\n\n\n\n\n\nAs you can see above, a lot of the differentially expressed genes are highly expressed in multiple clusters. We can filter the differentially expressed genes to select for more cluster-specific differentially expressed genes:\n\nsc.tl.filter_rank_genes_groups(\n    adata_raw_norm,\n    min_in_group_fraction=0.2,\n    max_out_group_fraction=0.2,\n    key=\"dea_leiden\",\n    key_added=\"dea_leiden_filtered\",\n)\n\n\nsc.pl.rank_genes_groups_dotplot(\n    adata_raw_norm, groupby=\"leiden\", standard_scale=\"var\", n_genes=5, key=\"dea_leiden_filtered\"\n)\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_dotplot.py:747: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap', 'norm' will be ignored\n  dot_ax.scatter(x, y, **kwds)\n\n\n\n\n\n\n\n\n\nExercise 1: Visualize marker genes on the UMAP or tSNE representation. Try to find 3-4 marker genes that are indeed specific to a particular cluster. Are there any clusters that do not seem to have unique marker genes? Are there any clusters containing markers that are only specific to a portion of the cluster? Marker genes should uniformly define cells “everywhere” in a cluster in UMAP space, otherwise the cluster might actually be two!\n\nsc.pl.umap(\n    adata,\n    color=[\"CD74\", \"SSR4\", \"CA2\", \"HBA2\", \"CST3\", \"CD37\", \"IL32\", \"leiden_res0_5\"],\n    vmax=\"p99\",\n    legend_loc=\"on data\",\n    frameon=False,\n    cmap=\"coolwarm\",\n)\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py:394: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  cax = scatter(\n\n\n\n\n\n\n\n\n\nExercise 2: Let’s take a few steps back to understand all of the previous steps a little bit better! The number of genes selected by the highly_variable_genes function can significantly impact your ability to cluster. Too few genes and you cannot discriminate between different cell types, too many genes and you capture lots of noisy clusters! Try repeating the previous analysis with either 200 or 5000 highly variable genes, naming the AnnData object differently (i.e. adata_200genes) to avoid overwriting your previous results. Transfer the metadata for the new cluster labels to the original AnnData object’s metadata at adata.obs and compare on the UMAP. Are the clusters different?\nOnce you have settled on the parameters for the dimensionality reduction and clustering steps, it is time to begin annotating your clusters with cell types. This is normally a challenging step! When you are not too familiar with the marker genes for a particular cluster, a good starting point is simply to Google a strong marker gene and understand its function. Other tools that might be useful include EnrichR and GSEAPy. - https://maayanlab.cloud/Enrichr/ - https://gseapy.readthedocs.io/en/latest/gseapy_example.html#2.-Enrichr-Example\nFortunately in our case, we will try automated cell type annotations!\n\nAutomated cell type annotation\nExercise 3: The methods discussed here focus on automated data annotation, distinct from manual methods. Unlike the previously detailed approach, these methods automate data annotation. They operate on different principles, using predefined markers or trained on comprehensive scRNA-seq datasets. It’s vital to note that automated annotations can vary in quality. Thus, they should be seen as a starting point rather than a final solution. Pasquini et al., 2021 and Abdelaal et al., 2019 offer extensive discussions on automated annotation methods.\nQuality depends on:\n\nClassifier Choice: Various classifier types perform similarly, with neural networks not necessarily outperforming linear models [1, 2, 3].\nTraining Data Quality: Annotation quality relies on the quality of the training data. Poorly annotated or noisy training data can impact the classifier.\nData Similarity: Similarity between your data and the classifier’s training data matters. Cross-dataset models often provide better annotations. For example, CellTypist, trained on diverse lung datasets, is likely to perform well on new lung data.\n\nWhile classifiers have limitations, they offer advantages like rapid annotation, leveraging previous studies, and promoting standardized terminology. Ensuring robust uncertainty measures to quantify annotation reliability is crucial.\nMany classification methods rely on a limited set of genes, typically just 1 to ~10 marker genes per cell type. An alternative approach utilizes classifiers that consider a more extensive gene set, often several thousands or more. These classifiers are trained on previously annotated datasets or atlases. Notable examples include CellTypist Conde et al., 2022 and Clustifyr Fu et al., 2020.\nLet’s explore CellTypist for our data. Referring to the CellTypist tutorial, we should prepare our data by normalizing counts to 10,000 counts per cell and subsequently applying a log1p transformation. So we need to re-normalize our data, without our logarithm shift approach, but with a more classical ‘Counts per ten-thousand’.\n\nimport re\nimport celltypist\nfrom celltypist import models\n\n\nadata_celltypist = adata_raw.copy()  # make a copy of our adata\nsc.pp.normalize_per_cell(\n    adata_celltypist, counts_per_cell_after=10000.0\n)  # normalize to 10,000 counts per cell\nsc.pp.log1p(adata_celltypist)  # log-transform\n# make .X dense instead of sparse, for compatibility with celltypist:\nadata_celltypist.X = adata_celltypist.X\n\nHere we will load the model directly from our folder on google drive, where we can find the model trained. Alternatively, CellTypist method propose a panel of models that can be download directly from python using models.download_models(force_update = True). The idea is of course to use a model that match our biological context, and for pre-trained model-based method like CellTypist, it is possible that your biological context is not available. In that situation, there is no other options than opting for manual annotations.\nThere are two models that might be relevant for this particular dataset we are working with. Let’s download both of them and try each one for the classification.\n\nmodels.download_models(\n    force_update=True, model=[\"Immune_All_Low.pkl\", \"Immune_All_High.pkl\"]\n)\n\n📜 Retrieving model list from server https://celltypist.cog.sanger.ac.uk/models/models.json\n📚 Total models in list: 48\n📂 Storing models in /home/alex/.celltypist/data/models\n💾 Total models to download: 2\n💾 Downloading model [1/2]: Immune_All_Low.pkl\n💾 Downloading model [2/2]: Immune_All_High.pkl\n\n\n\nmodel_low = models.Model.load(model=\"Immune_All_Low.pkl\")\nmodel_high = models.Model.load(model=\"Immune_All_High.pkl\")\n\nFor each of these, we can see which cell types it includes to see if bone marrow cell types are included:\n\n# We can print all the cell types covererd by the model\nmodel_low.cell_types\n\narray(['Age-associated B cells', 'Alveolar macrophages', 'B cells',\n       'CD16+ NK cells', 'CD16- NK cells', 'CD8a/a', 'CD8a/b(entry)',\n       'CMP', 'CRTAM+ gamma-delta T cells', 'Classical monocytes',\n       'Cycling B cells', 'Cycling DCs', 'Cycling NK cells',\n       'Cycling T cells', 'Cycling gamma-delta T cells',\n       'Cycling monocytes', 'DC', 'DC precursor', 'DC1', 'DC2', 'DC3',\n       'Double-negative thymocytes', 'Double-positive thymocytes', 'ELP',\n       'ETP', 'Early MK', 'Early erythroid', 'Early lymphoid/T lymphoid',\n       'Endothelial cells', 'Epithelial cells', 'Erythrocytes',\n       'Erythrophagocytic macrophages', 'Fibroblasts',\n       'Follicular B cells', 'Follicular helper T cells', 'GMP',\n       'Germinal center B cells', 'Granulocytes', 'HSC/MPP',\n       'Hofbauer cells', 'ILC', 'ILC precursor', 'ILC1', 'ILC2', 'ILC3',\n       'Intermediate macrophages', 'Intestinal macrophages',\n       'Kidney-resident macrophages', 'Kupffer cells',\n       'Large pre-B cells', 'Late erythroid', 'MAIT cells', 'MEMP', 'MNP',\n       'Macrophages', 'Mast cells', 'Megakaryocyte precursor',\n       'Megakaryocyte-erythroid-mast cell progenitor',\n       'Megakaryocytes/platelets', 'Memory B cells',\n       'Memory CD4+ cytotoxic T cells', 'Mid erythroid', 'Migratory DCs',\n       'Mono-mac', 'Monocyte precursor', 'Monocytes', 'Myelocytes',\n       'NK cells', 'NKT cells', 'Naive B cells',\n       'Neutrophil-myeloid progenitor', 'Neutrophils',\n       'Non-classical monocytes', 'Plasma cells', 'Plasmablasts',\n       'Pre-pro-B cells', 'Pro-B cells',\n       'Proliferative germinal center B cells', 'Promyelocytes',\n       'Regulatory T cells', 'Small pre-B cells', 'T(agonist)',\n       'Tcm/Naive cytotoxic T cells', 'Tcm/Naive helper T cells',\n       'Tem/Effector helper T cells', 'Tem/Effector helper T cells PD1+',\n       'Tem/Temra cytotoxic T cells', 'Tem/Trm cytotoxic T cells',\n       'Transitional B cells', 'Transitional DC', 'Transitional NK',\n       'Treg(diff)', 'Trm cytotoxic T cells', 'Type 1 helper T cells',\n       'Type 17 helper T cells', 'gamma-delta T cells', 'pDC',\n       'pDC precursor'], dtype=object)\n\n\n\n# We can print all the cell types covererd by the model\nmodel_high.cell_types\n\narray(['B cells', 'B-cell lineage', 'Cycling cells', 'DC', 'DC precursor',\n       'Double-negative thymocytes', 'Double-positive thymocytes', 'ETP',\n       'Early MK', 'Endothelial cells', 'Epithelial cells',\n       'Erythrocytes', 'Erythroid', 'Fibroblasts', 'Granulocytes',\n       'HSC/MPP', 'ILC', 'ILC precursor', 'MNP', 'Macrophages',\n       'Mast cells', 'Megakaryocyte precursor',\n       'Megakaryocytes/platelets', 'Mono-mac', 'Monocyte precursor',\n       'Monocytes', 'Myelocytes', 'Plasma cells', 'Promyelocytes',\n       'T cells', 'pDC', 'pDC precursor'], dtype=object)\n\n\nThe model_high seems to have fewer cell types, let’s start with that for obtaining broader cell type categories.\nExercise 4: Use the celltypist.annotate function to predict cell types using the model model_high and marjority_voting=True: Save the result to a variable called predictions_high.\n\nClick for Answer\n\n\nAnswer:\n    predictions_high = celltypist.annotate(\n    adata_celltypist, model=model_high, majority_voting=True)\n\n    🔬 Input data has 5459 cells and 10839 genes\n    🔗 Matching reference genes in the model\n    🧬 3729 features used for prediction\n    ⚖️ Scaling input data\n    🖋️ Predicting labels\n    ✅ Prediction done!\n    👀 Can not detect a neighborhood graph, will construct one before the over-clustering\n    ⛓️ Over-clustering input data with resolution set to 10\n    🗳️ Majority voting the predictions\n    ✅ Majority voting done!\n\n\n\n\npredictions_high_adata = predictions_high.to_adata()\npredictions_high_adata.obs[['majority_voting', 'conf_score']]\n\n\n\n\n\n\n\n\nmajority_voting\nconf_score\n\n\n\n\nTTCCCAGCAGACAAAT-1\nB-cell lineage\n0.990278\n\n\nGTGTGCGGTGTTTGGT-1\nB-cell lineage\n0.986694\n\n\nGATCAGTTCTTTAGTC-1\nErythroid\n0.984309\n\n\nGTCTTCGAGAAGATTC-1\nMonocytes\n0.999283\n\n\nTGCTACCTCGTTACAG-1\npDC\n0.996167\n\n\n...\n...\n...\n\n\nTCATTACCAAGCGTAG-1\nMonocytes\n0.938815\n\n\nCAGATCAAGATGCGAC-1\nMonocytes\n0.717744\n\n\nCGTCAGGAGTGCTGCC-1\nErythroid\n0.992524\n\n\nACACTGAGTTGATTCG-1\nErythroid\n0.933983\n\n\nGAAGCAGTCGAACTGT-1\nT cells\n0.997989\n\n\n\n\n5459 rows × 2 columns\n\n\n\n\nadata_raw_norm.obs[\"celltypist_annotations_high\"] = predictions_high_adata.obs[\"majority_voting\"]\nadata_raw_norm.obs[\"celltypist_conf_score_high\"] = predictions_high_adata.obs[\"conf_score\"]\n\nExercise 5: Now do the same (celltypist.annotate) for the finer-grained annotations. Save the result to a variable called predictions_low.\n\nClick for Answer\n\n\nAnswer:\n    predictions_low = celltypist.annotate(\n    adata_celltypist, model=model_low, majority_voting=True)\n\n    🔬 Input data has 5459 cells and 10839 genes\n    🔗 Matching reference genes in the model\n    🧬 3729 features used for prediction\n    ⚖️ Scaling input data\n    🖋️ Predicting labels\n    ✅ Prediction done!\n    👀 Can not detect a neighborhood graph, will construct one before the over-clustering\n    ⛓️ Over-clustering input data with resolution set to 10\n    🗳️ Majority voting the predictions\n    ✅ Majority voting done!\n\n\n\n\npredictions_low_adata = predictions_low.to_adata()\npredictions_low_adata.obs[['majority_voting', 'conf_score']]\n\n\n\n\n\n\n\n\nmajority_voting\nconf_score\n\n\n\n\nTTCCCAGCAGACAAAT-1\nLarge pre-B cells\n0.501054\n\n\nGTGTGCGGTGTTTGGT-1\nLarge pre-B cells\n0.827030\n\n\nGATCAGTTCTTTAGTC-1\nLate erythroid\n0.897641\n\n\nGTCTTCGAGAAGATTC-1\nMonocytes\n0.982690\n\n\nTGCTACCTCGTTACAG-1\npDC\n0.996716\n\n\n...\n...\n...\n\n\nTCATTACCAAGCGTAG-1\nClassical monocytes\n0.871458\n\n\nCAGATCAAGATGCGAC-1\nClassical monocytes\n0.318097\n\n\nCGTCAGGAGTGCTGCC-1\nMid erythroid\n0.361174\n\n\nACACTGAGTTGATTCG-1\nLate erythroid\n0.935759\n\n\nGAAGCAGTCGAACTGT-1\nTcm/Naive helper T cells\n0.155408\n\n\n\n\n5459 rows × 2 columns\n\n\n\nAnd we save our predictions to our AnnData object:\n\nadata_raw_norm.obs[\"celltypist_annotations_low\"] = predictions_low_adata.obs[\"majority_voting\"]\nadata_raw_norm.obs[\"celltypist_conf_score_low\"] = predictions_low_adata.obs[\"conf_score\"]\n\nCellTypist annotations can then be visualized on the UMAP embedding:\n\nsc.settings.set_figure_params(dpi=80, facecolor='white')\nsc.pl.umap(\n    adata_raw_norm,\n    color=[\"celltypist_annotations_low\", \"celltypist_annotations_high\"],\n    frameon=False,\n    sort_order=False,\n    wspace=1.2,\n)\n\n\n\n\n\n\n\n\nAlso, each cell gets a prediction score:\n\nsc.pl.umap(\n    adata_raw_norm,\n    color=[\"celltypist_conf_score_low\", \"celltypist_conf_score_high\"],\n    frameon=False,\n    sort_order=False,\n    wspace=1,\n)\n\n\n\n\n\n\n\n\nOne way of getting a feeling for the quality of these annotations is by looking if the observed cell type similarities correspond to our expectations:\n\nsc.pl.dendrogram(adata_raw_norm, groupby=\"celltypist_annotations_low\")\n\nWARNING: dendrogram data not found (using key=dendrogram_celltypist_annotations_low). Running `sc.tl.dendrogram` with default parameters. For fine tuning it is recommended to run `sc.tl.dendrogram` independently.\n\n\n\n\n\n\n\n\n\nExercise 6: Can you identify marker genes for each annotated cell type from celltypist_annotations_high? Do this similarly to how you did for the leiden clusters above, i.e., with the sc.tl.rank_genes_groups function, grouped by the annotated cell type. Can you convert the results into a pandas dataframe?\n\nClick for Answer\n\n\nAnswer: You should write the following code:\n    sc.tl.rank_genes_groups(adata_raw_norm, use_raw=False, \n          groupby=\"celltypist_annotations_high\", method=\"wilcoxon\", key_added=\"dea_celltype\")\n\n    marker_genes = pd.DataFrame(adata_raw_norm.uns[\"dea_celltype\"][\"names\"])\n    marker_genes.columns = sorted(list(adata_raw_norm.obs[\"celltypist_annotations_high\"].unique()))\n    marker_genes.head()\n\n\n\n\n\nAnother way to annotate: with label transfer from a reference dataset!\nIn addition to automated label transfer methods using machine learning, it is possible to transfer cell type labels acquired (either automatically or manually) from a reference dataset to a new, unannotated dataset, using the sc.tl.ingest function.\nThe ingest function assumes an annotated reference dataset that captures the biological variability of interest. The rational is to fit a model on the reference data and use it to project new data. For the time being, this model is a PCA combined with a neighbor lookup search tree, for which we use UMAP’s implementation [McInnes18]. Similar PCA-based integrations have been used before, for instance, in [Weinreb18].\n\nAs ingest is simple and the procedure clear, the workflow is transparent and fast.\nThe function leaves the data matrix itself invariant, unlike many integration methods.\nThe function also solves the label mapping problem and maintains an embedding that might have desired properties like specific clusters or trajectories.\n\nWe refer to this asymmetric dataset integration as ingesting annotations from an annotated reference adata_ref into an adata that still lacks this annotation. It is different from learning a joint representation that integrates datasets in a symmetric way as in CCA (e.g. from Seurat).\nTake a look at tools in the external API or at the ecoystem page for scanpy to read about other related data integration and label transfer tools.\nLet’s evaluate the role of the ingest function by considering the case where we only use celltypist to annotate cells from one sample: PBMMC_1. We can use ingest to then transfer the labels to another sample, PBMMC_3.\n\n# Reference dataset\nsample1_adata = adata_raw_norm[adata_raw_norm.obs[\"sample\"]==\"PBMMC_1\"].copy()\n\n# Unlabeled dataset to transfer cell type labels to\nsample3_adata = adata_raw_norm[adata_raw_norm.obs[\"sample\"]==\"PBMMC_3\"].copy()\n\nIn order for ingest to transfer the labels, we need to first remove the labels obtained with celltypist from our sample3_adata object. In the meantime, we can store them in a variable old_annotations_sample3:\n\nold_annotations_sample3 = np.array(sample3_adata.obs[\"celltypist_annotations_high\"])\ndel sample3_adata.obs[\"celltypist_annotations_high\"]\n\nWe need to quickly repeat the analysis steps we have previously performed before, but this time on the two samples (PBMMC_1; i.e. sample1_adata and PBMMC_3; i.e. sample3_adata) independently:\nExercise 7: Repeat the following steps, once with sample1_adata and once with sample3_adata:\nsc.pp.highly_variable_genes with subset=True and n_top_genes=3000\nsc.pp.pca\nsc.pp.neighbors\nsc.tl.umap\n\nClick for Answer\n\n\nAnswer: You should write the following code:\n    sc.pp.highly_variable_genes(sample1_adata, subset=True, n_top_genes=3000)\n    sc.pp.pca(sample1_adata)\n    sc.pp.neighbors(sample1_adata)\n    sc.tl.umap(sample1_adata)\n\n\n    sc.pp.highly_variable_genes(sample3_adata, subset=True, n_top_genes=3000)\n    sc.pp.pca(sample3_adata)\n    sc.pp.neighbors(sample3_adata)\n    sc.tl.umap(sample3_adata)\n\n\n\nVisualize your generated UMAPs for each individual sample:\n\nsc.pl.umap(sample1_adata, color=\"celltypist_annotations_high\")\n\n\n\n\n\n\n\n\n\nsc.pl.umap(sample3_adata)\n\n\n\n\n\n\n\n\nTo use sc.tl.ingest, the datasets need to be defined on the same variables.\n\nvar_names = sample1_adata.var_names.intersection(sample3_adata.var_names)\nsample1_adata = sample1_adata[:, var_names]\nsample3_adata = sample3_adata[:, var_names]\n\nWe can finally map labels and embeddings from sample1_adata (our reference) to sample3_adata (our unannotated new dataset) based on a chosen representation.\nExercise 8: Run the sc.tl.ingest command, with sample3_adata as your unlabeled dataset and sample1_adata as your reference dataset. Transfer the label celltypist_annotations_high using the pca as the embedding method.\n\nClick for Answer\n\n\nAnswer: You should write the following code:\n    sc.tl.ingest(adata=sample3_adata, \n                 adata_ref=sample1_adata, \n                 obs=\"celltypist_annotations_high\",\n                 embedding_method='pca')\n\n\n\nFinally, we want to rename the labels obtained for sample3_adata from celltypist and ingest, and compare them to each other on the UMAP space:\n\nsample3_adata.obs[\"celltypist_labels\"] = old_annotations_sample3\nsample3_adata.obs[\"ingest_labels\"] = sample3_adata.obs[\"celltypist_annotations_high\"]\n\n\nsc.pl.umap(sample3_adata, color=['celltypist_labels', 'ingest_labels'], wspace=0.50)\n\n\n\n\n\n\n\n\nBy comparing the ingest_labels annotation with celltypist_labels, we see that the data has been reasonably mapped."
  },
  {
    "objectID": "ipynb/day2-2_integration.html",
    "href": "ipynb/day2-2_integration.html",
    "title": "Data Integration",
    "section": "",
    "text": "Download Presentation: Dimensionality Reduction and Integration\n\nimport scanpy as sc\nimport pandas as pd # for handling data frames (i.e. data tables)\nimport numpy as np # for handling numbers, arrays, and matrices\nimport matplotlib.pyplot as plt # plotting package\nimport seaborn as sns # plotting package\n\nExercise 0: Before we continue in this notebook with the next steps of the analysis, we need to load our results from the previous notebook using the sc.read_h5ad function and assign them to the variable name adata. Give it a try!\n\nClick for Answer\n\n\nAnswer: Load your data from the previous notebook into an h5ad object with sc.read_h5ad\n    adata = sc.read_h5ad(\"PBMC_analysis_SIB_tutorial3.h5ad\")\n\n\n\nFor simple integration tasks, it is worth first trying the algorithm Harmony. The harmonypy package is a port of the original tool in R. Harmony uses a variant of singular value decomposition (SVD) to embed the data, then look for mutual nearest neighborhoods of similar cells across batches in the embedding, which it then uses to correct the batch effect in a locally adaptive (non-linear) manner.\n\nimport harmonypy as hm\n\nExercise 1: Harmonypy operates on the principal components you previous obtained using the command sc.tl.pca. These are stored in your adata object under the .obsm field as X_pca. Can you extract these and store them in a variable named mat_PC in order to provide them directly to harmonypy in the following code cell?\n\nClick for Answer\n\n\nAnswer:\n    mat_PC = adata.obsm[\"X_pca\"]\n\n\n\n\nadata = sc.read_h5ad(\"PBMC_analysis_SIB_tutorial3.h5ad\")\nmat_PC = adata.obsm[\"X_pca\"]\n\nExercise 2: Next, we run harmony on the data to correct for any batch effects. We must specify with vars_use the metadata column name in adata.obs that we would like to correct for. Use the SHIFT+TAB option in Jupyter notebook to read about the harmony function’s required arguments, and run the command!\n\nClick for Answer\n\n\nAnswer:\n    ho = hm.run_harmony(data_mat = mat_PC, meta_data=adata.obs, vars_use=[\"sample\"])\n\n    2024-05-20 14:39:55,993 - harmonypy - INFO - Computing initial centroids with sklearn.KMeans...\n    2024-05-20 14:39:58,408 - harmonypy - INFO - sklearn.KMeans initialization complete.\n    2024-05-20 14:39:58,445 - harmonypy - INFO - Iteration 1 of 10\n    2024-05-20 14:39:59,630 - harmonypy - INFO - Iteration 2 of 10\n    2024-05-20 14:40:00,821 - harmonypy - INFO - Iteration 3 of 10\n    2024-05-20 14:40:01,552 - harmonypy - INFO - Iteration 4 of 10\n    2024-05-20 14:40:02,182 - harmonypy - INFO - Iteration 5 of 10\n    2024-05-20 14:40:02,729 - harmonypy - INFO - Iteration 6 of 10\n    2024-05-20 14:40:03,304 - harmonypy - INFO - Iteration 7 of 10\n    2024-05-20 14:40:03,904 - harmonypy - INFO - Iteration 8 of 10\n    2024-05-20 14:40:04,484 - harmonypy - INFO - Iteration 9 of 10\n    2024-05-20 14:40:05,013 - harmonypy - INFO - Iteration 10 of 10\n    2024-05-20 14:40:05,611 - harmonypy - INFO - Converged after 10 iterations\n\n\n\nWe can then visualize the extent to which each of the principal components was rescaled by harmonypy to correct for batch effects.\n\npc_std = np.std(ho.Z_corr, axis=1).tolist()\n\n\nsns.scatterplot(x=range(0, len(pc_std)), y=sorted(pc_std, reverse=True))\n\n\n\n\n\n\n\n\nFinally, we store the results in our adata as X_pcahm variable.\n\nadata.obsm[\"X_pcahm\"] = ho.Z_corr.transpose()\n\nExercise 3: Now that we have used harmony to correct for batch effects in our principal components, we want to rerun the sc.pp.neighbors and sc.tl.umap steps from our previous exercises, to obtain a new UMAP representation. Write those commands out below to obtain your new UMAP. Important: when running sc.pp.neighbors, you must specify that you would like to use the harmony-adjusted PCs and not the original, unadjusted ones. To do this, specify the optional argument use_rep=X_pcahm!\n\nClick for Answer\n\n\nAnswer:\n    sc.pp.neighbors(adata, n_pcs=30, use_rep=\"X_pcahm\")\n    sc.tl.umap(adata)\n\n\n\nExercise 4: Finally, visualize your UMAP with sc.pl.umap. How does the embedding compare to the one obtained prior to running harmony?\n\nsc.pl.umap(adata, color=[\"sample\"])\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py:394: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  cax = scatter(\n\n\n\n\n\n\n\n\n\n\nClick for Answer\n\n\nAnswer: The emedding appears to be more intermixed among the three samples. However, still for the PBMMC-2 sample, there are regions of the UMAP that appear much more occupied by cells from this sample alone. Given the differences during quality control and in expression of HBA1, perhaps there are greater populations of a particular cell type (i.e. erythrocytes) in PBMMC-2 compared to PBMMC-1 and PBMMC-3.\n\n\n\nadata.write_h5ad(\"PBMC_analysis_SIB_tutorial4.h5ad\") # save your results!"
  },
  {
    "objectID": "ipynb/day1-3_normalization_scaling.html",
    "href": "ipynb/day1-3_normalization_scaling.html",
    "title": "Normalization",
    "section": "",
    "text": "Download Presentation: Introduction to scanpy"
  },
  {
    "objectID": "ipynb/day1-3_normalization_scaling.html#learning-outcomes",
    "href": "ipynb/day1-3_normalization_scaling.html#learning-outcomes",
    "title": "Normalization",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nAfter having completed this chapter you will be able to:\n\nPerform size normalization and log transformation of single-cell data.\nIntuitively understand what “normalization” does to your data\nIdentifying highly variable gene features\nPerform data scaling\nAssign categorical cell cycle phases to single cells\n\n\nimport scanpy as sc\nimport pandas as pd # for handling data frames (i.e. data tables)\nimport numpy as np # for handling numbers, arrays, and matrices\nimport matplotlib.pyplot as plt # plotting package\nimport seaborn as sns # plotting package\n\nExercise 0: Before we continue in this notebook with the next steps of the analysis, we need to load our results from the previous notebook using the sc.read_h5ad function and assign them to the variable name adata. Give it a try!\n\nClick for Answer\n\n\nAnswer: Load your data from the previous notebook into an h5ad object with sc.read_h5ad\n    adata = sc.read_h5ad(\"PBMC_analysis_SIB_tutorial.h5ad\")"
  },
  {
    "objectID": "ipynb/day1-3_normalization_scaling.html#shift-logarithm-normalization",
    "href": "ipynb/day1-3_normalization_scaling.html#shift-logarithm-normalization",
    "title": "Normalization",
    "section": "Shift logarithm normalization",
    "text": "Shift logarithm normalization\nEach count in a count matrix represents the successful capture, reverse transcription and sequencing of a molecule of cellular mRNA. Count depths for identical cells can differ due to the variability inherent in each of these steps. Thus, when gene expression is compared between cells based on count data, any difference may have arisen solely due to sampling effects. Normalization addresses this issue by e.g. scaling count data to obtain correct relative gene expression abundances between cells.\nA recent benchmark published by Ahlmann-Eltze and Huber (2023) compared 22 different transformations for single-cell data, which surprisingly showed that a seemingly simple shifted logarithm transformation outperformed gold-standard SCTransform (the default method in analysis package’s such as Seurat in R).\nWe will thus use the shifted logarithm for normalizing our data, which is based on the delta method v\n\\[f(y) = \\log \\left( \\frac{y}{s} + y_0 \\right) \\]\nwith \\(y\\) being the raw counts, \\(s\\) being a so-called size factor and \\(y_0\\) describing a pseudo-count. The size factors are determined for each cell to account for variations in sampling effects and different cell sizes. The size factor for a cell \\(c\\) can be calculated by:\n\\[ s_c = \\frac{\\sum_g y_{gc}}{L} \\]\nwith \\(g\\) indexing different genes and \\(L\\) describing a target sum. There are different approaches to determine the size factors from the data. We will leverage the scanpy default in this section with \\(L\\) being the median raw count depth in the dataset. Many analysis templates use fixed values for \\(L\\), for example \\(L = 10^6\\), resulting in “counts per million” metric (CPM). For a beginner, these values may seem arbitrary, but it can lead to an overestimation of the variance. Indeed, Ahlmann-Eltze and Huber (2023) showed that log(CPM+1) performed poorly on a variety of tasks.\nThe shifted logarithm is a fast normalization technique, outperforms other methods for uncovering the latent structure of the dataset (especially when followed by principal component analysis) and works beneficial for stabilizing variance for subsequent dimensionality reduction and identification of differentially expressed genes.\nExercise 1: Perform normalization using the logarithm shift, following the above formula. Without relying on scanpy implemented function, you should be able to compute the log-normalized data, and store the resulting matrix in the object log_shifted_matrix below. We then provide code to display the distribution of the raw counts versus the normalized data (exercise courtesy of Alexandre Coudray).\n\nClick for Answer\n\n\nAnswer:\n    median_raw_counts = np.median(adata.obs['n_counts'])\n    size_factors = np.array(adata.obs['n_counts'] / median_raw_counts)\n    log_shifted_matrix = np.log(adata.X / size_factors[:,None] + 1) # we use a pseudo-count of 1"
  },
  {
    "objectID": "ipynb/day1-3_normalization_scaling.html#visualize-results-and-compare-to-scanpy-normalization",
    "href": "ipynb/day1-3_normalization_scaling.html#visualize-results-and-compare-to-scanpy-normalization",
    "title": "Normalization",
    "section": "Visualize results and compare to scanpy normalization",
    "text": "Visualize results and compare to scanpy normalization\nOnce you have performed the log-normalization shift, you can visualize your results here to see how the counts at adata.X have changed:\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\np1 = sns.histplot(adata.obs[\"n_counts\"], bins=100, kde=False, ax=axes[0])\naxes[0].set_title(\"Total counts\")\np2 = sns.histplot(log_shifted_matrix.sum(1), bins=100, kde=False, ax=axes[1])\naxes[1].set_title(\"Shifted logarithm\")\nplt.show()\n\n\n\n\n\n\n\n\nThe shifted logarithm can be conveniently called with scanpy by running pp.normalized_total with target_sum=None. We then apply a log transformation with a pseudo-count of 1, which can be easily done with the function sc.pp.log1p.\n\n# This can be easily done with scanpy normalize_total and log1p functions\nscales_counts = sc.pp.normalize_total(adata, target_sum=None, inplace=False)\n# log1p transform - log the data and adds a pseudo-count of 1\nscales_counts = sc.pp.log1p(scales_counts[\"X\"], copy=True)\n\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\np1 = sns.histplot(adata.obs[\"n_counts\"], bins=100, kde=False, ax=axes[0])\naxes[0].set_title(\"Total counts\")\np2 = sns.histplot(scales_counts.sum(1), bins=100, kde=False, ax=axes[1])\naxes[1].set_title(\"Shifted logarithm\")\nplt.show()\n\n\n\n\n\n\n\n\nWe verify that our normalization is the same as the one implemented in scanpy. You should have a corelation above 0.999999. The reason why we are not reaching a corelation of 1.0 comes from the precision in float number used by scanpy.\n\nfrom scipy.stats import pearsonr\n\npearsonr(np.array(scales_counts).flatten(),\n         np.array(log_shifted_matrix).flatten())\n\nPearsonRResult(statistic=0.999998708385195, pvalue=0.0)\n\n\nWe then run the normalized_total and log1p functions it again so that the normalized data is set as our default matrix in .X emplacement.\nOf course, in your own analysis, you can just use the two scanpy commands in the code cell above. However, this exercise aims as giving you a better understanding of the transformation being applied to your data! Some more complex machine learning based algorithms actually work more optimally on the raw, untransformed counts!\nExercise 2: Perform normalization and log transformation using the built in scanpy functions, as hinted at above.\n\nClick for Answer\n\n\nAnswer:\n    # To directly change the data 'in place', use the following:\n    sc.pp.normalize_total(adata, target_sum=None)\n    sc.pp.log1p(adata)"
  },
  {
    "objectID": "ipynb/day1-3_normalization_scaling.html#cell-cycle-characterization-and-regression",
    "href": "ipynb/day1-3_normalization_scaling.html#cell-cycle-characterization-and-regression",
    "title": "Normalization",
    "section": "Cell cycle characterization and regression",
    "text": "Cell cycle characterization and regression\nNow, we begin with dimensionality reduction, i.e. reducing the number of variables in the data by removing features (genes) with little variability among the cells and by combining highly similar features. This is important because normally you start with tens of thousands of genes and it is difficult to represent their patterns in a two-dimensional visualization.\nOne useful (but optional) step is to perform cell cycle characterization, as the cell cycle signature is often a strong convoluting factor with cell type signatures.\nExercise 3: Perform cell cycle characterization on your dataset. Plot a scatter plot of the S_score and G2M_score metadata created and stored in your AnnData object. Color the points by the assigned cell cycle phase. What percentage of your cells are in a proliferative state (S or G2M phases)?\n\n# your code here\nS_genes_mouse = np.array(['MCM5', 'PCNA', 'TYMS', 'FEN1', 'MCM2', 'MCM4', 'RRM1', 'UNG',\n       'GINS2', 'MCM6', 'CDCA7', 'DTL', 'PRIM1', 'UHRF1', 'CENPU',\n       'HELLS', 'RFC2', 'RPA2', 'NASP', 'RAD51AP1', 'GMNN', 'WDR76',\n       'SLBP', 'CCNE2', 'UBR7', 'POLD3', 'MSH2', 'ATAD2', 'RAD51', 'RRM2',\n       'CDC45', 'CDC6', 'EXO1', 'TIPIN', 'DSCC1', 'BLM', 'CASP8AP2',\n       'USP1', 'CLSPN', 'POLA1', 'CHAF1B', 'BRIP1', 'E2F8'])\nG2M_genes_mouse = np.array(['HMGB2', 'CDK1', 'NUSAP1', 'UBE2C', 'BIRC5', 'TPX2', 'TOP2A',\n       'NDC80', 'CKS2', 'NUF2', 'CKS1B', 'MKI67', 'TMPO', 'CENPF',\n       'TACC3', 'PIMREG', 'SMC4', 'CCNB2', 'CKAP2L', 'CKAP2', 'AURKB',\n       'BUB1', 'KIF11', 'ANP32E', 'TUBB4B', 'GTSE1', 'KIF20B', 'HJURP',\n       'CDCA3', 'JPT1', 'CDC20', 'TTK', 'CDC25C', 'KIF2C', 'RANGAP1',\n       'NCAPD2', 'DLGAP5', 'CDCA2', 'CDCA8', 'ECT2', 'KIF23', 'HMMR',\n       'AURKA', 'PSRC1', 'ANLN', 'LBR', 'CKAP5', 'CENPE', 'CTCF', 'NEK2',\n       'G2E3', 'GAS2L3', 'CBX5', 'CENPA'])\n\nsc.tl.score_genes_cell_cycle(adata, s_genes=S_genes_mouse, g2m_genes=G2M_genes_mouse)\n\nWARNING: genes are not in var_names and ignored: ['PIMREG', 'JPT1']\n\n\nThe gene lists provided here are the “standard” in single cell analysis as come from the two papers: - Buettner et al (2015) - Satija et al (2015)\n\nn2c = {\"G1\":\"red\", \"S\":\"green\", \"G2M\":\"blue\"} # use to assign each cell a color based on phase in the scatter plot\nplt.scatter(adata.obs['S_score'], adata.obs['G2M_score'], c=[n2c[k] for k in adata.obs['phase']])\nplt.xlabel('S score') ; plt.ylabel('G2M score')\nplt.show()\n\n\n\n\n\n\n\n\nExercise 4: How many cells do you have assigned to each of the cell cycle phases? Can you check this using a function you have applied in the previous exercises? What is the number of cells as a percentage of the total?\n\nClick for Answer\n\n\nAnswer: typing adata.obs[“phase”].value_counts() should return:\n    phase\n    G1      3129\n    S       1400\n    G2M     940\n    Name: count, dtype: int64\n  \nYou can then use adata.n_obs to divide the estimates by the total. You should get about:\n    phase\n    G1     0.592796\n    S      0.249954\n    G2M    0.157250"
  },
  {
    "objectID": "ipynb/day1-3_normalization_scaling.html#highly-variable-gene-selection",
    "href": "ipynb/day1-3_normalization_scaling.html#highly-variable-gene-selection",
    "title": "Normalization",
    "section": "Highly variable gene selection",
    "text": "Highly variable gene selection\nWe next calculate a subset of features that exhibit high cell-to-cell variation in the dataset (i.e, they are highly expressed in some cells, and lowly expressed in others). Genes that are similarly expressed in all cells will not assist with discriminating different cell types from each other.\nThe procedure in scanpy models the mean-variance relationship inherent in single-cell data, and is implemented in the sc.pp.highly_variable_genes function. By default, 2,000 genes (features) per dataset are returned and these will be used in downstream analysis, like PCA.\n\n# suggestion: start with 2000 or 3000 highly variable genes\nsc.pp.highly_variable_genes(adata, n_top_genes=3000)\n\n\nadata = adata[:, adata.var[\"highly_variable\"]].copy() # actually do the filtering\n\nWhile correcting for technical covariates may be crucial to uncovering the underlying biological signal, correction for biological covariates serves to single out particular biological signals of interest. The most common biological data correction is to remove the effects of the cell cycle on the transcriptome, the number of raw counts that existed per cell, or the percentage of mitochondrial reads present. This data correction can be performed by a simple linear regression against a cell cycle score as implemented in scanpy.\nExercise 5: Use scanpy’s regress_out function to remove the effect of the cell cycle phase metadata from your downstream analyses steps.\n\nClick for Answer\n\n\nAnswer: sc.pp.regress_out(adata, ‘phase’) is the command you want to apply here."
  },
  {
    "objectID": "ipynb/day1-3_normalization_scaling.html#scaling",
    "href": "ipynb/day1-3_normalization_scaling.html#scaling",
    "title": "Normalization",
    "section": "Scaling",
    "text": "Scaling\nNext, we apply scaling, a linear transformation that is a standard pre-processing step prior to dimensional reduction techniques like PCA. The sc.pp.scale function:\n\nshifts the expression of each gene, so that the mean expression across cells is 0\nscales the expression of each gene, so that the variance across cells is 1\nThis step gives equal weight in downstream analyses, so that highly-expressed genes do not dominate. The results of this are stored as the updated count matrix at adata.X, and the original means and standard deviations for each gene are stored as metadata variables in adata.var[\"mean\"] and adata.var[\"std\"].\n\n\nsc.pp.scale(adata)\n\nExercise 6: Can you use the commands adata.X.mean() to check whether this method is successfully scaling the mean of each gene to be equal to 0, and adata.X.std() to check whether this method is successfully scaling the standard deviation of each gene to be equal to 1? Important: don’t forget to take the mean for each gene by specifying axis=0!\n\nClick for Answer\n\n\nAnswer: typing adata.X.mean(axis=0) should return:\n    array([ 9.97964231e-17,  2.44618185e-17,  3.11863822e-17, ...,\n        4.98982115e-17, -1.55931911e-17, -7.17286791e-17])\n\nIn other words, values extremely close to zero.\nadata.X.std(axis=0) should return:\n    array([0.99997257, 0.99997257, 0.99997257, ..., 0.99997257, 0.99997257,\n       0.99997257])\n\nIn other words, values extremely close to one.\n\n\n\n# Save the dataset\n\n\nadata.write_h5ad(\"PBMC_analysis_SIB_tutorial2.h5ad\") # feel free to choose whichever file name you prefer!"
  },
  {
    "objectID": "ipynb/day1-1_setup.html",
    "href": "ipynb/day1-1_setup.html",
    "title": "Setup",
    "section": "",
    "text": "If you are enrolled in the course, log on the server with the provided link, username, and password. The environment on the server contains all the necessary software pre-installed."
  },
  {
    "objectID": "ipynb/day1-1_setup.html#downloading-the-course-data",
    "href": "ipynb/day1-1_setup.html#downloading-the-course-data",
    "title": "Setup",
    "section": "Downloading the course data",
    "text": "Downloading the course data\nTo download and extract the dataset, copy-paste these commands inside the terminal tab:\nwget https://single-cell-transcriptomics-python.s3.eu-central-1.amazonaws.com/course_data.tar.gz\ntar -xvf course_data.tar.gz\nrm course_data.tar.gz\n\nIf on Windows\nIf you’re using Windows, you can directly open the link in your browser, and downloading will start automatically. Unpack the tar.gz file in the directory where you want to work in during the course.\nHave a look at the data directory you have downloaded. It should contain the following:\ncourse_data\n├── count_matrices\n│   ├── ETV6-RUNX1_1\n│   │   └── outs\n│   │       └── filtered_feature_bc_matrix\n│   │           ├── barcodes.tsv.gz\n│   │           ├── features.tsv.gz\n│   │           └── matrix.mtx.gz\n│   ├── ETV6-RUNX1_2\n│   │   └── outs\n│   │       └── filtered_feature_bc_matrix\n│   │           ├── barcodes.tsv.gz\n│   │           ├── features.tsv.gz\n│   │           └── matrix.mtx.gz\n│   ├── ETV6-RUNX1_3\n│   │   └── outs\n│   │       └── filtered_feature_bc_matrix\n│   │           ├── barcodes.tsv.gz\n│   │           ├── features.tsv.gz\n│   │           └── matrix.mtx.gz\n│   ├── PBMMC_1\n│   │   └── outs\n│   │       └── filtered_feature_bc_matrix\n│   │           ├── barcodes.tsv.gz\n│   │           ├── features.tsv.gz\n│   │           └── matrix.mtx.gz\n│   ├── PBMMC_2\n│   │   └── outs\n│   │       └── filtered_feature_bc_matrix\n│   │           ├── barcodes.tsv.gz\n│   │           ├── features.tsv.gz\n│   │           └── matrix.mtx.gz\n│   └── PBMMC_3\n│       └── outs\n│           └── filtered_feature_bc_matrix\n│               ├── barcodes.tsv.gz\n│               ├── features.tsv.gz\n│               └── matrix.mtx.gz\n└── reads\n    ├── ETV6-RUNX1_1_S1_L001_I1_001.fastq.gz\n    ├── ETV6-RUNX1_1_S1_L001_R1_001.fastq.gz\n    └── ETV6-RUNX1_1_S1_L001_R2_001.fastq.gz\n\n20 directories, 21 files\nThis data comes from:\nCaron M, St-Onge P, Sontag T, Wang YC, Richer C, Ragoussis I, et al. Single-cell analysis of childhood leukemia reveals a link between developmental states and ribosomal protein expression as a source of intra-individual heterogeneity. Scientific Reports. 2020;10:1–12. Available from: http://dx.doi.org/10.1038/s41598-020-64929-x\nWe will use the reads to showcase the use of cellranger count. The directory contains only reads from chromosome 21 and 22 of one sample (ETV6-RUNX1_1). The count matrices are output of cellranger count, and we will use those for the other exercises in R."
  },
  {
    "objectID": "precourse_preparations.html",
    "href": "precourse_preparations.html",
    "title": "Pre-course preparations",
    "section": "",
    "text": "Participants should already have a basic knowledge in Next Generation Sequencing (NGS) techniques, or have already followed the course NGS - Quality control, Alignment, Visualisation. Knowledge in RNA sequencing is a plus. A basic knowledge of the Python programming language is required. Test your Python skills with the quiz here, before registering."
  },
  {
    "objectID": "precourse_preparations.html#required-competences",
    "href": "precourse_preparations.html#required-competences",
    "title": "Pre-course preparations",
    "section": "",
    "text": "Participants should already have a basic knowledge in Next Generation Sequencing (NGS) techniques, or have already followed the course NGS - Quality control, Alignment, Visualisation. Knowledge in RNA sequencing is a plus. A basic knowledge of the Python programming language is required. Test your Python skills with the quiz here, before registering."
  },
  {
    "objectID": "precourse_preparations.html#software",
    "href": "precourse_preparations.html#software",
    "title": "Pre-course preparations",
    "section": "Software",
    "text": "Software\nAttendees should have a Wi-Fi enabled computer. An online Python and Jupyter Notebook environment will be provided. However, in case you wish to perform the practical exercises on your own computer, please take a moment to install the following before the course:\n\nPython version &gt;= 3.8 installed through Anaconda.\nJupyter Notebook or the Anaconda Navigator\nThe Python packages necessary for the course. Find the script to install them here."
  },
  {
    "objectID": "course_schedule.html",
    "href": "course_schedule.html",
    "title": "Course schedule",
    "section": "",
    "text": "Note\n\n\n\nOther than the starting time, the time schedule is an approximation. Because we cannot plan a course by the minute, in practice the time points will deviate."
  },
  {
    "objectID": "course_schedule.html#day-1",
    "href": "course_schedule.html#day-1",
    "title": "Course schedule",
    "section": "Day 1",
    "text": "Day 1\n\n\n\nblock\nstart\nend\nsubject\n\n\n\n\nintroduction\n9:15 AM\n9:30 AM\nIntroduction\n\n\nblock 1\n9:30 AM\n10:15 AM\nIntroduction scRNAseq\n\n\n\n10:15 AM\n10:45 AM\nBREAK\n\n\nblock 2\n10:45 AM\n12:00 PM\nPresentation: Andreia Gouvêa, 10X Field Application Scientist\n\n\n\n12:00 PM\n1:00 PM\nBREAK\n\n\nblock 3\n1:00 PM\n3:00 PM\nCell Ranger\n\n\n\n3:00 PM\n3:30 PM\nBREAK\n\n\nblock 4\n3:30 PM\n5:00 PM\nQuality Control and Normalization and Scaling"
  },
  {
    "objectID": "course_schedule.html#day-2",
    "href": "course_schedule.html#day-2",
    "title": "Course schedule",
    "section": "Day 2",
    "text": "Day 2\n\n\n\nblock\nstart\nend\nsubject\n\n\n\n\nblock 1\n9:15 AM\n10:15 AM\nNormalization and Scaling\n\n\n\n10:15 AM\n10:45 AM\nBREAK\n\n\nblock 2\n10:45 AM\n12:15 PM\nDimensionality Reduction and Integration\n\n\n\n12:15 PM\n1:15 PM\nBREAK\n\n\nblock 3\n1:15 PM\n3:00 PM\nClustering and Visualization\n\n\n\n3:00 PM\n3:30 PM\nBREAK\n\n\nblock 4\n3:30 PM\n5:00 PM\nVisualization & Cell Annotation"
  },
  {
    "objectID": "course_schedule.html#day-3",
    "href": "course_schedule.html#day-3",
    "title": "Course schedule",
    "section": "Day 3",
    "text": "Day 3\n\n\n\nblock\nstart\nend\nsubject\n\n\n\n\nblock 1\n9:15 AM\n10:30 AM\nTranscription factor analysis\n\n\n\n10:30 AM\n11:00 AM\nBREAK\n\n\nblock 2\n11:00 AM\n12:30 PM\nTrajectory Inference\n\n\n\n12:30 PM\n1:30 PM\nBREAK\n\n\nblock 3\n1:30 PM\n3:00 PM\nRNA velocity\n\n\n\n3:00 PM\n3:30 PM\nBREAK\n\n\nblock 4\n3:30 PM\n5:00 PM\nRNA velocity"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Single Cell Transcriptomics with Python",
    "section": "",
    "text": "Alex Russell Lederer ORCiD\nGeert van Geest ORCiD\nTania Wyss ORCiD"
  },
  {
    "objectID": "index.html#teachers",
    "href": "index.html#teachers",
    "title": "Single Cell Transcriptomics with Python",
    "section": "",
    "text": "Alex Russell Lederer ORCiD\nGeert van Geest ORCiD\nTania Wyss ORCiD"
  },
  {
    "objectID": "index.html#attribution",
    "href": "index.html#attribution",
    "title": "Single Cell Transcriptomics with Python",
    "section": "Attribution",
    "text": "Attribution\nParts of this course are inspired by the Single Cell Best Practices Guide, previous R courses from the SIB on Single Cell Transcriptomics, and from the BC2 Conference 2023 Workshop. These previous course materials were prepared by Tania Wass, Rachel Marcone-Jeitziner, Geert van Geert, Patricia Palagi, Alex Lederer, and Alexandre Coudray.\nThe current course material here is still under active development, for questions please contact Alex Lederer"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Single Cell Transcriptomics with Python",
    "section": "Overview",
    "text": "Overview\nSingle-cell RNA sequencing (scRNA-seq) can measure the gene expression of complex biological systems at the level of individual cells, enabling scientists to generate detailed tissue atlases describing the transcriptomic profiles of thousands or even millions of cells. While scRNA-seq has become a popular technique in diverse fields of biological research, the required expertise for handling such datasets has restricted its use among the larger scientific community. The aim of this 3-day course is to empower researchers to start applying the fundamental scRNA-seq analysis pipeline to their own data. We will outline how to design and interpret results of a scRNA-seq dataset and explore the basics of preprocessing and analysis in Python on real data. We will discuss common concerns in the field, including preprocessing choices, dimensionality reduction, cell type clustering and identification, batch effect correction, pseudotime, and RNA velocity methods. The course will be taught in Python."
  },
  {
    "objectID": "index.html#license-copyright",
    "href": "index.html#license-copyright",
    "title": "Single Cell Transcriptomics with Python",
    "section": "License & copyright",
    "text": "License & copyright\nLicense: CC BY-SA 4.0\nCopyright: SIB Swiss Institute of Bioinformatics"
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "Single Cell Transcriptomics with Python",
    "section": "Learning outcomes",
    "text": "Learning outcomes\n\nGeneral learning outcomes\nBy the end of the course, participants will be able to:\n\nRun Cell Ranger\nEvaluate the quality of a scRNA-seq experiment\nPerform scanpy analysis on their own data\nConfidently communicate about how to overcome potential bottlenecks\n\n\n\nLearning outcomes explained\nTo reach the general learning outcomes above, we have set a number of smaller learning outcomes. Each chapter starts with these smaller learning outcomes. Use these at the start of a chapter to get an idea what you will learn. Use them also at the end of a chapter to evaluate whether you have learned what you were expected to learn."
  },
  {
    "objectID": "index.html#learning-experiences",
    "href": "index.html#learning-experiences",
    "title": "Single Cell Transcriptomics with Python",
    "section": "Learning experiences",
    "text": "Learning experiences\nTo reach the learning outcomes we will use lectures, exercises, polls, and group work. During exercises, you are free to discuss with other participants. During lectures, focus on the lecture only.\n\nExercises\nEach block has practical work involved. Some more than others. The practicals are subdivided into chapters, and we’ll have a (short) discussion after each chapter. All answers to the practicals are incorporated, but they are hidden. Do the exercise first by yourself, before checking out the answer. If your answer is different from the answer in the practicals, try to figure out why they are different."
  },
  {
    "objectID": "ipynb/day1-1_cellranger.html",
    "href": "ipynb/day1-1_cellranger.html",
    "title": "Cell Ranger",
    "section": "",
    "text": "Download Presentation: General introduction\nDownload Presentation: Introduction to scRNA-seq\nDownload Presentation: Introduction to Cell Ranger"
  },
  {
    "objectID": "ipynb/day1-1_cellranger.html#learning-outcomes",
    "href": "ipynb/day1-1_cellranger.html#learning-outcomes",
    "title": "Cell Ranger",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nAfter having completed this chapter you will be able to:\n\nExplain what kind of information single-cell RNA-seq (scRNA-seq) can give you to answer a biological question\nDescribe essential considerations during the design of a single-cell RNA-seq experiment\nDescribe the pros and cons of different single-cell sequencing methods\nUse cellranger to:\n\nTo align reads and generate count tables\nPerform basic QC on alignments and counts"
  },
  {
    "objectID": "ipynb/day1-1_cellranger.html#running-cellranger-count",
    "href": "ipynb/day1-1_cellranger.html#running-cellranger-count",
    "title": "Cell Ranger",
    "section": "Running cellranger count",
    "text": "Running cellranger count\nThe exercises below assume that you are enrolled in the course, and have access to the server. These exercises are not essential to run for the rest of the course, so you can skip them. If you want to do them anyway, you can try to install cellranger locally (only on Linux or WSL). In addition, you will need to download the references. You can get it by with this code (choose your OS):"
  },
  {
    "objectID": "ipynb/day1-1_cellranger.html#cloud-serverlinuxmacoswsl",
    "href": "ipynb/day1-1_cellranger.html#cloud-serverlinuxmacoswsl",
    "title": "Cell Ranger",
    "section": "Cloud server/Linux/MacOS/WSL",
    "text": "Cloud server/Linux/MacOS/WSL\nRun the following commands in the terminal or other command line prompt application:\n    wget https://single-cell-transcriptomics.s3.eu-central-1.amazonaws.com/cellranger_index.tar.gz\n    tar -xvf cellranger_index.tar.gz\n    rm cellranger_index.tar.gz"
  },
  {
    "objectID": "ipynb/day1-1_cellranger.html#windows-only-relevant-if-working-locally",
    "href": "ipynb/day1-1_cellranger.html#windows-only-relevant-if-working-locally",
    "title": "Cell Ranger",
    "section": "Windows (only relevant if working locally)",
    "text": "Windows (only relevant if working locally)\nIf you are working on the cloud server, follow the instructions above. Download using the link, and unpack in your working directory. This will download and extract the index in the current directory. Specify the path to this reference in the exercises accordingly."
  },
  {
    "objectID": "ipynb/day1-1_cellranger.html#data-overview",
    "href": "ipynb/day1-1_cellranger.html#data-overview",
    "title": "Cell Ranger",
    "section": "Data overview",
    "text": "Data overview\nHave a look in the directory course_data/reads and cellranger_index. In the reads directory you will find reads on one sample: ETV6-RUNX1_1. In the analysis part of the course we will work with six samples, but due to time and computational limitations we will run cellranger count on one of the samples, and only reads originating from chromsome 21 and 22.\nThe input you need to run cellranger count are the sequence reads and a reference. Here, we have prepared a reference only with chromosome 21 and 22, but in ‘real life’ you would of course get the full reference genome of your species. The reference has a specific format. You can download precomputed human and mouse references from the 10X website. If your species of interest is not one of those, you will have to generate it yourself. For that, have a look here.\nTo be able to run cellranger in the compute environment, first run:\nexport PATH=/data/cellranger-8.0.1:$PATH\nHave a look at the documentation of cellranger count (scroll down to Command-line argument reference).\nYou can find the input files here:\n\nreads: course_data/reads/ (from the downloaded tar package in your home directory)\npre-indexed reference: cellranger_index\n\nExercise 1: Fill out the missing arguments (at FIXME) in the script below, and run it:\ncellranger count \\\n--id=FIXME \\\n--sample=FIXME \\\n--transcriptome=FIXME \\\n--fastqs=FIXME \\\n--localcores=4 \\\n--create-bam=true\n\nThis will take a while…\nOnce started, the process will need approximately 15 minutes to finish. Have a coffee and/or have a look at the other exercises.\n\nClick for Answer\n\n\n    cellranger count \\\n    --id=ETV6-RUNX1_1 \\\n    --sample=ETV6-RUNX1_1 \\\n    --transcriptome=cellranger_index \\\n    --fastqs=course_data/reads \\\n    --localcores=4 \\\n    --create-bam=true\n\n\n\nOpening cellranger output\nHave a look out the output directory (i.e. ~/ETV6-RUNX1_1/outs). The analysis report (web_summary.html) is usually a good place to start.\nExercise 2: Have a good look inside web_summary.html. Anything that draws your attention? Is this report good enough to continue the analysis?\n\nClick for Answer\n\n\nNot really. First of all there is a warning: Fraction of RNA read bases with Q-score &gt;= 30 is low. This means that there is a low base quality of the reads. A low base quality gives results in more sequencing error and therefore possibly lower performance while mapping the reads to genes. However, a Q-score of 30 still represents 99.9% accuracy.\nWhat should worry us more is the number of reads per cell (363) and the sequencing saturation (7.9%). In most cases you should aim for 30.000 - 50.000 reads per cell (depending on the application). We therefore don’t have enough reads per cell. However, as you might remember, this was a subset of reads (1 million) mapped against chromosome 21 & 22, while the original dataset contains 210,987,037 reads. You can check out the original report at course_data/count_matrices/ETV6-RUNX1_1/outs/web_summary.html.\nFor more info on sequencing saturation, have a look here.\n\n\nUnderstanding the CellRanger output\nThe run summary from cellranger count can be viewed by clicking “Summary” in the top left corner. The summary metrics describe sequencing quality and various characteristics of the detected cells. Similar web summaries are also output from the cellranger reanalyze and cellranger aggr pipelines.\nThis report will serve you as first-line feedback on how the experiment went. It provides an easily accessible summary to scrutinize the success of the experiment.\nIt will help answering the questions like:\n\nWhat is the quality of the run?\nHow many cells do you have?\nIs the cell count estimate credible?\nWas the sample sequenced deep enough? Where the cells intact and well?\nIs the quality of the cells uniform?\n\nNote that some of these questions will be more definitively answered during a successive (more hands-on) part of quality control (QC) process. Consider this just the beginning of the scrutiny.\nBasic QC metrics\nThe number of cells detected, the mean reads per cell, and the median genes detected per cell are prominently displayed near the top of the page.\n\nEstimated Cell number: This is determined from the number of cell barcodes with a ‘reasonable’ numbers of observations. This number is an estimate because there is no binary flag “full/empty” that tell us if a droplet had a cell inside or not. Every droplet will enter in contact with some free-floating RNA, therefore some threshold needs to be set to cell-associated barcodes vs noise from empty GEMs. However, this threshold cannot be a fixed number as it will depend on the overall quality of the experiment, size of the cells and depth of the sequences and mis-called sequences. So, this number automatically estimated from the “Barcode Rank Plot” that we will see below.\nNote: this number is estimated using the thresholds to cellranger count as a bias, if the threshold is changed the count can give different predictions, and in some cases it will be necessary to do so. For example: to account for a not-so-successful experiment with high level of free-floating mRNA in the input cell suspension or a lysis caused mixing the RT mix with the cell suspension.\nMean Read per cell: This is the mean of sequencing reads that is obtained on average to the cells. Note that this refers only to the ones counted in “Estimated number of cells” and therefore:\nEstimated_Cell_number * Mean_Read_per_cell ≠ Illumina_Reads.\nAlso note that this number DOES NOT correspond to the number of UMI per cells (the value that is actually used for the analysis).\nExercise 3: On the basis of what learned in the lectures and above, can you explain how “Mean Reads per cell” and “UMI count” are related? How is “UMI count” obtained by the pipeline? Will doubling the number of reads double the number of UMIs?\n\nClick for Answer\n\n\nThe mean reads per cell is the number of FASTQ READS containing a cell barcode assigned to a particular cell. The UMI count is the number of unique RNA MOLECULES that were sequenced assigned to a particular gene and cell. The cellranger pipeline determines the UMI count using the UMI barcode, which is different from the cell barcode. The UMI barcode is unique to each distinct RNA molecule in a cell. So, there is a possibility of multiple reads containing the same UMI barcode and being “collapsed” into a single count for the UMI count.\nDoubling the number of reads will likely increase the number of UMIs, due to the detection of new RNA molecules with increased sequencing depth, but it will not double the number of UMIs because many additional reads will be assigned to the same RNA/UMI barcode.\n\n\nDiagnostics\n\nQC metrics – Sequencing\n\n\nNumber of Reads\nTotal number of read pairs that were assigned to this library in demultiplexing.\n\n\nValid Barcodes\nFraction of reads with barcodes that match the whitelist after barcode correction.\n\n\nSequencing Saturation\nThe fraction of reads originating from an already-observed UMI. This is a function of library complexity and sequencing depth. More specifically, this is the fraction of confidently mapped, valid cell-barcode, valid UMI reads that had a non-unique (cell-barcode, UMI, gene). This metric was called “cDNA PCR Duplication” in versions of Cell Ranger prior to 1.2.\n\n\nQ30 Bases in Barcode\nFraction of cell barcode bases with Q-score &gt;= 30, excluding very low quality/no-call (Q &lt;= 2) bases from the denominator.\n\n\nQ30 Bases in RNA Read\nFraction of RNA read bases with Q-score &gt;= 30, excluding very low quality/no-call (Q &lt;= 2) bases from the denominator. This is Read 1 for the Single Cell 3’ v1 chemistry and Read 2 for the Single Cell 3’ v2 chemistry.\n\n\nQ30 Bases in Sample Index\nFraction of sample index bases with Q-score &gt;= 30, excluding very low quality/no-call (Q &lt;= 2) bases from the denominator.\n\n\nQ30 Bases in UMI\nFraction of UMI bases with Q-score &gt;= 30, excluding very low quality/no-call (Q &lt;= 2) bases from the denominator.\n\n\n\nQC metrics – Mapping\n\n\nReads Mapped to Genome\nFraction of reads that mapped to the genome.\n\n\nReads Mapped Confidently to Genome\nFraction of reads that mapped uniquely to the genome. If a gene mapped to exonic loci from a single gene and also to non-exonic loci, it is considered uniquely mapped to one of the exonic loci.\n\n\nReads Mapped Confidently to Intergenic Regions\nFraction of reads that mapped uniquely to an intergenic region of the genome.\n\n\nReads Mapped Confidently to Intronic Regions\nFraction of reads that mapped uniquely to an intronic region of the genome.\n\n\nReads Mapped Confidently to Exonic Regions\nFraction of reads that mapped uniquely to an exonic region of the genome.\n\n\nReads Mapped Confidently to Transcriptome\nFraction of reads that mapped to a unique gene in the transcriptome. The read must be consistent with annotated splice junctions. These reads are considered for UMI counting.\n\n\nReads Mapped Antisense to Gene\nFraction of reads confidently mapped to the transcriptome, but on the opposite strand of their annotated gene. A read is counted as antisense if it has any alignments that are consistent with an exon of a transcript but antisense to it, and has no sense alignments.\n\n\n\n\nRanked Barcode Plot\nThe Barcode Rank Plot under the “Cells” dashboard shows the distribution of barcode counts and which barcodes were inferred to be associated with cells. It is one of the most informative QC plots, it enables one to assess sample quality and to formulate hypothesis of what might have gone wrong if the experiment was not perfectly successful.\nTo obtain this plot, reads are grouped by barcode, the number of UMI is counted, resulting in a vector of UMI count per barcode (note: one barcode - one GEM!). The counts are then sorted and the vector is displayed in rank vs counts plot:\nThe y-axis is the number of UMI counts mapped to each barcode and the x-axis is the number of barcodes below that value.\nNote that due to the high number of GEMs with at least one UMI the only way to visualize all the data is a log-log axes plot.\nHow does one interpret the plot? What to expect?\nIdeally there is a steep dropoff separating high UMI count cells from low UMI count background noise:\nA steep drop-off is indicative of good separation between the cell-associated barcodes and the barcodes associated with empty partitions.\nBarcodes can be determined to be cell-associated based on their UMI count or by their RNA profiles, therefore some regions of the graph can contain both cell-associated and background-associated barcodes.\nThe color of the graph represents the local density of barcodes that are cell-associated.\nIn fact, the cutoff is determined with a two-step procedure:\n\nIt uses a cutoff based on total UMI counts of each barcode to identify cells. This step identifies the primary mode of high RNA content cells.\nThen the algorithm uses the RNA profile of each remaining barcode to determine if it is an “empty” or a cell containing partition. This second step captures low RNA content cells, whose total UMI counts may be similar to empty GEMs.\n\n\n\nSaturation - is there a gain in sequencing more?\nThe sequencing saturation plot allows the user to assess the relative tradeoffs of sequencing deeper or shallower. As sequencing saturation increases, the total number of molecules detected in the library increases, but with diminishing returns as saturation is approached.\nA good rule of thumb for most cell types is that: An average of 40k reads per cell is a minimal sufficient that with 80k reads being usually an excellent depth. There is certainly gain in sequencing more but it is not cost-effective in general. So, it is important to evaluate if going deeper has a value to your scientific question.\n\n\n\nAnalysis view\nThe automated secondary analysis results can be viewed by clicking “Analysis” in the top left corner. The secondary analysis provides the following:\nA dimensional reduction analysis which projects the cells into a 2-D space (t-SNE), including an automated clustering analysis which groups together cells that have similar expression profiles.\n\nA list of genes that are differentially expressed between the selected clusters\nPlots showing the effect of decreased sequencing depth on observed library complexity and on median genes per cell detected\n\n\nTroubleshooting: when things go bad\n\n\nFor other situations like these two below, there is usually little you can do and you’d better contact 10X genomics support and/or the sequencing core facility at your institution\n\nExercise 4: We provide you with some web_summary.html files. Use what you have learned above to evaluate each one of the experiments and write a short evaluation of what you observe at least ~50 words per each file. When you write a short summary, imagine you are reporting to your supervisor about how the experiment went.\nClick the links and type Ctrl+S (Windows) or Cmd+S (Mac) to save the file to your computer. After that, open them in your browser.\n\n\nExperiment 1\n\n\nExperiment 2\n\n\nExperiment 3\n\n\nExperiment 4\n\n\nExperiment 5\n\n\nExperiment 6\n\n\nExperiment 7\n\n\nExperiment 8\n\n\nExperiment 9"
  },
  {
    "objectID": "ipynb/day1-1_cellranger.html#opening-cellranger-output",
    "href": "ipynb/day1-1_cellranger.html#opening-cellranger-output",
    "title": "Cell Ranger",
    "section": "Opening cellranger output",
    "text": "Opening cellranger output\nHave a look out the output directory (i.e. ~/ETV6-RUNX1_1/outs). The analysis report (web_summary.html) is usually a good place to start.\nExercise 2: Have a good look inside web_summary.html. Anything that draws your attention? Is this report good enough to continue the analysis?\n\nClick for Answer\n\n\nNot really. First of all there is a warning: Fraction of RNA read bases with Q-score &gt;= 30 is low. This means that there is a low base quality of the reads. A low base quality gives results in more sequencing error and therefore possibly lower performance while mapping the reads to genes. However, a Q-score of 30 still represents 99.9% accuracy.\nWhat should worry us more is the number of reads per cell (363) and the sequencing saturation (7.9%). In most cases you should aim for 30.000 - 50.000 reads per cell (depending on the application). We therefore don’t have enough reads per cell. However, as you might remember, this was a subset of reads (1 million) mapped against chromosome 21 & 22, while the original dataset contains 210,987,037 reads. You can check out the original report at course_data/count_matrices/ETV6-RUNX1_1/outs/web_summary.html.\nFor more info on sequencing saturation, have a look here.\n\n\nUnderstanding the CellRanger output\nThe run summary from cellranger count can be viewed by clicking “Summary” in the top left corner. The summary metrics describe sequencing quality and various characteristics of the detected cells. Similar web summaries are also output from the cellranger reanalyze and cellranger aggr pipelines.\nThis report will serve you as first-line feedback on how the experiment went. It provides an easily accessible summary to scrutinize the success of the experiment.\nIt will help answering the questions like:\n\nWhat is the quality of the run?\nHow many cells do you have?\nIs the cell count estimate credible?\nWas the sample sequenced deep enough? Where the cells intact and well?\nIs the quality of the cells uniform?\n\nNote that some of these questions will be more definitively answered during a successive (more hands-on) part of quality control (QC) process. Consider this just the beginning of the scrutiny.\nBasic QC metrics\nThe number of cells detected, the mean reads per cell, and the median genes detected per cell are prominently displayed near the top of the page.\n\nEstimated Cell number: This is determined from the number of cell barcodes with a ‘reasonable’ numbers of observations. This number is an estimate because there is no binary flag “full/empty” that tell us if a droplet had a cell inside or not. Every droplet will enter in contact with some free-floating RNA, therefore some threshold needs to be set to cell-associated barcodes vs noise from empty GEMs. However, this threshold cannot be a fixed number as it will depend on the overall quality of the experiment, size of the cells and depth of the sequences and mis-called sequences. So, this number automatically estimated from the “Barcode Rank Plot” that we will see below.\nNote: this number is estimated using the thresholds to cellranger count as a bias, if the threshold is changed the count can give different predictions, and in some cases it will be necessary to do so. For example: to account for a not-so-successful experiment with high level of free-floating mRNA in the input cell suspension or a lysis caused mixing the RT mix with the cell suspension.\nMean Read per cell: This is the mean of sequencing reads that is obtained on average to the cells. Note that this refers only to the ones counted in “Estimated number of cells” and therefore:\nEstimated_Cell_number * Mean_Read_per_cell ≠ Illumina_Reads.\nAlso note that this number DOES NOT correspond to the number of UMI per cells (the value that is actually used for the analysis).\nExercise 3: On the basis of what learned in the lectures and above, can you explain how “Mean Reads per cell” and “UMI count” are related? How is “UMI count” obtained by the pipeline? Will doubling the number of reads double the number of UMIs?\n\nClick for Answer\n\n\nThe mean reads per cell is the number of FASTQ READS containing a cell barcode assigned to a particular cell. The UMI count is the number of unique RNA MOLECULES that were sequenced assigned to a particular gene and cell. The cellranger pipeline determines the UMI count using the UMI barcode, which is different from the cell barcode. The UMI barcode is unique to each distinct RNA molecule in a cell. So, there is a possibility of multiple reads containing the same UMI barcode and being “collapsed” into a single count for the UMI count.\nDoubling the number of reads will likely increase the number of UMIs, due to the detection of new RNA molecules with increased sequencing depth, but it will not double the number of UMIs because many additional reads will be assigned to the same RNA/UMI barcode.\n\n\nDiagnostics\n\nQC metrics – Sequencing\n\n\nNumber of Reads\nTotal number of read pairs that were assigned to this library in demultiplexing.\n\n\nValid Barcodes\nFraction of reads with barcodes that match the whitelist after barcode correction.\n\n\nSequencing Saturation\nThe fraction of reads originating from an already-observed UMI. This is a function of library complexity and sequencing depth. More specifically, this is the fraction of confidently mapped, valid cell-barcode, valid UMI reads that had a non-unique (cell-barcode, UMI, gene). This metric was called “cDNA PCR Duplication” in versions of Cell Ranger prior to 1.2.\n\n\nQ30 Bases in Barcode\nFraction of cell barcode bases with Q-score &gt;= 30, excluding very low quality/no-call (Q &lt;= 2) bases from the denominator.\n\n\nQ30 Bases in RNA Read\nFraction of RNA read bases with Q-score &gt;= 30, excluding very low quality/no-call (Q &lt;= 2) bases from the denominator. This is Read 1 for the Single Cell 3’ v1 chemistry and Read 2 for the Single Cell 3’ v2 chemistry.\n\n\nQ30 Bases in Sample Index\nFraction of sample index bases with Q-score &gt;= 30, excluding very low quality/no-call (Q &lt;= 2) bases from the denominator.\n\n\nQ30 Bases in UMI\nFraction of UMI bases with Q-score &gt;= 30, excluding very low quality/no-call (Q &lt;= 2) bases from the denominator.\n\n\n\nQC metrics – Mapping\n\n\nReads Mapped to Genome\nFraction of reads that mapped to the genome.\n\n\nReads Mapped Confidently to Genome\nFraction of reads that mapped uniquely to the genome. If a gene mapped to exonic loci from a single gene and also to non-exonic loci, it is considered uniquely mapped to one of the exonic loci.\n\n\nReads Mapped Confidently to Intergenic Regions\nFraction of reads that mapped uniquely to an intergenic region of the genome.\n\n\nReads Mapped Confidently to Intronic Regions\nFraction of reads that mapped uniquely to an intronic region of the genome.\n\n\nReads Mapped Confidently to Exonic Regions\nFraction of reads that mapped uniquely to an exonic region of the genome.\n\n\nReads Mapped Confidently to Transcriptome\nFraction of reads that mapped to a unique gene in the transcriptome. The read must be consistent with annotated splice junctions. These reads are considered for UMI counting.\n\n\nReads Mapped Antisense to Gene\nFraction of reads confidently mapped to the transcriptome, but on the opposite strand of their annotated gene. A read is counted as antisense if it has any alignments that are consistent with an exon of a transcript but antisense to it, and has no sense alignments.\n\n\n\n\nRanked Barcode Plot\nThe Barcode Rank Plot under the “Cells” dashboard shows the distribution of barcode counts and which barcodes were inferred to be associated with cells. It is one of the most informative QC plots, it enables one to assess sample quality and to formulate hypothesis of what might have gone wrong if the experiment was not perfectly successful.\nTo obtain this plot, reads are grouped by barcode, the number of UMI is counted, resulting in a vector of UMI count per barcode (note: one barcode - one GEM!). The counts are then sorted and the vector is displayed in rank vs counts plot:\nThe y-axis is the number of UMI counts mapped to each barcode and the x-axis is the number of barcodes below that value.\nNote that due to the high number of GEMs with at least one UMI the only way to visualize all the data is a log-log axes plot.\nHow does one interpret the plot? What to expect?\nIdeally there is a steep dropoff separating high UMI count cells from low UMI count background noise:\nA steep drop-off is indicative of good separation between the cell-associated barcodes and the barcodes associated with empty partitions.\nBarcodes can be determined to be cell-associated based on their UMI count or by their RNA profiles, therefore some regions of the graph can contain both cell-associated and background-associated barcodes.\nThe color of the graph represents the local density of barcodes that are cell-associated.\nIn fact, the cutoff is determined with a two-step procedure:\n\nIt uses a cutoff based on total UMI counts of each barcode to identify cells. This step identifies the primary mode of high RNA content cells.\nThen the algorithm uses the RNA profile of each remaining barcode to determine if it is an “empty” or a cell containing partition. This second step captures low RNA content cells, whose total UMI counts may be similar to empty GEMs.\n\n\n\nSaturation - is there a gain in sequencing more?\nThe sequencing saturation plot allows the user to assess the relative tradeoffs of sequencing deeper or shallower. As sequencing saturation increases, the total number of molecules detected in the library increases, but with diminishing returns as saturation is approached.\nA good rule of thumb for most cell types is that: An average of 40k reads per cell is a minimal sufficient that with 80k reads being usually an excellent depth. There is certainly gain in sequencing more but it is not cost-effective in general. So, it is important to evaluate if going deeper has a value to your scientific question.\n\n\n\nAnalysis view\nThe automated secondary analysis results can be viewed by clicking “Analysis” in the top left corner. The secondary analysis provides the following:\nA dimensional reduction analysis which projects the cells into a 2-D space (t-SNE), including an automated clustering analysis which groups together cells that have similar expression profiles.\n\nA list of genes that are differentially expressed between the selected clusters\nPlots showing the effect of decreased sequencing depth on observed library complexity and on median genes per cell detected\n\n\nTroubleshooting: when things go bad\n\n\nFor other situations like these two below, there is usually little you can do and you’d better contact 10X genomics support and/or the sequencing core facility at your institution\n\nExercise 4: We provide you with some web_summary.html files. Use what you have learned above to evaluate each one of the experiments and write a short evaluation of what you observe at least ~50 words per each file. When you write a short summary, imagine you are reporting to your supervisor about how the experiment went.\nClick the links and type Ctrl+S (Windows) or Cmd+S (Mac) to save the file to your computer. After that, open them in your browser.\n\n\nExperiment 1\n\n\nExperiment 2\n\n\nExperiment 3\n\n\nExperiment 4\n\n\nExperiment 5\n\n\nExperiment 6\n\n\nExperiment 7\n\n\nExperiment 8\n\n\nExperiment 9"
  },
  {
    "objectID": "ipynb/day1-1_cellranger.html#understanding-the-cellranger-output",
    "href": "ipynb/day1-1_cellranger.html#understanding-the-cellranger-output",
    "title": "Cell Ranger",
    "section": "Understanding the CellRanger output",
    "text": "Understanding the CellRanger output\nThe run summary from cellranger count can be viewed by clicking “Summary” in the top left corner. The summary metrics describe sequencing quality and various characteristics of the detected cells. Similar web summaries are also output from the cellranger reanalyze and cellranger aggr pipelines.\nThis report will serve you as first-line feedback on how the experiment went. It provides an easily accessible summary to scrutinize the success of the experiment.\nIt will help answering the questions like:\n\nWhat is the quality of the run?\nHow many cells do you have?\nIs the cell count estimate credible?\nWas the sample sequenced deep enough? Where the cells intact and well?\nIs the quality of the cells uniform?\n\nNote that some of these questions will be more definitively answered during a successive (more hands-on) part of quality control (QC) process. Consider this just the beginning of the scrutiny.\nBasic QC metrics\nThe number of cells detected, the mean reads per cell, and the median genes detected per cell are prominently displayed near the top of the page.\n\nEstimated Cell number: This is determined from the number of cell barcodes with a ‘reasonable’ numbers of observations. This number is an estimate because there is no binary flag “full/empty” that tell us if a droplet had a cell inside or not. Every droplet will enter in contact with some free-floating RNA, therefore some threshold needs to be set to cell-associated barcodes vs noise from empty GEMs. However, this threshold cannot be a fixed number as it will depend on the overall quality of the experiment, size of the cells and depth of the sequences and mis-called sequences. So, this number automatically estimated from the “Barcode Rank Plot” that we will see below.\nNote: this number is estimated using the thresholds to cellranger count as a bias, if the threshold is changed the count can give different predictions, and in some cases it will be necessary to do so. For example: to account for a not-so-successful experiment with high level of free-floating mRNA in the input cell suspension or a lysis caused mixing the RT mix with the cell suspension.\nMean Read per cell: This is the mean of sequencing reads that is obtained on average to the cells. Note that this refers only to the ones counted in “Estimated number of cells” and therefore:\nEstimated_Cell_number * Mean_Read_per_cell ≠ Illumina_Reads.\nAlso note that this number DOES NOT correspond to the number of UMI per cells (the value that is actually used for the analysis).\nExercise 3: On the basis of what learned in the lectures and above, can you explain how “Mean Reads per cell” and “UMI count” are related? How is “UMI count” obtained by the pipeline? Will doubling the number of reads double the number of UMIs?\n\nClick for Answer\n\n\nThe mean reads per cell is the number of FASTQ READS containing a cell barcode assigned to a particular cell. The UMI count is the number of unique RNA MOLECULES that were sequenced assigned to a particular gene and cell. The cellranger pipeline determines the UMI count using the UMI barcode, which is different from the cell barcode. The UMI barcode is unique to each distinct RNA molecule in a cell. So, there is a possibility of multiple reads containing the same UMI barcode and being “collapsed” into a single count for the UMI count.\nDoubling the number of reads will likely increase the number of UMIs, due to the detection of new RNA molecules with increased sequencing depth, but it will not double the number of UMIs because many additional reads will be assigned to the same RNA/UMI barcode.\n\n\nDiagnostics\n\nQC metrics – Sequencing\n\n\nNumber of Reads\nTotal number of read pairs that were assigned to this library in demultiplexing.\n\n\nValid Barcodes\nFraction of reads with barcodes that match the whitelist after barcode correction.\n\n\nSequencing Saturation\nThe fraction of reads originating from an already-observed UMI. This is a function of library complexity and sequencing depth. More specifically, this is the fraction of confidently mapped, valid cell-barcode, valid UMI reads that had a non-unique (cell-barcode, UMI, gene). This metric was called “cDNA PCR Duplication” in versions of Cell Ranger prior to 1.2.\n\n\nQ30 Bases in Barcode\nFraction of cell barcode bases with Q-score &gt;= 30, excluding very low quality/no-call (Q &lt;= 2) bases from the denominator.\n\n\nQ30 Bases in RNA Read\nFraction of RNA read bases with Q-score &gt;= 30, excluding very low quality/no-call (Q &lt;= 2) bases from the denominator. This is Read 1 for the Single Cell 3’ v1 chemistry and Read 2 for the Single Cell 3’ v2 chemistry.\n\n\nQ30 Bases in Sample Index\nFraction of sample index bases with Q-score &gt;= 30, excluding very low quality/no-call (Q &lt;= 2) bases from the denominator.\n\n\nQ30 Bases in UMI\nFraction of UMI bases with Q-score &gt;= 30, excluding very low quality/no-call (Q &lt;= 2) bases from the denominator.\n\n\n\nQC metrics – Mapping\n\n\nReads Mapped to Genome\nFraction of reads that mapped to the genome.\n\n\nReads Mapped Confidently to Genome\nFraction of reads that mapped uniquely to the genome. If a gene mapped to exonic loci from a single gene and also to non-exonic loci, it is considered uniquely mapped to one of the exonic loci.\n\n\nReads Mapped Confidently to Intergenic Regions\nFraction of reads that mapped uniquely to an intergenic region of the genome.\n\n\nReads Mapped Confidently to Intronic Regions\nFraction of reads that mapped uniquely to an intronic region of the genome.\n\n\nReads Mapped Confidently to Exonic Regions\nFraction of reads that mapped uniquely to an exonic region of the genome.\n\n\nReads Mapped Confidently to Transcriptome\nFraction of reads that mapped to a unique gene in the transcriptome. The read must be consistent with annotated splice junctions. These reads are considered for UMI counting.\n\n\nReads Mapped Antisense to Gene\nFraction of reads confidently mapped to the transcriptome, but on the opposite strand of their annotated gene. A read is counted as antisense if it has any alignments that are consistent with an exon of a transcript but antisense to it, and has no sense alignments.\n\n\n\n\nRanked Barcode Plot\nThe Barcode Rank Plot under the “Cells” dashboard shows the distribution of barcode counts and which barcodes were inferred to be associated with cells. It is one of the most informative QC plots, it enables one to assess sample quality and to formulate hypothesis of what might have gone wrong if the experiment was not perfectly successful.\nTo obtain this plot, reads are grouped by barcode, the number of UMI is counted, resulting in a vector of UMI count per barcode (note: one barcode - one GEM!). The counts are then sorted and the vector is displayed in rank vs counts plot:\nThe y-axis is the number of UMI counts mapped to each barcode and the x-axis is the number of barcodes below that value.\nNote that due to the high number of GEMs with at least one UMI the only way to visualize all the data is a log-log axes plot.\nHow does one interpret the plot? What to expect?\nIdeally there is a steep dropoff separating high UMI count cells from low UMI count background noise:\nA steep drop-off is indicative of good separation between the cell-associated barcodes and the barcodes associated with empty partitions.\nBarcodes can be determined to be cell-associated based on their UMI count or by their RNA profiles, therefore some regions of the graph can contain both cell-associated and background-associated barcodes.\nThe color of the graph represents the local density of barcodes that are cell-associated.\nIn fact, the cutoff is determined with a two-step procedure:\n\nIt uses a cutoff based on total UMI counts of each barcode to identify cells. This step identifies the primary mode of high RNA content cells.\nThen the algorithm uses the RNA profile of each remaining barcode to determine if it is an “empty” or a cell containing partition. This second step captures low RNA content cells, whose total UMI counts may be similar to empty GEMs.\n\n\n\nSaturation - is there a gain in sequencing more?\nThe sequencing saturation plot allows the user to assess the relative tradeoffs of sequencing deeper or shallower. As sequencing saturation increases, the total number of molecules detected in the library increases, but with diminishing returns as saturation is approached.\nA good rule of thumb for most cell types is that: An average of 40k reads per cell is a minimal sufficient that with 80k reads being usually an excellent depth. There is certainly gain in sequencing more but it is not cost-effective in general. So, it is important to evaluate if going deeper has a value to your scientific question.\n\n\n\nAnalysis view\nThe automated secondary analysis results can be viewed by clicking “Analysis” in the top left corner. The secondary analysis provides the following:\nA dimensional reduction analysis which projects the cells into a 2-D space (t-SNE), including an automated clustering analysis which groups together cells that have similar expression profiles.\n\nA list of genes that are differentially expressed between the selected clusters\nPlots showing the effect of decreased sequencing depth on observed library complexity and on median genes per cell detected\n\n\nTroubleshooting: when things go bad\n\n\nFor other situations like these two below, there is usually little you can do and you’d better contact 10X genomics support and/or the sequencing core facility at your institution\n\nExercise 4: We provide you with some web_summary.html files. Use what you have learned above to evaluate each one of the experiments and write a short evaluation of what you observe at least ~50 words per each file. When you write a short summary, imagine you are reporting to your supervisor about how the experiment went.\nClick the links and type Ctrl+S (Windows) or Cmd+S (Mac) to save the file to your computer. After that, open them in your browser.\n\n\nExperiment 1\n\n\nExperiment 2\n\n\nExperiment 3\n\n\nExperiment 4\n\n\nExperiment 5\n\n\nExperiment 6\n\n\nExperiment 7\n\n\nExperiment 8\n\n\nExperiment 9"
  },
  {
    "objectID": "ipynb/day1-1_cellranger.html#diagnostics",
    "href": "ipynb/day1-1_cellranger.html#diagnostics",
    "title": "Cell Ranger",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nQC metrics – Sequencing\n\n\nNumber of Reads\nTotal number of read pairs that were assigned to this library in demultiplexing.\n\n\nValid Barcodes\nFraction of reads with barcodes that match the whitelist after barcode correction.\n\n\nSequencing Saturation\nThe fraction of reads originating from an already-observed UMI. This is a function of library complexity and sequencing depth. More specifically, this is the fraction of confidently mapped, valid cell-barcode, valid UMI reads that had a non-unique (cell-barcode, UMI, gene). This metric was called “cDNA PCR Duplication” in versions of Cell Ranger prior to 1.2.\n\n\nQ30 Bases in Barcode\nFraction of cell barcode bases with Q-score &gt;= 30, excluding very low quality/no-call (Q &lt;= 2) bases from the denominator.\n\n\nQ30 Bases in RNA Read\nFraction of RNA read bases with Q-score &gt;= 30, excluding very low quality/no-call (Q &lt;= 2) bases from the denominator. This is Read 1 for the Single Cell 3’ v1 chemistry and Read 2 for the Single Cell 3’ v2 chemistry.\n\n\nQ30 Bases in Sample Index\nFraction of sample index bases with Q-score &gt;= 30, excluding very low quality/no-call (Q &lt;= 2) bases from the denominator.\n\n\nQ30 Bases in UMI\nFraction of UMI bases with Q-score &gt;= 30, excluding very low quality/no-call (Q &lt;= 2) bases from the denominator.\n\n\n\nQC metrics – Mapping\n\n\nReads Mapped to Genome\nFraction of reads that mapped to the genome.\n\n\nReads Mapped Confidently to Genome\nFraction of reads that mapped uniquely to the genome. If a gene mapped to exonic loci from a single gene and also to non-exonic loci, it is considered uniquely mapped to one of the exonic loci.\n\n\nReads Mapped Confidently to Intergenic Regions\nFraction of reads that mapped uniquely to an intergenic region of the genome.\n\n\nReads Mapped Confidently to Intronic Regions\nFraction of reads that mapped uniquely to an intronic region of the genome.\n\n\nReads Mapped Confidently to Exonic Regions\nFraction of reads that mapped uniquely to an exonic region of the genome.\n\n\nReads Mapped Confidently to Transcriptome\nFraction of reads that mapped to a unique gene in the transcriptome. The read must be consistent with annotated splice junctions. These reads are considered for UMI counting.\n\n\nReads Mapped Antisense to Gene\nFraction of reads confidently mapped to the transcriptome, but on the opposite strand of their annotated gene. A read is counted as antisense if it has any alignments that are consistent with an exon of a transcript but antisense to it, and has no sense alignments."
  },
  {
    "objectID": "ipynb/day1-1_cellranger.html#ranked-barcode-plot",
    "href": "ipynb/day1-1_cellranger.html#ranked-barcode-plot",
    "title": "Cell Ranger",
    "section": "Ranked Barcode Plot",
    "text": "Ranked Barcode Plot\nThe Barcode Rank Plot under the “Cells” dashboard shows the distribution of barcode counts and which barcodes were inferred to be associated with cells. It is one of the most informative QC plots, it enables one to assess sample quality and to formulate hypothesis of what might have gone wrong if the experiment was not perfectly successful.\nTo obtain this plot, reads are grouped by barcode, the number of UMI is counted, resulting in a vector of UMI count per barcode (note: one barcode - one GEM!). The counts are then sorted and the vector is displayed in rank vs counts plot:\nThe y-axis is the number of UMI counts mapped to each barcode and the x-axis is the number of barcodes below that value.\nNote that due to the high number of GEMs with at least one UMI the only way to visualize all the data is a log-log axes plot.\nHow does one interpret the plot? What to expect?\nIdeally there is a steep dropoff separating high UMI count cells from low UMI count background noise:\nA steep drop-off is indicative of good separation between the cell-associated barcodes and the barcodes associated with empty partitions.\nBarcodes can be determined to be cell-associated based on their UMI count or by their RNA profiles, therefore some regions of the graph can contain both cell-associated and background-associated barcodes.\nThe color of the graph represents the local density of barcodes that are cell-associated.\nIn fact, the cutoff is determined with a two-step procedure:\n\nIt uses a cutoff based on total UMI counts of each barcode to identify cells. This step identifies the primary mode of high RNA content cells.\nThen the algorithm uses the RNA profile of each remaining barcode to determine if it is an “empty” or a cell containing partition. This second step captures low RNA content cells, whose total UMI counts may be similar to empty GEMs."
  },
  {
    "objectID": "ipynb/day1-1_cellranger.html#saturation---is-there-a-gain-in-sequencing-more",
    "href": "ipynb/day1-1_cellranger.html#saturation---is-there-a-gain-in-sequencing-more",
    "title": "Cell Ranger",
    "section": "Saturation - is there a gain in sequencing more?",
    "text": "Saturation - is there a gain in sequencing more?\nThe sequencing saturation plot allows the user to assess the relative tradeoffs of sequencing deeper or shallower. As sequencing saturation increases, the total number of molecules detected in the library increases, but with diminishing returns as saturation is approached.\nA good rule of thumb for most cell types is that: An average of 40k reads per cell is a minimal sufficient that with 80k reads being usually an excellent depth. There is certainly gain in sequencing more but it is not cost-effective in general. So, it is important to evaluate if going deeper has a value to your scientific question."
  },
  {
    "objectID": "ipynb/day1-1_cellranger.html#analysis-view",
    "href": "ipynb/day1-1_cellranger.html#analysis-view",
    "title": "Cell Ranger",
    "section": "Analysis view",
    "text": "Analysis view\nThe automated secondary analysis results can be viewed by clicking “Analysis” in the top left corner. The secondary analysis provides the following:\nA dimensional reduction analysis which projects the cells into a 2-D space (t-SNE), including an automated clustering analysis which groups together cells that have similar expression profiles.\n\nA list of genes that are differentially expressed between the selected clusters\nPlots showing the effect of decreased sequencing depth on observed library complexity and on median genes per cell detected"
  },
  {
    "objectID": "ipynb/day1-1_cellranger.html#troubleshooting-when-things-go-bad",
    "href": "ipynb/day1-1_cellranger.html#troubleshooting-when-things-go-bad",
    "title": "Cell Ranger",
    "section": "Troubleshooting: when things go bad",
    "text": "Troubleshooting: when things go bad\n\n\nFor other situations like these two below, there is usually little you can do and you’d better contact 10X genomics support and/or the sequencing core facility at your institution\n\nExercise 4: We provide you with some web_summary.html files. Use what you have learned above to evaluate each one of the experiments and write a short evaluation of what you observe at least ~50 words per each file. When you write a short summary, imagine you are reporting to your supervisor about how the experiment went.\nClick the links and type Ctrl+S (Windows) or Cmd+S (Mac) to save the file to your computer. After that, open them in your browser.\n\n\nExperiment 1\n\n\nExperiment 2\n\n\nExperiment 3\n\n\nExperiment 4\n\n\nExperiment 5\n\n\nExperiment 6\n\n\nExperiment 7\n\n\nExperiment 8\n\n\nExperiment 9"
  },
  {
    "objectID": "ipynb/day1-2_analysis_tools_qc.html",
    "href": "ipynb/day1-2_analysis_tools_qc.html",
    "title": "Analysis tools and quality control (QC)",
    "section": "",
    "text": "Download Presentation: Introduction to scanpy\nIn this exercise, you will begin to learn about the standard workflow for analyzing scRNA-seq count data in Python. As single cell data is complex and often tailored to the particular experimental design, so there is not one “correct” approach to analyzing these data. However, certain steps have become accepted as a sort of standard “best practice.”\nA useful overview on the current best practices is found in the articles below, which we also borrow from in this tutorial. We thank the authors for compiling such handy resources!\nCurrent best practices in single-cell RNA-seq analysis are explained in a recent Nature Review\nAccompanying this review is an online webpage, which is still under development but can be quite handy nonetheless:"
  },
  {
    "objectID": "ipynb/day1-2_analysis_tools_qc.html#learning-outcomes",
    "href": "ipynb/day1-2_analysis_tools_qc.html#learning-outcomes",
    "title": "Analysis tools and quality control (QC)",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nAfter having completed this chapter you will be able to:\n\nLoad single cell data into Python.\nExplain the basic structure of a AnnData object and extract count data and metadata.\nCalculate and visualize quality measures based on:\n\nmitochondrial genes\nribosomal genes\nhemoglobin genes\nrelative gene expression\n\nInterpret the above quality measures per cell.\nPerform cell filtering based on user-selected quality thresholds."
  },
  {
    "objectID": "ipynb/day1-2_analysis_tools_qc.html#loading-scrnaseq-data",
    "href": "ipynb/day1-2_analysis_tools_qc.html#loading-scrnaseq-data",
    "title": "Analysis tools and quality control (QC)",
    "section": "Loading scRNAseq data",
    "text": "Loading scRNAseq data\nAfter the generation of the count matrices with cellranger, the next step is the data analysis. The scanpy package is currently the most popular software in Python to do this. To start working with scanpy, you must import the package into your Jupyter notebook as follows:\n\nimport scanpy as sc\n\nAn excellent resource for documentation on scanpy can be found on the software page at the following link.\nThere are some supplemental packages for data handling and visualization that are also very useful to import into your notebook as well.\n\nimport pandas as pd # for handling data frames (i.e. data tables)\nimport numpy as np # for handling numbers, arrays, and matrices\nimport matplotlib.pyplot as plt # plotting package\n\nFirst, we will load a file specifying the different samples, and create a dictionary “datadirs” specifying the location of the count data:\n\nsample_info = pd.read_csv(\"course_data/sample_info_course.csv\")\n\ndatadirs = {}\nfor sample_name in sample_info[\"SampleName\"]:\n    if \"PBMMC\" in sample_name:\n        datadirs[sample_name] = \"course_data/count_matrices/\" + sample_name + \"/outs/filtered_feature_bc_matrix\"\n\nTo run through a typical scanpy analysis, we will use the files that are in the directory outs/filtered_feature_bc_matrix. This directory is part of the output generated by CellRanger.\nWe will use the list of file paths generated in the previous step to load each sample into a separate AnnData object. We will then store all six of those samples in a list called adatas, and combine them into a single AnnData object for our analysis.\n\nadatas = []\nfor sample in datadirs.keys():\n    print(\"Loading: \", sample)\n    curr_adata = sc.read_10x_mtx(datadirs[sample]) # load file into an AnnData object\n    curr_adata.obs[\"sample\"] = sample\n    curr_adata.X = curr_adata.X.toarray()\n    adatas.append(curr_adata)\n    \nadata = sc.concat(adatas) # combine all samples into a single AnnData object\nadata.obs_names_make_unique() # make sure each cell barcode has a unique identifier\n\nLoading:  PBMMC_1\nLoading:  PBMMC_2\nLoading:  PBMMC_3\n\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/anndata/_core/anndata.py:1838: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n  utils.warn_names_duplicates(\"obs\")\n\n\nThe AnnData object is similar to a detailed spreadsheet! Some basic commands to view the object are shown below. For a new dataset, there will be little to no metdata other than Cell IDs and gene names, but as you perform analyses, the metadata fields will be populated with more detail.\nExercise 1: Check what’s in the adata object, by typing adata in the Python console. How many gene features are in there? And how many cells?\n\nClick for Answer\n\n\nAnswer: typing adata should return:\n    AnnData object with n_obs × n_vars = 6946 × 33694\n    obs: 'sample'\n\nYou can also confirm the number of observations (cells) and variables (genes/features) using the commands below:\n    adata.n_obs # number of cells\n    adata.n_vars # number of genes\n\n\n\nThe AnnData object\nThe adata object we have created has the class AnnData. The object contains the single-cell count matrix, accessible with the command adata.X as well as various slots that specify sample metadata. This metadata is pretty limited when first loading the output from cellranger, but we will populate it with more useful information (i.e., cluster information) in later steps of the analysis.\nTo access the metadata corresponding to the cells (i.e. cell barcode, batch), you can enter the command adata.obs. To access the metadata corresponding to the genes (i.e, gene names, chromosome, etc), you can enter the command adata.var. These commands return a pandas.dataframe object. This data frame can be manipulated using any functions from the pandas package, including sum(), mean(), groupby(), and value_counts().\n\nadata.X # view the count matrix (rows x columns, cells x genes)\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)\n\n\n\nadata.obs.head() # view a pandas data frame containing metadata on the cells\n\n\n\n\n\n\n\n\nsample\n\n\n\n\nAAACCTGCAGACGCAA-1\nPBMMC_1\n\n\nAAACCTGTCATCACCC-1\nPBMMC_1\n\n\nAAAGATGCATAAAGGT-1\nPBMMC_1\n\n\nAAAGCAAAGCAGCGTA-1\nPBMMC_1\n\n\nAAAGCAACAATAACGA-1\nPBMMC_1\n\n\n\n\n\n\n\n\nadata.var.head() # view a pandas data frame containing metadata on the cells\n\n\n\n\n\n\n\n\n\n\n\n\nRP11-34P13.3\n\n\nFAM138A\n\n\nOR4F5\n\n\nRP11-34P13.7\n\n\nRP11-34P13.8\n\n\n\n\n\n\n\nExercise 2: Use the pandas value_counts() function to determine how many cells were collected for each of the six samples saved into your adata object? Keep in mind, since this is cell-specific metadata, we will want to work with the data frame returned by typing adata.obs.\n\nClick for Answer\n\n\nAnswer: You can run the command adata.obs[“sample”].value_counts() to view the number of cells per sample.\nThe output should be:\n    sample\n    PBMMC_2         3105\n    PBMMC_3         2229\n    PBMMC_1         1612\n    Name: count, dtype: int64\n\n\n\nQuality control\nIn general, quality control (QC) should be done before any downstream analysis is performed. How the data is cleaned will likely have huge effects on downstream results, so it’s imperative to invest the time in choosing QC methods that you think are appropriate for your data! There are some “best practices” but these are by no means strict standards and also have certain limitations: https://www.sc-best-practices.org/preprocessing_visualization/quality_control.html\nGoals: - Filter the data to only include cells that are of high quality. This includes empty droplets, cells with a low total number of UMIs, doublets (two cells that got the same cell barcode), and dying cells (with a high fraction of mitochondrial counts).\nChallenges: - Delineating cells that are poor quality from less complex cell types - Choosing appropriate thresholds for filtering, so as to keep high quality cells without removing biologically relevant cell types or cell states.\nBefore analyzing the scRNA-seq gene expression data, we should ensure that all cellular barcode data corresponds to viable cells. Cell QC is commonly performed based on three QC covariates: - Library size: the number of counts per barcode (count depth) - Detected genes: the number of genes per barcode - Mitochondrial counts: the fraction of counts from mitochondrial genes per barcode.\nLibrary size: First, consider the total number of counts (UMIs) detected per cell. Cells with few counts are likely to have been broken or failed to capture a cell, and should thus be removed. Cells with many counts above the average for a sample are likely to be doublets, or two cells encapsulated in the gel bead during the protocol.\nExercise 3: Using the scanpy and matplotlib packages, visualize a histogram of the distribution of total counts per cell in the dataset. Save this information as metadata to the adata.obs dataframe using a command such as: adata.obs[\"n_counts\"] = n_counts_array. Choose lower and upper boundaries to filter out poor-quality cells and doublets.\nThe histogram function is: plt.hist() https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html\nTip: each function, such as plt.hist() has a set of required arguments. To view those required arguments, as well as an optional arguments, from your jupyter notebook, simply click with your cursor between the () parenthesis of the function, and tab tab+shift on your keyboard.\n\nn_counts_array = adata.X.sum(axis=1) # axis=1 to sum over genes, axis=0 to sum over cells\nadata.obs['n_counts'] = n_counts_array\n\n\nplt.hist(adata.obs['n_counts'], bins=100)\nplt.xlabel(\"Number of UMIs\")\nplt.ylabel(\"Number of cells\")\nplt.axvline(2000, c=\"r\") # choose a lower cutoff for total UMIs\nplt.axvline(12500, c=\"r\") # choose a upper cutoff for total UMIs\nplt.xlim(0, 20000)\nplt.show()\n\n\n\n\n\n\n\n\nExercise 4: Using the scanpy and matplotlib packages, visualize a histogram of the distribution of total genes expressed per cell in the dataset. Save this information as metadata to the adata.obs dataframe using a command such as: adata.obs[\"n_genes\"] = n_genes_array. Choose lower and upper boundaries to filter out low-diversity cells.\n\nexpressed_genes = np.sum(adata.X &gt; 0, 1)\nadata.obs['n_genes'] = expressed_genes\n\nplt.hist(adata.obs['n_genes'], bins=100)\nplt.axvline(500, c=\"r\") # choose a lower cutoff for number of detected genes\nplt.axvline(4000, c=\"r\") # choose a upper cutoff for number of detected genes\nplt.xlabel(\"Number of Genes\")\nplt.ylabel(\"Number of Cells\")\nplt.show()\n\n\n\n\n\n\n\n\nNext, we want to consider filtering cells with high levels of certain classes of genes, namely mitochondrial, ribosomal, and/or hemoglobin genes. There is a different rationale for filtering cells with high levels of these gene classes:\n\nMitochondrial genes: If a cell membrane is damaged, it looses free RNA quicker compared to mitochondrial RNA, because the latter is part of the mitochondrion. A high relative amount of mitochondrial counts can therefore point to damaged cells (Lun et al. 2016).\nRibosomal genes: Are not rRNA (ribosomal RNA) but is mRNA that code for ribosomal proteins. They do not point to specific issues, but it can be good to have a look at their relative abundance. They can have biological relevance (e.g. Caron et al. 2020).\nHemoglobin genes: these transcripts are very abundant in erythrocytes. Depending on your application, you can expect ‘contamination’ of erythrocytes and select against it.\n\nIn order to have an idea about the relative counts of these type of genes in our dataset, we can calculate their expression as relative counts in each cell. We do that by selecting genes based on patterns (e.g. ^MT- matches with all gene names starting with MT, i.e. mitochondrial genes):\n\n# mitochondrial genes\nadata.var[\"mt\"] = adata.var_names.str.startswith(\"MT-\")\n\n# ribosomal genes\nadata.var[\"ribo\"] = adata.var_names.str.startswith((\"RPS\", \"RPL\"))\n\n# hemoglobin genes.\nadata.var[\"hb\"] = adata.var_names.str.contains((\"^HB[^(P)]\"))\n\n\nsc.pp.calculate_qc_metrics(adata, qc_vars=[\"mt\", \"ribo\", \"hb\"], inplace=True, percent_top=[20]) # this step can be a little slow to run\n\nExercise 5: Run the commands and check out the metadata data frame at adata.obs. What has changed?\n\nClick for Answer\n\n\nAnswer: If we type adata.obs, a lot more metadata is present compared to before! This should include the following columns:\nThe output should be:\n    pct_counts_mt\n    total_counts_mt\n    pct_counts_ribo\n    total_counts_ribo\n    pct_counts_hb\n    total_counts_hb\n\n\n\nPlotting experiment metadata\nExercise 6: Using scanpy’s sc.pl.violin function, create a violin plot of the percent of counts corresponding to mitochondrial, ribosomal, and hemoglobin genes per cell. Choose an upper boundary to filter out poor quality cells with high mitochondrial counts. Note that we might want to view the results as a separate violin plot for each of our six samples. To do this, please use the optional groupby=\"sample\" argument.\nPlease note that depending on your experimental setup, it might not make sense to filter on all these criteria.\n\n# Violin plots for all samples together\nsc.pl.violin(adata, [\"pct_counts_mt\", \"pct_counts_ribo\", \"pct_counts_hb\"])\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_anndata.py:839: FutureWarning: \n\nThe `scale` parameter has been renamed and will be removed in v0.15.0. Pass `density_norm='width'` for the same effect.\n  ax = sns.violinplot(\n\n\n\n\n\n\n\n\n\n\n# Violin plots for each sample separately\nsc.pl.violin(adata, [\"pct_counts_mt\", \"pct_counts_ribo\", \"pct_counts_hb\"], groupby=\"sample\")\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_anndata.py:839: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  ax = sns.violinplot(\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_anndata.py:839: FutureWarning: \n\nThe `scale` parameter has been renamed and will be removed in v0.15.0. Pass `density_norm='width'` for the same effect.\n  ax = sns.violinplot(\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_anndata.py:839: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  ax = sns.violinplot(\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_anndata.py:839: FutureWarning: \n\nThe `scale` parameter has been renamed and will be removed in v0.15.0. Pass `density_norm='width'` for the same effect.\n  ax = sns.violinplot(\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_anndata.py:839: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  ax = sns.violinplot(\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_anndata.py:839: FutureWarning: \n\nThe `scale` parameter has been renamed and will be removed in v0.15.0. Pass `density_norm='width'` for the same effect.\n  ax = sns.violinplot(\n\n\n\n\n\n\n\n\n\nYou can see that PBMMC-2 is quite different from the two others, it has a group of cells with very low ribosomal counts and one with very high globin counts. Maybe these two percentages are negatively correlated? Let’s have a look, by plotting the two percentages against each other:\n\nsc.pl.scatter(adata, x=\"pct_counts_hb\", y=\"pct_counts_ribo\", color='sample')\n\n\n\n\n\n\n\n\nExercise 7: Are they correlated? What kind of cells might have a high abundance of hemoglobin transcripts and low ribosomal transcripts?\n\nClick for Answer\n\n\nAnswer: Yes there is a negative correlation. Erythrocytes (red blood cells) have a high abundance of hemoglobin transcripts and low abundance of ribosomal transcripts. These are most likely erythroid cells, i.e. the precursor cells for erythrocytes in the bone marrow.\n\n\nPlotting genes express as a function of total counts\nWe can also evaluate the relative expression of other genes in our dataset, for example, the ones that are most highly expressed. Some very highly expressed genes might point to a technical cause, and we might consider to remove them. Below you will find a simple function to generate a boxplot of relative counts per gene per cell:\n\nsc.pl.highest_expr_genes(adata, n_top=20)\n\n\n\n\n\n\n\n\n\n\nDoublet detection\nThere are several tools for identifying doublets (i.e. two cells that were encapsulated with the same gel bead, obtaining the same cell barcode). Recently a benchmarking study was conducted comparing approaches: https://www.sciencedirect.com/science/article/pii/S2405471220304592\nHere, we suggest you use DoubletDetection: https://doubletdetection.readthedocs.io/en/latest/tutorial.html\nIf you want to compare doublet detection methods, another method is scrublet: https://www.cell.com/cell-systems/pdfExtended/S2405-4712(18)30474-5\nA tutorial with scanpy is described below: https://github.com/swolock/scrublet\n\nimport doubletdetection\n\n\nclf = doubletdetection.BoostClassifier()\n\n\n# raw_counts is a cells by genes count matrix\nlabels = clf.fit(adata.X).predict()\n\n# higher means more likely to be doublet\nscores = clf.doublet_score()\n\n\n\n\nExercise 8: Run the above steps. The variable labels will store the output of the doublet detection: if labels[i]==0, the cell at that position is not a doublet, whereas if labels[i]==1, then the cell at that position is a doublet. How many doublets are predicted? Can you assign this labels metadata to your adata object with adata.obs[\"is_doublet\"] and then use value_counts() to see the number of doublets?\n\nadata.obs[\"is_doublet\"] = labels\n\n\nadata.obs[\"is_doublet\"].value_counts()\n\nis_doublet\n0.0    6879\n1.0      59\nName: count, dtype: int64\n\n\nFinally, let’s filter out the doublet cells!\n\nadata = adata[adata.obs[\"is_doublet\"]==0].copy()\n\n\nadata # is now populated with some more metadata from the QC steps above\n\nAnnData object with n_obs × n_vars = 6879 × 33694\n    obs: 'sample', 'n_counts', 'n_genes', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_20_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'log1p_total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'log1p_total_counts_hb', 'pct_counts_hb', 'is_doublet'\n    var: 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts'\n    uns: 'sample_colors'\n\n\n\n\nCell Filtering\nExercise 9: Once you have selected your QC filtering criteria, you need to actually do the filtering! You can do this either (1) using the QC thresholds you selected above or (2) obtaining automated thresholds using scanpy quality control metrics. Whether you choose the (1) or (2) approach is up to you. Unfortunately, there is not much automation in the quality control steps at this stage, although there are ongoing efforts by the single cell community to create a more unbiased QC approaches. We suggest manually selecting your filtering thresholds using the plots generated above as a guide.\nUse the following scanpy quality control checks to filter out poor quality cells based either (1) your semi-subjective criteria OR (2) the somewhat automated approach offered described here: https://www.sc-best-practices.org/preprocessing_visualization/quality_control.html\nHint: you can also filter an AnnData object using indexing approaches, as with numpy arrays and pandas data frames. For instance, the following command filters genes (columns) on a qc metric for percent mitochondrial counts: adata = adata[adata.obs['pct_counts_mt'] &lt; 0.08, :].copy()\n\nsc.pp.filter_cells(adata, min_counts=??) # apply threshold from above to actually do the filtering\nsc.pp.filter_cells(adata, max_counts=??) # apply threshold from above to actually do the filtering\n\n\nsc.pp.filter_cells(adata, min_genes=??) # apply threshold from above to actually do the filtering\nsc.pp.filter_cells(adata, max_genes=??) # apply threshold from above to actually do the filtering\n\n\nadata = adata[adata.obs['pct_counts_mt'] &lt;= 8, :].copy() # apply threshold from above to actually do the filtering\n# the authors of the original study used 8% as their threshold here.\n\n\nadata\n\nAnnData object with n_obs × n_vars = 5469 × 33694\n    obs: 'sample', 'n_counts', 'n_genes', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_20_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'log1p_total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'log1p_total_counts_hb', 'pct_counts_hb', 'is_doublet'\n    var: 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts'\n    uns: 'sample_colors'\n\n\nExercise 10: We have been discussing cell filtering, but you may also want to filter out genes that are not detected in your data! For this, you can use the function sc.pp.filter_genes. Try doing this to filter out genes expressed in fewer than 1% of your total cells. How many genes are removed (you can check the value of adata.n_vars before and after filtering with sc.pp.filter_genes.\n\nn_cells = adata.n_obs\nsc.pp.filter_genes(adata, min_cells=int(n_cells*0.01)) # specify min cells equal to 1% of your total cell count\n\n\nadata\n\nAnnData object with n_obs × n_vars = 5469 × 10839\n    obs: 'sample', 'n_counts', 'n_genes', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_20_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'log1p_total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'log1p_total_counts_hb', 'pct_counts_hb', 'is_doublet'\n    var: 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts', 'n_cells'\n    uns: 'sample_colors'\n\n\nExercise 11: You have finished this set of exercises! One important final step: in case you want to save your results at any time, you can use the command adata.write_h5ad() to save your AnnData object for later use. Try doing this here, so that you can load the adata object into the next Jupyter notebook tutorial on Normalization and Scaling.\n\nadata.write_h5ad(\"PBMC_analysis_SIB_tutorial.h5ad\") # feel free to choose whichever file name you prefer!"
  },
  {
    "objectID": "ipynb/day1-2_analysis_tools_qc.html#the-anndata-object",
    "href": "ipynb/day1-2_analysis_tools_qc.html#the-anndata-object",
    "title": "Analysis tools and quality control (QC)",
    "section": "The AnnData object",
    "text": "The AnnData object\nThe adata object we have created has the class AnnData. The object contains the single-cell count matrix, accessible with the command adata.X as well as various slots that specify sample metadata. This metadata is pretty limited when first loading the output from cellranger, but we will populate it with more useful information (i.e., cluster information) in later steps of the analysis.\nTo access the metadata corresponding to the cells (i.e. cell barcode, batch), you can enter the command adata.obs. To access the metadata corresponding to the genes (i.e, gene names, chromosome, etc), you can enter the command adata.var. These commands return a pandas.dataframe object. This data frame can be manipulated using any functions from the pandas package, including sum(), mean(), groupby(), and value_counts().\n\nadata.X # view the count matrix (rows x columns, cells x genes)\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)\n\n\n\nadata.obs.head() # view a pandas data frame containing metadata on the cells\n\n\n\n\n\n\n\n\nsample\n\n\n\n\nAAACCTGCAGACGCAA-1\nPBMMC_1\n\n\nAAACCTGTCATCACCC-1\nPBMMC_1\n\n\nAAAGATGCATAAAGGT-1\nPBMMC_1\n\n\nAAAGCAAAGCAGCGTA-1\nPBMMC_1\n\n\nAAAGCAACAATAACGA-1\nPBMMC_1\n\n\n\n\n\n\n\n\nadata.var.head() # view a pandas data frame containing metadata on the cells\n\n\n\n\n\n\n\n\n\n\n\n\nRP11-34P13.3\n\n\nFAM138A\n\n\nOR4F5\n\n\nRP11-34P13.7\n\n\nRP11-34P13.8\n\n\n\n\n\n\n\nExercise 2: Use the pandas value_counts() function to determine how many cells were collected for each of the six samples saved into your adata object? Keep in mind, since this is cell-specific metadata, we will want to work with the data frame returned by typing adata.obs.\n\nClick for Answer\n\n\nAnswer: You can run the command adata.obs[“sample”].value_counts() to view the number of cells per sample.\nThe output should be:\n    sample\n    PBMMC_2         3105\n    PBMMC_3         2229\n    PBMMC_1         1612\n    Name: count, dtype: int64\n\n\n\nQuality control\nIn general, quality control (QC) should be done before any downstream analysis is performed. How the data is cleaned will likely have huge effects on downstream results, so it’s imperative to invest the time in choosing QC methods that you think are appropriate for your data! There are some “best practices” but these are by no means strict standards and also have certain limitations: https://www.sc-best-practices.org/preprocessing_visualization/quality_control.html\nGoals: - Filter the data to only include cells that are of high quality. This includes empty droplets, cells with a low total number of UMIs, doublets (two cells that got the same cell barcode), and dying cells (with a high fraction of mitochondrial counts).\nChallenges: - Delineating cells that are poor quality from less complex cell types - Choosing appropriate thresholds for filtering, so as to keep high quality cells without removing biologically relevant cell types or cell states.\nBefore analyzing the scRNA-seq gene expression data, we should ensure that all cellular barcode data corresponds to viable cells. Cell QC is commonly performed based on three QC covariates: - Library size: the number of counts per barcode (count depth) - Detected genes: the number of genes per barcode - Mitochondrial counts: the fraction of counts from mitochondrial genes per barcode.\nLibrary size: First, consider the total number of counts (UMIs) detected per cell. Cells with few counts are likely to have been broken or failed to capture a cell, and should thus be removed. Cells with many counts above the average for a sample are likely to be doublets, or two cells encapsulated in the gel bead during the protocol.\nExercise 3: Using the scanpy and matplotlib packages, visualize a histogram of the distribution of total counts per cell in the dataset. Save this information as metadata to the adata.obs dataframe using a command such as: adata.obs[\"n_counts\"] = n_counts_array. Choose lower and upper boundaries to filter out poor-quality cells and doublets.\nThe histogram function is: plt.hist() https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html\nTip: each function, such as plt.hist() has a set of required arguments. To view those required arguments, as well as an optional arguments, from your jupyter notebook, simply click with your cursor between the () parenthesis of the function, and tab tab+shift on your keyboard.\n\nn_counts_array = adata.X.sum(axis=1) # axis=1 to sum over genes, axis=0 to sum over cells\nadata.obs['n_counts'] = n_counts_array\n\n\nplt.hist(adata.obs['n_counts'], bins=100)\nplt.xlabel(\"Number of UMIs\")\nplt.ylabel(\"Number of cells\")\nplt.axvline(2000, c=\"r\") # choose a lower cutoff for total UMIs\nplt.axvline(12500, c=\"r\") # choose a upper cutoff for total UMIs\nplt.xlim(0, 20000)\nplt.show()\n\n\n\n\n\n\n\n\nExercise 4: Using the scanpy and matplotlib packages, visualize a histogram of the distribution of total genes expressed per cell in the dataset. Save this information as metadata to the adata.obs dataframe using a command such as: adata.obs[\"n_genes\"] = n_genes_array. Choose lower and upper boundaries to filter out low-diversity cells.\n\nexpressed_genes = np.sum(adata.X &gt; 0, 1)\nadata.obs['n_genes'] = expressed_genes\n\nplt.hist(adata.obs['n_genes'], bins=100)\nplt.axvline(500, c=\"r\") # choose a lower cutoff for number of detected genes\nplt.axvline(4000, c=\"r\") # choose a upper cutoff for number of detected genes\nplt.xlabel(\"Number of Genes\")\nplt.ylabel(\"Number of Cells\")\nplt.show()\n\n\n\n\n\n\n\n\nNext, we want to consider filtering cells with high levels of certain classes of genes, namely mitochondrial, ribosomal, and/or hemoglobin genes. There is a different rationale for filtering cells with high levels of these gene classes:\n\nMitochondrial genes: If a cell membrane is damaged, it looses free RNA quicker compared to mitochondrial RNA, because the latter is part of the mitochondrion. A high relative amount of mitochondrial counts can therefore point to damaged cells (Lun et al. 2016).\nRibosomal genes: Are not rRNA (ribosomal RNA) but is mRNA that code for ribosomal proteins. They do not point to specific issues, but it can be good to have a look at their relative abundance. They can have biological relevance (e.g. Caron et al. 2020).\nHemoglobin genes: these transcripts are very abundant in erythrocytes. Depending on your application, you can expect ‘contamination’ of erythrocytes and select against it.\n\nIn order to have an idea about the relative counts of these type of genes in our dataset, we can calculate their expression as relative counts in each cell. We do that by selecting genes based on patterns (e.g. ^MT- matches with all gene names starting with MT, i.e. mitochondrial genes):\n\n# mitochondrial genes\nadata.var[\"mt\"] = adata.var_names.str.startswith(\"MT-\")\n\n# ribosomal genes\nadata.var[\"ribo\"] = adata.var_names.str.startswith((\"RPS\", \"RPL\"))\n\n# hemoglobin genes.\nadata.var[\"hb\"] = adata.var_names.str.contains((\"^HB[^(P)]\"))\n\n\nsc.pp.calculate_qc_metrics(adata, qc_vars=[\"mt\", \"ribo\", \"hb\"], inplace=True, percent_top=[20]) # this step can be a little slow to run\n\nExercise 5: Run the commands and check out the metadata data frame at adata.obs. What has changed?\n\nClick for Answer\n\n\nAnswer: If we type adata.obs, a lot more metadata is present compared to before! This should include the following columns:\nThe output should be:\n    pct_counts_mt\n    total_counts_mt\n    pct_counts_ribo\n    total_counts_ribo\n    pct_counts_hb\n    total_counts_hb\n\n\n\nPlotting experiment metadata\nExercise 6: Using scanpy’s sc.pl.violin function, create a violin plot of the percent of counts corresponding to mitochondrial, ribosomal, and hemoglobin genes per cell. Choose an upper boundary to filter out poor quality cells with high mitochondrial counts. Note that we might want to view the results as a separate violin plot for each of our six samples. To do this, please use the optional groupby=\"sample\" argument.\nPlease note that depending on your experimental setup, it might not make sense to filter on all these criteria.\n\n# Violin plots for all samples together\nsc.pl.violin(adata, [\"pct_counts_mt\", \"pct_counts_ribo\", \"pct_counts_hb\"])\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_anndata.py:839: FutureWarning: \n\nThe `scale` parameter has been renamed and will be removed in v0.15.0. Pass `density_norm='width'` for the same effect.\n  ax = sns.violinplot(\n\n\n\n\n\n\n\n\n\n\n# Violin plots for each sample separately\nsc.pl.violin(adata, [\"pct_counts_mt\", \"pct_counts_ribo\", \"pct_counts_hb\"], groupby=\"sample\")\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_anndata.py:839: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  ax = sns.violinplot(\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_anndata.py:839: FutureWarning: \n\nThe `scale` parameter has been renamed and will be removed in v0.15.0. Pass `density_norm='width'` for the same effect.\n  ax = sns.violinplot(\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_anndata.py:839: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  ax = sns.violinplot(\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_anndata.py:839: FutureWarning: \n\nThe `scale` parameter has been renamed and will be removed in v0.15.0. Pass `density_norm='width'` for the same effect.\n  ax = sns.violinplot(\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_anndata.py:839: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  ax = sns.violinplot(\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_anndata.py:839: FutureWarning: \n\nThe `scale` parameter has been renamed and will be removed in v0.15.0. Pass `density_norm='width'` for the same effect.\n  ax = sns.violinplot(\n\n\n\n\n\n\n\n\n\nYou can see that PBMMC-2 is quite different from the two others, it has a group of cells with very low ribosomal counts and one with very high globin counts. Maybe these two percentages are negatively correlated? Let’s have a look, by plotting the two percentages against each other:\n\nsc.pl.scatter(adata, x=\"pct_counts_hb\", y=\"pct_counts_ribo\", color='sample')\n\n\n\n\n\n\n\n\nExercise 7: Are they correlated? What kind of cells might have a high abundance of hemoglobin transcripts and low ribosomal transcripts?\n\nClick for Answer\n\n\nAnswer: Yes there is a negative correlation. Erythrocytes (red blood cells) have a high abundance of hemoglobin transcripts and low abundance of ribosomal transcripts. These are most likely erythroid cells, i.e. the precursor cells for erythrocytes in the bone marrow.\n\n\nPlotting genes express as a function of total counts\nWe can also evaluate the relative expression of other genes in our dataset, for example, the ones that are most highly expressed. Some very highly expressed genes might point to a technical cause, and we might consider to remove them. Below you will find a simple function to generate a boxplot of relative counts per gene per cell:\n\nsc.pl.highest_expr_genes(adata, n_top=20)\n\n\n\n\n\n\n\n\n\n\nDoublet detection\nThere are several tools for identifying doublets (i.e. two cells that were encapsulated with the same gel bead, obtaining the same cell barcode). Recently a benchmarking study was conducted comparing approaches: https://www.sciencedirect.com/science/article/pii/S2405471220304592\nHere, we suggest you use DoubletDetection: https://doubletdetection.readthedocs.io/en/latest/tutorial.html\nIf you want to compare doublet detection methods, another method is scrublet: https://www.cell.com/cell-systems/pdfExtended/S2405-4712(18)30474-5\nA tutorial with scanpy is described below: https://github.com/swolock/scrublet\n\nimport doubletdetection\n\n\nclf = doubletdetection.BoostClassifier()\n\n\n# raw_counts is a cells by genes count matrix\nlabels = clf.fit(adata.X).predict()\n\n# higher means more likely to be doublet\nscores = clf.doublet_score()\n\n\n\n\nExercise 8: Run the above steps. The variable labels will store the output of the doublet detection: if labels[i]==0, the cell at that position is not a doublet, whereas if labels[i]==1, then the cell at that position is a doublet. How many doublets are predicted? Can you assign this labels metadata to your adata object with adata.obs[\"is_doublet\"] and then use value_counts() to see the number of doublets?\n\nadata.obs[\"is_doublet\"] = labels\n\n\nadata.obs[\"is_doublet\"].value_counts()\n\nis_doublet\n0.0    6879\n1.0      59\nName: count, dtype: int64\n\n\nFinally, let’s filter out the doublet cells!\n\nadata = adata[adata.obs[\"is_doublet\"]==0].copy()\n\n\nadata # is now populated with some more metadata from the QC steps above\n\nAnnData object with n_obs × n_vars = 6879 × 33694\n    obs: 'sample', 'n_counts', 'n_genes', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_20_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'log1p_total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'log1p_total_counts_hb', 'pct_counts_hb', 'is_doublet'\n    var: 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts'\n    uns: 'sample_colors'\n\n\n\n\nCell Filtering\nExercise 9: Once you have selected your QC filtering criteria, you need to actually do the filtering! You can do this either (1) using the QC thresholds you selected above or (2) obtaining automated thresholds using scanpy quality control metrics. Whether you choose the (1) or (2) approach is up to you. Unfortunately, there is not much automation in the quality control steps at this stage, although there are ongoing efforts by the single cell community to create a more unbiased QC approaches. We suggest manually selecting your filtering thresholds using the plots generated above as a guide.\nUse the following scanpy quality control checks to filter out poor quality cells based either (1) your semi-subjective criteria OR (2) the somewhat automated approach offered described here: https://www.sc-best-practices.org/preprocessing_visualization/quality_control.html\nHint: you can also filter an AnnData object using indexing approaches, as with numpy arrays and pandas data frames. For instance, the following command filters genes (columns) on a qc metric for percent mitochondrial counts: adata = adata[adata.obs['pct_counts_mt'] &lt; 0.08, :].copy()\n\nsc.pp.filter_cells(adata, min_counts=??) # apply threshold from above to actually do the filtering\nsc.pp.filter_cells(adata, max_counts=??) # apply threshold from above to actually do the filtering\n\n\nsc.pp.filter_cells(adata, min_genes=??) # apply threshold from above to actually do the filtering\nsc.pp.filter_cells(adata, max_genes=??) # apply threshold from above to actually do the filtering\n\n\nadata = adata[adata.obs['pct_counts_mt'] &lt;= 8, :].copy() # apply threshold from above to actually do the filtering\n# the authors of the original study used 8% as their threshold here.\n\n\nadata\n\nAnnData object with n_obs × n_vars = 5469 × 33694\n    obs: 'sample', 'n_counts', 'n_genes', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_20_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'log1p_total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'log1p_total_counts_hb', 'pct_counts_hb', 'is_doublet'\n    var: 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts'\n    uns: 'sample_colors'\n\n\nExercise 10: We have been discussing cell filtering, but you may also want to filter out genes that are not detected in your data! For this, you can use the function sc.pp.filter_genes. Try doing this to filter out genes expressed in fewer than 1% of your total cells. How many genes are removed (you can check the value of adata.n_vars before and after filtering with sc.pp.filter_genes.\n\nn_cells = adata.n_obs\nsc.pp.filter_genes(adata, min_cells=int(n_cells*0.01)) # specify min cells equal to 1% of your total cell count\n\n\nadata\n\nAnnData object with n_obs × n_vars = 5469 × 10839\n    obs: 'sample', 'n_counts', 'n_genes', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_20_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'log1p_total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'log1p_total_counts_hb', 'pct_counts_hb', 'is_doublet'\n    var: 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts', 'n_cells'\n    uns: 'sample_colors'\n\n\nExercise 11: You have finished this set of exercises! One important final step: in case you want to save your results at any time, you can use the command adata.write_h5ad() to save your AnnData object for later use. Try doing this here, so that you can load the adata object into the next Jupyter notebook tutorial on Normalization and Scaling.\n\nadata.write_h5ad(\"PBMC_analysis_SIB_tutorial.h5ad\") # feel free to choose whichever file name you prefer!"
  },
  {
    "objectID": "ipynb/day1-2_analysis_tools_qc.html#quality-control",
    "href": "ipynb/day1-2_analysis_tools_qc.html#quality-control",
    "title": "Analysis tools and quality control (QC)",
    "section": "Quality control",
    "text": "Quality control\nIn general, quality control (QC) should be done before any downstream analysis is performed. How the data is cleaned will likely have huge effects on downstream results, so it’s imperative to invest the time in choosing QC methods that you think are appropriate for your data! There are some “best practices” but these are by no means strict standards and also have certain limitations: https://www.sc-best-practices.org/preprocessing_visualization/quality_control.html\nGoals: - Filter the data to only include cells that are of high quality. This includes empty droplets, cells with a low total number of UMIs, doublets (two cells that got the same cell barcode), and dying cells (with a high fraction of mitochondrial counts).\nChallenges: - Delineating cells that are poor quality from less complex cell types - Choosing appropriate thresholds for filtering, so as to keep high quality cells without removing biologically relevant cell types or cell states.\nBefore analyzing the scRNA-seq gene expression data, we should ensure that all cellular barcode data corresponds to viable cells. Cell QC is commonly performed based on three QC covariates: - Library size: the number of counts per barcode (count depth) - Detected genes: the number of genes per barcode - Mitochondrial counts: the fraction of counts from mitochondrial genes per barcode.\nLibrary size: First, consider the total number of counts (UMIs) detected per cell. Cells with few counts are likely to have been broken or failed to capture a cell, and should thus be removed. Cells with many counts above the average for a sample are likely to be doublets, or two cells encapsulated in the gel bead during the protocol.\nExercise 3: Using the scanpy and matplotlib packages, visualize a histogram of the distribution of total counts per cell in the dataset. Save this information as metadata to the adata.obs dataframe using a command such as: adata.obs[\"n_counts\"] = n_counts_array. Choose lower and upper boundaries to filter out poor-quality cells and doublets.\nThe histogram function is: plt.hist() https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html\nTip: each function, such as plt.hist() has a set of required arguments. To view those required arguments, as well as an optional arguments, from your jupyter notebook, simply click with your cursor between the () parenthesis of the function, and tab tab+shift on your keyboard.\n\nn_counts_array = adata.X.sum(axis=1) # axis=1 to sum over genes, axis=0 to sum over cells\nadata.obs['n_counts'] = n_counts_array\n\n\nplt.hist(adata.obs['n_counts'], bins=100)\nplt.xlabel(\"Number of UMIs\")\nplt.ylabel(\"Number of cells\")\nplt.axvline(2000, c=\"r\") # choose a lower cutoff for total UMIs\nplt.axvline(12500, c=\"r\") # choose a upper cutoff for total UMIs\nplt.xlim(0, 20000)\nplt.show()\n\n\n\n\n\n\n\n\nExercise 4: Using the scanpy and matplotlib packages, visualize a histogram of the distribution of total genes expressed per cell in the dataset. Save this information as metadata to the adata.obs dataframe using a command such as: adata.obs[\"n_genes\"] = n_genes_array. Choose lower and upper boundaries to filter out low-diversity cells.\n\nexpressed_genes = np.sum(adata.X &gt; 0, 1)\nadata.obs['n_genes'] = expressed_genes\n\nplt.hist(adata.obs['n_genes'], bins=100)\nplt.axvline(500, c=\"r\") # choose a lower cutoff for number of detected genes\nplt.axvline(4000, c=\"r\") # choose a upper cutoff for number of detected genes\nplt.xlabel(\"Number of Genes\")\nplt.ylabel(\"Number of Cells\")\nplt.show()\n\n\n\n\n\n\n\n\nNext, we want to consider filtering cells with high levels of certain classes of genes, namely mitochondrial, ribosomal, and/or hemoglobin genes. There is a different rationale for filtering cells with high levels of these gene classes:\n\nMitochondrial genes: If a cell membrane is damaged, it looses free RNA quicker compared to mitochondrial RNA, because the latter is part of the mitochondrion. A high relative amount of mitochondrial counts can therefore point to damaged cells (Lun et al. 2016).\nRibosomal genes: Are not rRNA (ribosomal RNA) but is mRNA that code for ribosomal proteins. They do not point to specific issues, but it can be good to have a look at their relative abundance. They can have biological relevance (e.g. Caron et al. 2020).\nHemoglobin genes: these transcripts are very abundant in erythrocytes. Depending on your application, you can expect ‘contamination’ of erythrocytes and select against it.\n\nIn order to have an idea about the relative counts of these type of genes in our dataset, we can calculate their expression as relative counts in each cell. We do that by selecting genes based on patterns (e.g. ^MT- matches with all gene names starting with MT, i.e. mitochondrial genes):\n\n# mitochondrial genes\nadata.var[\"mt\"] = adata.var_names.str.startswith(\"MT-\")\n\n# ribosomal genes\nadata.var[\"ribo\"] = adata.var_names.str.startswith((\"RPS\", \"RPL\"))\n\n# hemoglobin genes.\nadata.var[\"hb\"] = adata.var_names.str.contains((\"^HB[^(P)]\"))\n\n\nsc.pp.calculate_qc_metrics(adata, qc_vars=[\"mt\", \"ribo\", \"hb\"], inplace=True, percent_top=[20]) # this step can be a little slow to run\n\nExercise 5: Run the commands and check out the metadata data frame at adata.obs. What has changed?\n\nClick for Answer\n\n\nAnswer: If we type adata.obs, a lot more metadata is present compared to before! This should include the following columns:\nThe output should be:\n    pct_counts_mt\n    total_counts_mt\n    pct_counts_ribo\n    total_counts_ribo\n    pct_counts_hb\n    total_counts_hb\n\n\n\nPlotting experiment metadata\nExercise 6: Using scanpy’s sc.pl.violin function, create a violin plot of the percent of counts corresponding to mitochondrial, ribosomal, and hemoglobin genes per cell. Choose an upper boundary to filter out poor quality cells with high mitochondrial counts. Note that we might want to view the results as a separate violin plot for each of our six samples. To do this, please use the optional groupby=\"sample\" argument.\nPlease note that depending on your experimental setup, it might not make sense to filter on all these criteria.\n\n# Violin plots for all samples together\nsc.pl.violin(adata, [\"pct_counts_mt\", \"pct_counts_ribo\", \"pct_counts_hb\"])\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_anndata.py:839: FutureWarning: \n\nThe `scale` parameter has been renamed and will be removed in v0.15.0. Pass `density_norm='width'` for the same effect.\n  ax = sns.violinplot(\n\n\n\n\n\n\n\n\n\n\n# Violin plots for each sample separately\nsc.pl.violin(adata, [\"pct_counts_mt\", \"pct_counts_ribo\", \"pct_counts_hb\"], groupby=\"sample\")\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_anndata.py:839: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  ax = sns.violinplot(\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_anndata.py:839: FutureWarning: \n\nThe `scale` parameter has been renamed and will be removed in v0.15.0. Pass `density_norm='width'` for the same effect.\n  ax = sns.violinplot(\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_anndata.py:839: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  ax = sns.violinplot(\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_anndata.py:839: FutureWarning: \n\nThe `scale` parameter has been renamed and will be removed in v0.15.0. Pass `density_norm='width'` for the same effect.\n  ax = sns.violinplot(\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_anndata.py:839: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  ax = sns.violinplot(\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_anndata.py:839: FutureWarning: \n\nThe `scale` parameter has been renamed and will be removed in v0.15.0. Pass `density_norm='width'` for the same effect.\n  ax = sns.violinplot(\n\n\n\n\n\n\n\n\n\nYou can see that PBMMC-2 is quite different from the two others, it has a group of cells with very low ribosomal counts and one with very high globin counts. Maybe these two percentages are negatively correlated? Let’s have a look, by plotting the two percentages against each other:\n\nsc.pl.scatter(adata, x=\"pct_counts_hb\", y=\"pct_counts_ribo\", color='sample')\n\n\n\n\n\n\n\n\nExercise 7: Are they correlated? What kind of cells might have a high abundance of hemoglobin transcripts and low ribosomal transcripts?\n\nClick for Answer\n\n\nAnswer: Yes there is a negative correlation. Erythrocytes (red blood cells) have a high abundance of hemoglobin transcripts and low abundance of ribosomal transcripts. These are most likely erythroid cells, i.e. the precursor cells for erythrocytes in the bone marrow.\n\n\nPlotting genes express as a function of total counts\nWe can also evaluate the relative expression of other genes in our dataset, for example, the ones that are most highly expressed. Some very highly expressed genes might point to a technical cause, and we might consider to remove them. Below you will find a simple function to generate a boxplot of relative counts per gene per cell:\n\nsc.pl.highest_expr_genes(adata, n_top=20)\n\n\n\n\n\n\n\n\n\n\nDoublet detection\nThere are several tools for identifying doublets (i.e. two cells that were encapsulated with the same gel bead, obtaining the same cell barcode). Recently a benchmarking study was conducted comparing approaches: https://www.sciencedirect.com/science/article/pii/S2405471220304592\nHere, we suggest you use DoubletDetection: https://doubletdetection.readthedocs.io/en/latest/tutorial.html\nIf you want to compare doublet detection methods, another method is scrublet: https://www.cell.com/cell-systems/pdfExtended/S2405-4712(18)30474-5\nA tutorial with scanpy is described below: https://github.com/swolock/scrublet\n\nimport doubletdetection\n\n\nclf = doubletdetection.BoostClassifier()\n\n\n# raw_counts is a cells by genes count matrix\nlabels = clf.fit(adata.X).predict()\n\n# higher means more likely to be doublet\nscores = clf.doublet_score()\n\n\n\n\nExercise 8: Run the above steps. The variable labels will store the output of the doublet detection: if labels[i]==0, the cell at that position is not a doublet, whereas if labels[i]==1, then the cell at that position is a doublet. How many doublets are predicted? Can you assign this labels metadata to your adata object with adata.obs[\"is_doublet\"] and then use value_counts() to see the number of doublets?\n\nadata.obs[\"is_doublet\"] = labels\n\n\nadata.obs[\"is_doublet\"].value_counts()\n\nis_doublet\n0.0    6879\n1.0      59\nName: count, dtype: int64\n\n\nFinally, let’s filter out the doublet cells!\n\nadata = adata[adata.obs[\"is_doublet\"]==0].copy()\n\n\nadata # is now populated with some more metadata from the QC steps above\n\nAnnData object with n_obs × n_vars = 6879 × 33694\n    obs: 'sample', 'n_counts', 'n_genes', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_20_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'log1p_total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'log1p_total_counts_hb', 'pct_counts_hb', 'is_doublet'\n    var: 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts'\n    uns: 'sample_colors'\n\n\n\n\nCell Filtering\nExercise 9: Once you have selected your QC filtering criteria, you need to actually do the filtering! You can do this either (1) using the QC thresholds you selected above or (2) obtaining automated thresholds using scanpy quality control metrics. Whether you choose the (1) or (2) approach is up to you. Unfortunately, there is not much automation in the quality control steps at this stage, although there are ongoing efforts by the single cell community to create a more unbiased QC approaches. We suggest manually selecting your filtering thresholds using the plots generated above as a guide.\nUse the following scanpy quality control checks to filter out poor quality cells based either (1) your semi-subjective criteria OR (2) the somewhat automated approach offered described here: https://www.sc-best-practices.org/preprocessing_visualization/quality_control.html\nHint: you can also filter an AnnData object using indexing approaches, as with numpy arrays and pandas data frames. For instance, the following command filters genes (columns) on a qc metric for percent mitochondrial counts: adata = adata[adata.obs['pct_counts_mt'] &lt; 0.08, :].copy()\n\nsc.pp.filter_cells(adata, min_counts=??) # apply threshold from above to actually do the filtering\nsc.pp.filter_cells(adata, max_counts=??) # apply threshold from above to actually do the filtering\n\n\nsc.pp.filter_cells(adata, min_genes=??) # apply threshold from above to actually do the filtering\nsc.pp.filter_cells(adata, max_genes=??) # apply threshold from above to actually do the filtering\n\n\nadata = adata[adata.obs['pct_counts_mt'] &lt;= 8, :].copy() # apply threshold from above to actually do the filtering\n# the authors of the original study used 8% as their threshold here.\n\n\nadata\n\nAnnData object with n_obs × n_vars = 5469 × 33694\n    obs: 'sample', 'n_counts', 'n_genes', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_20_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'log1p_total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'log1p_total_counts_hb', 'pct_counts_hb', 'is_doublet'\n    var: 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts'\n    uns: 'sample_colors'\n\n\nExercise 10: We have been discussing cell filtering, but you may also want to filter out genes that are not detected in your data! For this, you can use the function sc.pp.filter_genes. Try doing this to filter out genes expressed in fewer than 1% of your total cells. How many genes are removed (you can check the value of adata.n_vars before and after filtering with sc.pp.filter_genes.\n\nn_cells = adata.n_obs\nsc.pp.filter_genes(adata, min_cells=int(n_cells*0.01)) # specify min cells equal to 1% of your total cell count\n\n\nadata\n\nAnnData object with n_obs × n_vars = 5469 × 10839\n    obs: 'sample', 'n_counts', 'n_genes', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_20_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'log1p_total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'log1p_total_counts_hb', 'pct_counts_hb', 'is_doublet'\n    var: 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts', 'n_cells'\n    uns: 'sample_colors'\n\n\nExercise 11: You have finished this set of exercises! One important final step: in case you want to save your results at any time, you can use the command adata.write_h5ad() to save your AnnData object for later use. Try doing this here, so that you can load the adata object into the next Jupyter notebook tutorial on Normalization and Scaling.\n\nadata.write_h5ad(\"PBMC_analysis_SIB_tutorial.h5ad\") # feel free to choose whichever file name you prefer!"
  },
  {
    "objectID": "ipynb/day1-2_analysis_tools_qc.html#plotting-experiment-metadata",
    "href": "ipynb/day1-2_analysis_tools_qc.html#plotting-experiment-metadata",
    "title": "Analysis tools and quality control (QC)",
    "section": "Plotting experiment metadata",
    "text": "Plotting experiment metadata\nExercise 6: Using scanpy’s sc.pl.violin function, create a violin plot of the percent of counts corresponding to mitochondrial, ribosomal, and hemoglobin genes per cell. Choose an upper boundary to filter out poor quality cells with high mitochondrial counts. Note that we might want to view the results as a separate violin plot for each of our six samples. To do this, please use the optional groupby=\"sample\" argument.\nPlease note that depending on your experimental setup, it might not make sense to filter on all these criteria.\n\n# Violin plots for all samples together\nsc.pl.violin(adata, [\"pct_counts_mt\", \"pct_counts_ribo\", \"pct_counts_hb\"])\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_anndata.py:839: FutureWarning: \n\nThe `scale` parameter has been renamed and will be removed in v0.15.0. Pass `density_norm='width'` for the same effect.\n  ax = sns.violinplot(\n\n\n\n\n\n\n\n\n\n\n# Violin plots for each sample separately\nsc.pl.violin(adata, [\"pct_counts_mt\", \"pct_counts_ribo\", \"pct_counts_hb\"], groupby=\"sample\")\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_anndata.py:839: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  ax = sns.violinplot(\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_anndata.py:839: FutureWarning: \n\nThe `scale` parameter has been renamed and will be removed in v0.15.0. Pass `density_norm='width'` for the same effect.\n  ax = sns.violinplot(\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_anndata.py:839: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  ax = sns.violinplot(\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_anndata.py:839: FutureWarning: \n\nThe `scale` parameter has been renamed and will be removed in v0.15.0. Pass `density_norm='width'` for the same effect.\n  ax = sns.violinplot(\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_anndata.py:839: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  ax = sns.violinplot(\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_anndata.py:839: FutureWarning: \n\nThe `scale` parameter has been renamed and will be removed in v0.15.0. Pass `density_norm='width'` for the same effect.\n  ax = sns.violinplot(\n\n\n\n\n\n\n\n\n\nYou can see that PBMMC-2 is quite different from the two others, it has a group of cells with very low ribosomal counts and one with very high globin counts. Maybe these two percentages are negatively correlated? Let’s have a look, by plotting the two percentages against each other:\n\nsc.pl.scatter(adata, x=\"pct_counts_hb\", y=\"pct_counts_ribo\", color='sample')\n\n\n\n\n\n\n\n\nExercise 7: Are they correlated? What kind of cells might have a high abundance of hemoglobin transcripts and low ribosomal transcripts?\n\nClick for Answer\n\n\nAnswer: Yes there is a negative correlation. Erythrocytes (red blood cells) have a high abundance of hemoglobin transcripts and low abundance of ribosomal transcripts. These are most likely erythroid cells, i.e. the precursor cells for erythrocytes in the bone marrow.\n\n\nPlotting genes express as a function of total counts\nWe can also evaluate the relative expression of other genes in our dataset, for example, the ones that are most highly expressed. Some very highly expressed genes might point to a technical cause, and we might consider to remove them. Below you will find a simple function to generate a boxplot of relative counts per gene per cell:\n\nsc.pl.highest_expr_genes(adata, n_top=20)\n\n\n\n\n\n\n\n\n\n\nDoublet detection\nThere are several tools for identifying doublets (i.e. two cells that were encapsulated with the same gel bead, obtaining the same cell barcode). Recently a benchmarking study was conducted comparing approaches: https://www.sciencedirect.com/science/article/pii/S2405471220304592\nHere, we suggest you use DoubletDetection: https://doubletdetection.readthedocs.io/en/latest/tutorial.html\nIf you want to compare doublet detection methods, another method is scrublet: https://www.cell.com/cell-systems/pdfExtended/S2405-4712(18)30474-5\nA tutorial with scanpy is described below: https://github.com/swolock/scrublet\n\nimport doubletdetection\n\n\nclf = doubletdetection.BoostClassifier()\n\n\n# raw_counts is a cells by genes count matrix\nlabels = clf.fit(adata.X).predict()\n\n# higher means more likely to be doublet\nscores = clf.doublet_score()\n\n\n\n\nExercise 8: Run the above steps. The variable labels will store the output of the doublet detection: if labels[i]==0, the cell at that position is not a doublet, whereas if labels[i]==1, then the cell at that position is a doublet. How many doublets are predicted? Can you assign this labels metadata to your adata object with adata.obs[\"is_doublet\"] and then use value_counts() to see the number of doublets?\n\nadata.obs[\"is_doublet\"] = labels\n\n\nadata.obs[\"is_doublet\"].value_counts()\n\nis_doublet\n0.0    6879\n1.0      59\nName: count, dtype: int64\n\n\nFinally, let’s filter out the doublet cells!\n\nadata = adata[adata.obs[\"is_doublet\"]==0].copy()\n\n\nadata # is now populated with some more metadata from the QC steps above\n\nAnnData object with n_obs × n_vars = 6879 × 33694\n    obs: 'sample', 'n_counts', 'n_genes', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_20_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'log1p_total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'log1p_total_counts_hb', 'pct_counts_hb', 'is_doublet'\n    var: 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts'\n    uns: 'sample_colors'\n\n\n\n\nCell Filtering\nExercise 9: Once you have selected your QC filtering criteria, you need to actually do the filtering! You can do this either (1) using the QC thresholds you selected above or (2) obtaining automated thresholds using scanpy quality control metrics. Whether you choose the (1) or (2) approach is up to you. Unfortunately, there is not much automation in the quality control steps at this stage, although there are ongoing efforts by the single cell community to create a more unbiased QC approaches. We suggest manually selecting your filtering thresholds using the plots generated above as a guide.\nUse the following scanpy quality control checks to filter out poor quality cells based either (1) your semi-subjective criteria OR (2) the somewhat automated approach offered described here: https://www.sc-best-practices.org/preprocessing_visualization/quality_control.html\nHint: you can also filter an AnnData object using indexing approaches, as with numpy arrays and pandas data frames. For instance, the following command filters genes (columns) on a qc metric for percent mitochondrial counts: adata = adata[adata.obs['pct_counts_mt'] &lt; 0.08, :].copy()\n\nsc.pp.filter_cells(adata, min_counts=??) # apply threshold from above to actually do the filtering\nsc.pp.filter_cells(adata, max_counts=??) # apply threshold from above to actually do the filtering\n\n\nsc.pp.filter_cells(adata, min_genes=??) # apply threshold from above to actually do the filtering\nsc.pp.filter_cells(adata, max_genes=??) # apply threshold from above to actually do the filtering\n\n\nadata = adata[adata.obs['pct_counts_mt'] &lt;= 8, :].copy() # apply threshold from above to actually do the filtering\n# the authors of the original study used 8% as their threshold here.\n\n\nadata\n\nAnnData object with n_obs × n_vars = 5469 × 33694\n    obs: 'sample', 'n_counts', 'n_genes', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_20_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'log1p_total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'log1p_total_counts_hb', 'pct_counts_hb', 'is_doublet'\n    var: 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts'\n    uns: 'sample_colors'\n\n\nExercise 10: We have been discussing cell filtering, but you may also want to filter out genes that are not detected in your data! For this, you can use the function sc.pp.filter_genes. Try doing this to filter out genes expressed in fewer than 1% of your total cells. How many genes are removed (you can check the value of adata.n_vars before and after filtering with sc.pp.filter_genes.\n\nn_cells = adata.n_obs\nsc.pp.filter_genes(adata, min_cells=int(n_cells*0.01)) # specify min cells equal to 1% of your total cell count\n\n\nadata\n\nAnnData object with n_obs × n_vars = 5469 × 10839\n    obs: 'sample', 'n_counts', 'n_genes', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_20_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'log1p_total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'log1p_total_counts_hb', 'pct_counts_hb', 'is_doublet'\n    var: 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts', 'n_cells'\n    uns: 'sample_colors'\n\n\nExercise 11: You have finished this set of exercises! One important final step: in case you want to save your results at any time, you can use the command adata.write_h5ad() to save your AnnData object for later use. Try doing this here, so that you can load the adata object into the next Jupyter notebook tutorial on Normalization and Scaling.\n\nadata.write_h5ad(\"PBMC_analysis_SIB_tutorial.h5ad\") # feel free to choose whichever file name you prefer!"
  },
  {
    "objectID": "ipynb/day1-2_analysis_tools_qc.html#plotting-genes-express-as-a-function-of-total-counts",
    "href": "ipynb/day1-2_analysis_tools_qc.html#plotting-genes-express-as-a-function-of-total-counts",
    "title": "Analysis tools and quality control (QC)",
    "section": "Plotting genes express as a function of total counts",
    "text": "Plotting genes express as a function of total counts\nWe can also evaluate the relative expression of other genes in our dataset, for example, the ones that are most highly expressed. Some very highly expressed genes might point to a technical cause, and we might consider to remove them. Below you will find a simple function to generate a boxplot of relative counts per gene per cell:\n\nsc.pl.highest_expr_genes(adata, n_top=20)"
  },
  {
    "objectID": "ipynb/day1-2_analysis_tools_qc.html#doublet-detection",
    "href": "ipynb/day1-2_analysis_tools_qc.html#doublet-detection",
    "title": "Analysis tools and quality control (QC)",
    "section": "Doublet detection",
    "text": "Doublet detection\nThere are several tools for identifying doublets (i.e. two cells that were encapsulated with the same gel bead, obtaining the same cell barcode). Recently a benchmarking study was conducted comparing approaches: https://www.sciencedirect.com/science/article/pii/S2405471220304592\nHere, we suggest you use DoubletDetection: https://doubletdetection.readthedocs.io/en/latest/tutorial.html\nIf you want to compare doublet detection methods, another method is scrublet: https://www.cell.com/cell-systems/pdfExtended/S2405-4712(18)30474-5\nA tutorial with scanpy is described below: https://github.com/swolock/scrublet\n\nimport doubletdetection\n\n\nclf = doubletdetection.BoostClassifier()\n\n\n# raw_counts is a cells by genes count matrix\nlabels = clf.fit(adata.X).predict()\n\n# higher means more likely to be doublet\nscores = clf.doublet_score()\n\n\n\n\nExercise 8: Run the above steps. The variable labels will store the output of the doublet detection: if labels[i]==0, the cell at that position is not a doublet, whereas if labels[i]==1, then the cell at that position is a doublet. How many doublets are predicted? Can you assign this labels metadata to your adata object with adata.obs[\"is_doublet\"] and then use value_counts() to see the number of doublets?\n\nadata.obs[\"is_doublet\"] = labels\n\n\nadata.obs[\"is_doublet\"].value_counts()\n\nis_doublet\n0.0    6879\n1.0      59\nName: count, dtype: int64\n\n\nFinally, let’s filter out the doublet cells!\n\nadata = adata[adata.obs[\"is_doublet\"]==0].copy()\n\n\nadata # is now populated with some more metadata from the QC steps above\n\nAnnData object with n_obs × n_vars = 6879 × 33694\n    obs: 'sample', 'n_counts', 'n_genes', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_20_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'log1p_total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'log1p_total_counts_hb', 'pct_counts_hb', 'is_doublet'\n    var: 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts'\n    uns: 'sample_colors'"
  },
  {
    "objectID": "ipynb/day1-2_analysis_tools_qc.html#cell-filtering",
    "href": "ipynb/day1-2_analysis_tools_qc.html#cell-filtering",
    "title": "Analysis tools and quality control (QC)",
    "section": "Cell Filtering",
    "text": "Cell Filtering\nExercise 9: Once you have selected your QC filtering criteria, you need to actually do the filtering! You can do this either (1) using the QC thresholds you selected above or (2) obtaining automated thresholds using scanpy quality control metrics. Whether you choose the (1) or (2) approach is up to you. Unfortunately, there is not much automation in the quality control steps at this stage, although there are ongoing efforts by the single cell community to create a more unbiased QC approaches. We suggest manually selecting your filtering thresholds using the plots generated above as a guide.\nUse the following scanpy quality control checks to filter out poor quality cells based either (1) your semi-subjective criteria OR (2) the somewhat automated approach offered described here: https://www.sc-best-practices.org/preprocessing_visualization/quality_control.html\nHint: you can also filter an AnnData object using indexing approaches, as with numpy arrays and pandas data frames. For instance, the following command filters genes (columns) on a qc metric for percent mitochondrial counts: adata = adata[adata.obs['pct_counts_mt'] &lt; 0.08, :].copy()\n\nsc.pp.filter_cells(adata, min_counts=??) # apply threshold from above to actually do the filtering\nsc.pp.filter_cells(adata, max_counts=??) # apply threshold from above to actually do the filtering\n\n\nsc.pp.filter_cells(adata, min_genes=??) # apply threshold from above to actually do the filtering\nsc.pp.filter_cells(adata, max_genes=??) # apply threshold from above to actually do the filtering\n\n\nadata = adata[adata.obs['pct_counts_mt'] &lt;= 8, :].copy() # apply threshold from above to actually do the filtering\n# the authors of the original study used 8% as their threshold here.\n\n\nadata\n\nAnnData object with n_obs × n_vars = 5469 × 33694\n    obs: 'sample', 'n_counts', 'n_genes', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_20_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'log1p_total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'log1p_total_counts_hb', 'pct_counts_hb', 'is_doublet'\n    var: 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts'\n    uns: 'sample_colors'\n\n\nExercise 10: We have been discussing cell filtering, but you may also want to filter out genes that are not detected in your data! For this, you can use the function sc.pp.filter_genes. Try doing this to filter out genes expressed in fewer than 1% of your total cells. How many genes are removed (you can check the value of adata.n_vars before and after filtering with sc.pp.filter_genes.\n\nn_cells = adata.n_obs\nsc.pp.filter_genes(adata, min_cells=int(n_cells*0.01)) # specify min cells equal to 1% of your total cell count\n\n\nadata\n\nAnnData object with n_obs × n_vars = 5469 × 10839\n    obs: 'sample', 'n_counts', 'n_genes', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_20_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'log1p_total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'log1p_total_counts_hb', 'pct_counts_hb', 'is_doublet'\n    var: 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts', 'n_cells'\n    uns: 'sample_colors'\n\n\nExercise 11: You have finished this set of exercises! One important final step: in case you want to save your results at any time, you can use the command adata.write_h5ad() to save your AnnData object for later use. Try doing this here, so that you can load the adata object into the next Jupyter notebook tutorial on Normalization and Scaling.\n\nadata.write_h5ad(\"PBMC_analysis_SIB_tutorial.h5ad\") # feel free to choose whichever file name you prefer!"
  },
  {
    "objectID": "ipynb/day2-1_dimensionality_reduction.html",
    "href": "ipynb/day2-1_dimensionality_reduction.html",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "After having completed this chapter you will be able to:\n\nUnderstand the differences between PCA and UMAP\nPerform PCA and select a reasonable number of components for downstream UMAP embeddings\nVisualize the contribution of different genes to each principal component\n\n\nimport scanpy as sc\nimport pandas as pd # for handling data frames (i.e. data tables)\nimport numpy as np # for handling numbers, arrays, and matrices\nimport matplotlib.pyplot as plt # plotting package\nimport seaborn as sns # plotting package\n\nExercise 0: Before we continue in this notebook with the next steps of the analysis, we need to load our results from the previous notebook using the sc.read_h5ad function and assign them to the variable name adata. Give it a try!\n\nClick for Answer\n\n\nAnswer: Load your data from the previous notebook into an h5ad object with sc.read_h5ad\n    adata = sc.read_h5ad(\"PBMC_analysis_SIB_tutorial2.h5ad\")"
  },
  {
    "objectID": "ipynb/day2-1_dimensionality_reduction.html#learning-outcomes",
    "href": "ipynb/day2-1_dimensionality_reduction.html#learning-outcomes",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "After having completed this chapter you will be able to:\n\nUnderstand the differences between PCA and UMAP\nPerform PCA and select a reasonable number of components for downstream UMAP embeddings\nVisualize the contribution of different genes to each principal component\n\n\nimport scanpy as sc\nimport pandas as pd # for handling data frames (i.e. data tables)\nimport numpy as np # for handling numbers, arrays, and matrices\nimport matplotlib.pyplot as plt # plotting package\nimport seaborn as sns # plotting package\n\nExercise 0: Before we continue in this notebook with the next steps of the analysis, we need to load our results from the previous notebook using the sc.read_h5ad function and assign them to the variable name adata. Give it a try!\n\nClick for Answer\n\n\nAnswer: Load your data from the previous notebook into an h5ad object with sc.read_h5ad\n    adata = sc.read_h5ad(\"PBMC_analysis_SIB_tutorial2.h5ad\")"
  },
  {
    "objectID": "ipynb/day2-1_dimensionality_reduction.html#principal-component-analysis",
    "href": "ipynb/day2-1_dimensionality_reduction.html#principal-component-analysis",
    "title": "Dimensionality Reduction",
    "section": "Principal component analysis",
    "text": "Principal component analysis\nDimensionality reduction methods seek to take a large set of variables and return a smaller set of components that still contain most of the information in the original dataset. One of the simplest forms of dimensionality reduction is PCA. Principal component analysis (PCA) is a mathematical procedure that transforms a number of possibly correlated (e.g., expression of genes in a network) variables into a (smaller) number of uncorrelated variables called principal components (“PCs”).\nExercise 1: Run PCA analysis using sc.pp.pca in scanpy using n_comps=50. Plot a scatter plot of the first two principal components with sc.pl.pca, which should capture the largest axes of variability in the data.\n\nClick for Answer\n\n\nAnswer:\n    sc.pp.pca(adata, svd_solver=\"arpack\", n_comps=50)\n    sc.pl.pca(adata, color=\"sample\")\n\n\n\nExercise 2: We can color the PCA plot according to any factor that is present in adata.obs, or for any gene’s expression. Can you color by the column n_counts and phase? What about by the genes HBA1 (an alpha subunit of hemoglobin) IGKC (one of the most highly variable genes)?\n\nClick for Answer\n\n\nAnswer:\n    # visualize the first PCs, color by cell cycle phase and n_counts (unnormalized)\n    sc.pl.pca_scatter(adata, color=[\"n_counts\", \"phase\"])\n    sc.pl.pca_scatter(adata, color=[\"HBA1\", \"IGKC\"])\n\n\n\nEach principal component scores the contribution of each gene to that component. Therefore, we can see which genes are more highly correlated to one component compared to the others. You can see that there are certain biases in the data due to the cell cycle phase and n_counts.\n\nsc.pl.pca_loadings(adata, components = '1,2,3')\n\n\n\n\n\n\n\n\nSince PCA is a geometric form of dimensionality reduction, we can visualize the specific genes that most strongly contribute to each principal component. This allows us to get an idea of which components discriminate which cells from each other.\n\ndef pca_heatmap(adata, component, groupby, use_raw=False, layer=None):\n    attr = 'varm'\n    keys = 'PCs'\n    scores = getattr(adata, attr)[keys][:, component]\n    dd = pd.DataFrame(scores, index=adata.var_names)\n    var_names_pos = dd.sort_values(0, ascending=False).index[:20]\n\n    var_names_neg = dd.sort_values(0, ascending=True).index[:20]\n\n    pd2 = pd.DataFrame(adata.obsm['X_pca'][:, component], index=adata.obs.index)\n\n    bottom_cells = pd2.sort_values(0).index[:300].tolist()\n    top_cells = pd2.sort_values(0, ascending=False).index[:300].tolist()\n\n    sc.pl.heatmap(adata[top_cells+bottom_cells], list(var_names_pos) + list(var_names_neg), \n                        show_gene_labels=True, groupby=groupby,\n                        swap_axes=True, cmap='viridis', \n                        use_raw=use_raw, layer=layer, figsize=(6,4))\n\nExercise 3: Use the pca_heatmap function defined above to plot the PC1 and PC2 components. Group the top/bottom cells by their sample identifier?\n\npca_heatmap(adata, component=0, groupby=\"sample\")\n\n\n\n\n\n\n\n\n\npca_heatmap(adata, component=1, groupby=\"sample\")\n\n\n\n\n\n\n\n\nExercise 4: Do you see anything strange about these results? Is there something undesirable about the way in which the PCs discriminate cell populations that might not be biological?\n\nClick for Answer\n\n\nAnswer: Yes! The first principal component (PC1) seems to separate cells according to their sample, which may not be biological but rather a batch effect. For example, the genes most commonly associated with a positive value for PC1 are most highly expressed in PBMMC-3 and not PBMMC-1 or PBMMC-2. Conversely, the genes most commonly associated with a negative value for PC1 are most highly expressed in PBMMC-1 and PBMMC-2 but not PBMMC-3.\n\n\nFor further dimensionality reduction, we need to select a number of PCs to use (the rest are excluded). Ideally, we want to capture as much data variance as possible in as few PCs as possible. The plot generated with pca_variance_ratio can help you in determining how many PCs to use for downstream analysis such as UMAP:\n\nsc.pl.pca_variance_ratio(adata, log=False) # see contribution of each PC to variance\n\n\n\n\n\n\n\n\nThis plot ranks principle components based on the percentage of variance explained by each one. The majority of true signal is captured by the PCs up until the point at which an elbow (and plateau) is reached.\nIncluding too many PCs usually does not affect much the result, while including too few PCs can affect the results very much\nAnother - maybe more intuitive - way to see it, is to use a cumulative sum to show the % unexplained variance in function of the number of PCs. If we look at the first PCs, we can see that we passed the elbow with 30 to 50 PCs, which should ensure that we capture the major part of the variability in our dataset to be shown on a 2D representation like UMAP or tSNE.\n\nplt.plot(100 - (np.cumsum(adata.uns[\"pca\"][\"variance_ratio\"])*100)/sum(adata.uns[\"pca\"][\"variance_ratio\"]))\nplt.ylabel('% unexplained variance') ; plt.xlabel('N PCs')\n\nText(0.5, 0, 'N PCs')"
  },
  {
    "objectID": "ipynb/day2-1_dimensionality_reduction.html#dimensionality-reduction",
    "href": "ipynb/day2-1_dimensionality_reduction.html#dimensionality-reduction",
    "title": "Dimensionality Reduction",
    "section": "Dimensionality reduction",
    "text": "Dimensionality reduction\nExercise 5: Compute the neighborhood graph of cells using the PCA representation of the data matrix. The purpose of this step is to understand the “distance” between individual cells in the lower-dimensional PCA space, important for creating 2D scatter plot representations of your data. The number of neighbors used will influence how much the data is smoothened, which is a necessary step due to the sparsity (missing values) widely present in scRNA-seq data compared to bulk methods. Use the sc.pp.neighbors function and a number of components chosen based on the pca_variance_ratio and cumulative sum plots above.\n\nClick for Answer\n\n\nAnswer:\n    sc.pp.neighbors(adata, n_pcs = 30) # specify the number of neighbors and number of PCs you wish to use\n\n\n\nEmbedding the graph in a 2D representation can be performed using either tSNE or UMAP algorithms.\nExercise 6: Run tSNE and UMAP algorithms on your data after completing the previous steps with default parameters. We will evaluate the quality of each approach in later exercises.\n\nClick for Answer\n\n\nAnswer:\n    sc.tl.tsne(adata)\n    sc.tl.umap(adata)\n\n\n\nTo view the UMAP plot:\n\nsc.pl.tsne(adata, color=[\"sample\"])\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py:394: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  cax = scatter(\n\n\n\n\n\n\n\n\n\n\nsc.pl.umap(adata, color=[\"sample\"])\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py:394: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  cax = scatter(\n\n\n\n\n\n\n\n\n\nExercise 7: Try out the following:\nA. Color the dots in the UMAP according to a variable (e.g. n_counts or HBA1). Any idea where the erythrocytes probably are in the UMAP?\nB. Change the number of neighbors used for the calculation of the UMAP. Which is the parameter to change and how did it affect the output. What is the default? In which situation would you lower/increase this?\nC. Change the number of principal components (n_pcs) to extremely few (5) or many (50). How does this it affect the output? In your opinion, it is better with fewer or more PCs? Why does n_pcs=150 not work? When would more precision be needed?\n\nsc.pl.umap(adata, color=[\"n_counts\", \"HBA1\"])\n\n\n\n\n\n\n\n\nExercise 8: What do you think is the main problem with this low dimensional embeddings - do they capture the expected variance in the dataset?\n\nClick for Answer\n\n\nAnswer: The biggest problem appears to be that there is a strong batch effect. Different regions of the UMAP and tSNE are both occupied by cells belonging to a single sample.\n\n\nSave your results!\n\nadata.write_h5ad(\"PBMC_analysis_SIB_tutorial3.h5ad\")"
  },
  {
    "objectID": "ipynb/day2-3_clustering.html",
    "href": "ipynb/day2-3_clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Download Presentation: Clustering\n\nimport scanpy as sc\nimport pandas as pd # for handling data frames (i.e. data tables)\nimport numpy as np # for handling numbers, arrays, and matrices\nimport matplotlib.pyplot as plt # plotting package\nimport seaborn as sns # plotting package\n\nExercise 0: Before we continue in this notebook with the next steps of the analysis, we need to load our results from the previous notebook using the sc.read_h5ad function and assign them to the variable name adata. Give it a try!\n\nClick for Answer\n\n\nAnswer: Load your data from the previous notebook into an h5ad object with sc.read_h5ad\n    adata = sc.read_h5ad(\"PBMC_analysis_SIB_tutorial4.h5ad\")\n\n\n\n\nadata\n\nAnnData object with n_obs × n_vars = 5465 × 3000\n    obs: 'sample', 'n_counts', 'n_genes', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_20_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'log1p_total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'log1p_total_counts_hb', 'pct_counts_hb', 'is_doublet', 'S_score', 'G2M_score', 'phase'\n    var: 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'mean', 'std'\n    uns: 'hvg', 'log1p', 'neighbors', 'pca', 'phase_colors', 'sample_colors', 'umap'\n    obsm: 'X_pca', 'X_pcahm', 'X_umap'\n    varm: 'PCs'\n    obsp: 'connectivities', 'distances'\n\n\nClustering the data helps to identify cells with similar gene expression properties that may belong to the same cell type or cell state. There are two popular clustering methods, both available in scanpy: Louvain and Leiden clustering.\nExercise 1: Run Louvain and Leiden clustering algorithms. Visualize the clusters on your UMAP representation. Are the clusters different from each method?\n\nClick for Answer\n\n\nAnswer:\n    sc.tl.louvain(adata)\n    sc.tl.leiden(adata)\n\n\n\nNext, you can visualize your UMAP and tSNE representations of the scRNA-seq and color by various metadata attributes (including Louvian or Leiden clusters) from the prior steps. For example:\n\nsc.pl.umap(adata, color=[\"louvain\", \"leiden\"], legend_loc=\"on data\")\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py:394: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  cax = scatter(\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py:394: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  cax = scatter(\n\n\n\n\n\n\n\n\n\nExercise 2: Focus on the leiden clusters from this exercise and forward. How many cells do you have per cluster? Can you plot a histogram of this?\n\nClick for Answer\n\n\nAnswer:\n    n_clusters = adata.obs[\"leiden\"].value_counts()\n    ax = n_clusters.plot(kind=\"bar\")\n    plt.show()\n\n\n\nExercise 3: Visualize some of the other metadata on the UMAP embedding, including the n_counts, sample, n_genes, pct_counts_mt, and phase metadata found in adata.obs.\nDo any clusters seem to have an obvious bias towards particular attributes? This might be a sign that we want to optimize prior steps of the analysis, such as adjusting the number of principal components used in the neighborhood smoothing or regressing out particular variables. As with a pandas dataframe, you can also examine the frequency of various attributes using a command such as: adata.obs[\"phase\"].value_counts().\n\nsc.pl.umap(adata, color=['n_counts', 'sample'])\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py:394: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  cax = scatter(\n\n\n\n\n\n\n\n\n\n\nsc.pl.umap(adata, color=['n_genes', 'pct_counts_mt', 'phase'])\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py:394: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  cax = scatter(\n\n\n\n\n\n\n\n\n\nExercise 4: Let’s proceed with Leiden clustering and UMAP embeddings for the time being.\n\nCreate a new metadata attribute for your current clusters, i.e. adata.obs[\"leiden_res1\"] = adata.obs[\"leiden\"].\nRepeat leiden clustering using different values for the resolution parameter: 0.1, 0.5, 2.0.\nSave the clusters in a new metadata column and visualize them on the UMAP representation.\nHow does the number of clusters change with adjustments to the resolution parameter? Using the resolution=1 as a basis, do any clusters divide into two smaller clusters upon changing the resolution parameter? Do any clusters merge together? Can you plot the three different clustering results side-by-side on the UMAP to compare?\n\n\nadata.obs[\"leiden_res1\"] = adata.obs[\"leiden\"]\n\n\nsc.tl.leiden(adata, key_added=\"leiden_res0_1\", resolution=0.1)\nsc.tl.leiden(adata, key_added=\"leiden_res0_5\", resolution=0.5)\nsc.tl.leiden(adata, key_added=\"leiden_res2\", resolution=2)\n\n\nsc.pl.umap(\n    adata,\n    color=[\"leiden_res0_1\", \"leiden_res0_5\", \"leiden_res1\", \"leiden_res2\"],\n    legend_loc=\"on data\",\n    ncols=2,\n)\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py:394: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  cax = scatter(\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py:394: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  cax = scatter(\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py:394: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  cax = scatter(\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py:394: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  cax = scatter(\n\n\n\n\n\n\n\n\n\n\nadata.write_h5ad(\"PBMC_analysis_SIB_tutorial5.h5ad\")"
  },
  {
    "objectID": "ipynb/day3-1_tf.html",
    "href": "ipynb/day3-1_tf.html",
    "title": "Gene regulatory analysis with pySCENIC",
    "section": "",
    "text": "This exercise has been largely borrowed from the pySCENIC tutorial and adapted to work with a dataset of 10X PBMCs which you have downloaded with the course data. As it takes a long time run, we suggest if you are curious in the method to run it after the course.\nRelated paper – SCENIC: single-cell regulatory network inference and clustering.\nDownload Presentation: SCENIC Overview\nOnce single-cell genomics data has been processed, one can dissect important relationships between observed features in their genome context. In our genome, the activation of genes is controlled in the nucleus by the RNA transcriptional machinery, which activates local (promoters) or distal cis-regulatory elements (enhancers), to control the amount of RNA produced by every gene.\nConceptually, a Gene Regulatory Network (GRN) refers to a graph representation of how certain genes that control transcription i.e. “Transcription Factors” (TF) are in charge of directly controlling the transcription rates of their target genes (cis-regulation). At the same time, such target genes once activated, can be in charge of controlling other downstream target genes (trans-regulation). Computationally, methods that infer GRNs consider the co-variation of gene and chromatin accessibility features to identify modules that could be grouped and associated simultaneously with a few TFs. A group of genes controlled by the activity of the same TF is defined as a regulon.\nIn addition to co-variation, several methods have recognized the gathering and injection of prior knowledge data, such as the locations where a TF bind in the genome, or previously reported TF-target gene associations, to pre-define gene-gene edges that guide the inference of GRNs that are most-supported by that type of evidence.\nThe purpose of this exercise is to showcase the ability to infer information about gene regulatory networks using transcriptomic data alone: by pairing expression of transcription factors (TFs) with co-expressed genes, we can predict which genes are likely being activate by a particular TF in a specific cell population."
  },
  {
    "objectID": "ipynb/day3-1_tf.html#pyscenic",
    "href": "ipynb/day3-1_tf.html#pyscenic",
    "title": "Gene regulatory analysis with pySCENIC",
    "section": "pySCENIC",
    "text": "pySCENIC\n\n# import dependencies\nimport os\nimport numpy as np\nimport pandas as pd\nimport scanpy as sc\nimport loompy as lp\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport glob\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:68: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_to_dna(twobit: int, size: int) -&gt; str:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:85: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def dna_to_twobit(dna: str) -&gt; int:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:102: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_1hamming(twobit: int, size: int) -&gt; List[int]:\n\n\n\nLoad dataset\n\nadata = sc.read_10x_h5(\"course_data/pbmc_10k_v3_filtered_feature_bc_matrix.h5\")\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/anndata/_core/anndata.py:1840: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n  utils.warn_names_duplicates(\"var\")\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/anndata/_core/anndata.py:1840: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n  utils.warn_names_duplicates(\"var\")\n\n\n\nadata.var_names_make_unique()\n\n\nnCountsPerGene = np.sum(adata.X, axis=0)\nnCellsPerGene = np.sum(adata.X&gt;0, axis=0)\n\n# Show info\nprint(\"Number of counts (in the dataset units) per gene:\", nCountsPerGene.min(), \" - \" ,nCountsPerGene.max())\nprint(\"Number of cells in which each gene is detected:\", nCellsPerGene.min(), \" - \" ,nCellsPerGene.max())\n\nNumber of counts (in the dataset units) per gene: 0.0  -  3567008.0\nNumber of cells in which each gene is detected: 0  -  11766\n\n\n\nnCells=adata.X.shape[0]\n\n# pySCENIC thresholds\nminCountsPerGene=3*.01*nCells # 3 counts in 1% of cells\nprint(\"minCountsPerGene: \", minCountsPerGene)\n\nminSamples=.01*nCells # 1% of cells\nprint(\"minSamples: \", minSamples)\n\nminCountsPerGene:  353.07\nminSamples:  117.69\n\n\n\n# simply compute the number of genes per cell (computers 'n_genes' column)\nsc.pp.filter_cells(adata, min_genes=0)\n# mito and genes/counts cuts\nmito_genes = adata.var_names.str.startswith('MT-')\n# for each cell compute fraction of counts in mito genes vs. all genes\nadata.obs['percent_mito'] = np.sum(\n    adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1\n# add the total counts per cell as observations-annotation to adata\nadata.obs['n_counts'] = adata.X.sum(axis=1).A1\n\n\n# initial cuts\nsc.pp.filter_cells(adata, min_genes=200)\nsc.pp.filter_genes(adata, min_cells=3)\n\n\nadata = adata[adata.obs['n_genes'] &lt; 4000, :]\nadata = adata[adata.obs['percent_mito'] &lt; 0.15, :]\n\n\n# save a copy of the raw data\nadata.raw = adata\n\n# Total-count normalize (library-size correct) to 10,000 reads/cell\nsc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4)\n\n# log transform the data.\nsc.pp.log1p(adata)\n\n# identify highly variable genes.\nsc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5)\nsc.pl.highly_variable_genes(adata)\n\n# keep only highly variable genes:\nadata = adata[:, adata.var['highly_variable']]\n\n# regress out total counts per cell and the percentage of mitochondrial genes expressed\nsc.pp.regress_out(adata, ['n_counts', 'percent_mito'] ) #, n_jobs=args.threads)\n\n# scale each gene to unit variance, clip values exceeding SD 10.\nsc.pp.scale(adata, max_value=10)\n\n\n\n\n\n\n\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/anndata/_core/anndata.py:1230: ImplicitModificationWarning: Trying to modify attribute `.var` of view, initializing view as actual.\n  df[key] = c\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/anndata/_core/anndata.py:1230: ImplicitModificationWarning: Trying to modify attribute `.var` of view, initializing view as actual.\n  df[key] = c\n\n\n\n# principal component analysis\nsc.tl.pca(adata, svd_solver='arpack')\nsc.pl.pca_variance_ratio(adata, log=True)\n\n\n\n\n\n\n\n\n\nsc.pp.neighbors(adata, n_neighbors=15, n_pcs=30)\nsc.tl.umap(adata)\n\n\nsc.tl.louvain(adata,resolution=0.4)\nsc.pl.umap(adata, color=['louvain'] )\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py:394: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  cax = scatter(\n\n\n\n\n\n\n\n\n\n\nsc.tl.rank_genes_groups(adata, 'louvain', method='t-test')\nsc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)\n\nWARNING: It seems you use rank_genes_groups on the raw count data. Please logarithmize your data before calling rank_genes_groups.\n\n\n\n\n\n\n\n\n\n\n\nStep 1: Gene regulatory network inference, and generation of co-expression modules\nGRN inference using the GRNBoost2 algorithm: for this step the CLI version of SCENIC is used. This step can be deployed on an High Performance Computing system. We use the counts matrix (without log transformation or further processing) from the loom file we wrote earlier.\nOutput: List of adjacencies between a TF and its targets stored in ADJACENCIES_FNAME.\n\n## this file has to be downloaded if not found\n!wget -nc https://raw.githubusercontent.com/aertslab/SCENICprotocol/master/example/allTFs_hg38.txt\n\nFile ‘allTFs_hg38.txt’ already there; not retrieving.\n\n\n\n\nf_tfs = \"allTFs_hg38.txt\"\nf_loom_path_scenic = \"pbmc10k_filtered_scenic.loom\"\n\n\n# create basic row and column attributes for the loom file:\nrow_attrs = {\n    \"Gene\": np.array(adata.var_names) ,\n}\ncol_attrs = {\n    \"CellID\": np.array(adata.obs_names) ,\n    \"nGene\": np.array( np.sum(adata.X.transpose()&gt;0 , axis=0)).flatten() ,\n    \"nUMI\": np.array( np.sum(adata.X.transpose() , axis=0)).flatten() ,\n}\nlp.create( f_loom_path_scenic, adata.X.transpose(), row_attrs, col_attrs)\n\n\n!pyscenic grn {f_loom_path_scenic} {f_tfs} -o adj.csv --num_workers 20\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:68: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_to_dna(twobit: int, size: int) -&gt; str:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:85: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def dna_to_twobit(dna: str) -&gt; int:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:102: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_1hamming(twobit: int, size: int) -&gt; List[int]:\n\n2024-05-28 21:54:30,489 - pyscenic.cli.pyscenic - INFO - Loading expression matrix.\n\n2024-05-28 21:54:31,078 - pyscenic.cli.pyscenic - INFO - Inferring regulatory networks.\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:68: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_to_dna(twobit: int, size: int) -&gt; str:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:85: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def dna_to_twobit(dna: str) -&gt; int:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:102: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_1hamming(twobit: int, size: int) -&gt; List[int]:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:68: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_to_dna(twobit: int, size: int) -&gt; str:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:85: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def dna_to_twobit(dna: str) -&gt; int:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:102: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_1hamming(twobit: int, size: int) -&gt; List[int]:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:68: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_to_dna(twobit: int, size: int) -&gt; str:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:85: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def dna_to_twobit(dna: str) -&gt; int:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:102: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_1hamming(twobit: int, size: int) -&gt; List[int]:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:68: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_to_dna(twobit: int, size: int) -&gt; str:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:85: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def dna_to_twobit(dna: str) -&gt; int:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:102: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_1hamming(twobit: int, size: int) -&gt; List[int]:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:68: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_to_dna(twobit: int, size: int) -&gt; str:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:85: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def dna_to_twobit(dna: str) -&gt; int:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:102: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_1hamming(twobit: int, size: int) -&gt; List[int]:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:68: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_to_dna(twobit: int, size: int) -&gt; str:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:85: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def dna_to_twobit(dna: str) -&gt; int:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:102: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_1hamming(twobit: int, size: int) -&gt; List[int]:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:68: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_to_dna(twobit: int, size: int) -&gt; str:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:85: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def dna_to_twobit(dna: str) -&gt; int:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:102: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_1hamming(twobit: int, size: int) -&gt; List[int]:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:68: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_to_dna(twobit: int, size: int) -&gt; str:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:85: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def dna_to_twobit(dna: str) -&gt; int:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:102: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_1hamming(twobit: int, size: int) -&gt; List[int]:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:68: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_to_dna(twobit: int, size: int) -&gt; str:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:85: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def dna_to_twobit(dna: str) -&gt; int:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:102: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_1hamming(twobit: int, size: int) -&gt; List[int]:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:68: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_to_dna(twobit: int, size: int) -&gt; str:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:85: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def dna_to_twobit(dna: str) -&gt; int:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:102: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_1hamming(twobit: int, size: int) -&gt; List[int]:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:68: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_to_dna(twobit: int, size: int) -&gt; str:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:85: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def dna_to_twobit(dna: str) -&gt; int:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:102: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_1hamming(twobit: int, size: int) -&gt; List[int]:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:68: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_to_dna(twobit: int, size: int) -&gt; str:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:85: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def dna_to_twobit(dna: str) -&gt; int:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:102: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_1hamming(twobit: int, size: int) -&gt; List[int]:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:68: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_to_dna(twobit: int, size: int) -&gt; str:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:85: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def dna_to_twobit(dna: str) -&gt; int:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:102: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_1hamming(twobit: int, size: int) -&gt; List[int]:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:68: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_to_dna(twobit: int, size: int) -&gt; str:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:85: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def dna_to_twobit(dna: str) -&gt; int:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:102: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_1hamming(twobit: int, size: int) -&gt; List[int]:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:68: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_to_dna(twobit: int, size: int) -&gt; str:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:85: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def dna_to_twobit(dna: str) -&gt; int:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:102: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_1hamming(twobit: int, size: int) -&gt; List[int]:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:68: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_to_dna(twobit: int, size: int) -&gt; str:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:85: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def dna_to_twobit(dna: str) -&gt; int:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:102: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_1hamming(twobit: int, size: int) -&gt; List[int]:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:68: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_to_dna(twobit: int, size: int) -&gt; str:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:85: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def dna_to_twobit(dna: str) -&gt; int:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:102: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_1hamming(twobit: int, size: int) -&gt; List[int]:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:68: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_to_dna(twobit: int, size: int) -&gt; str:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:85: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def dna_to_twobit(dna: str) -&gt; int:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:102: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_1hamming(twobit: int, size: int) -&gt; List[int]:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:68: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_to_dna(twobit: int, size: int) -&gt; str:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:85: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def dna_to_twobit(dna: str) -&gt; int:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:102: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_1hamming(twobit: int, size: int) -&gt; List[int]:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:68: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_to_dna(twobit: int, size: int) -&gt; str:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:85: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def dna_to_twobit(dna: str) -&gt; int:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:102: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_1hamming(twobit: int, size: int) -&gt; List[int]:\npreparing dask client\nparsing input\ncreating dask graph\n20 partitions\ncomputing dask graph\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/distributed/client.py:3108: UserWarning: Sending large graph of size 21.19 MiB.\nThis may cause some slowdown.\nConsider scattering data ahead of time and using futures.\n  warnings.warn(\nnot shutting down client, client was created externally\nfinished\n\n2024-05-28 21:58:19,684 - pyscenic.cli.pyscenic - INFO - Writing results to file.\n\n\n\nadjacencies = pd.read_csv(\"adj.csv\", index_col=False)\n\nVisualize the distribution of weights for general inspection of the quantiles and thresholds obtained from pyscenic. As provided by the pyscenic grn step, the importance scores follow a unimodal distribution, with negative/positive values indicating TF-gene associations with less/more importance, respectively. From the right-tail of this distribution, we can recover the most relevant interactions between TFs and potential target genes, supported by gene expression values and the analysis done by pyscenic.\n\nadjacencies.head()\n\n\n\n\n\n\n\n\nTF\ntarget\nimportance\n\n\n\n\n0\nCEBPD\nVCAN\n33.587173\n\n\n1\nZEB2\nLTB\n33.086506\n\n\n2\nKLF4\nVCAN\n29.961844\n\n\n3\nCEBPD\nSRGN\n29.306472\n\n\n4\nMEF2C\nHLA-DRA\n28.519784\n\n\n\n\n\n\n\n\nplt.hist(np.log10(adjacencies[\"importance\"]), bins=50)\nplt.xlim([-10, 10])\n\n\n\n\n\n\n\n\nAs targets genes have DNA motifs at promoters (sequence specific DNA motifs), those can be used to link TFs to target genes. Next, we use an annotation of TF associations to Transcription Start Sites (TSSs) to refine this annotation.\n\n\nStep 2-3: Regulon prediction aka cisTarget from CLI\nFor this step the CLI version of SCENIC is used. This step can be deployed on an High Performance Computing system.\nOutput: List of adjacencies between a TF and its targets stored in MOTIFS_FNAME.\nDownload TSS annotations precalculated by Aerts’s lab:\n\n!wget -nc https://resources.aertslab.org/cistarget/databases/homo_sapiens/hg38/refseq_r80/mc9nr/gene_based/hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings.feather\n\nFile ‘hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings.feather’ already there; not retrieving.\n\n\n\n\n# ranking databases\ndb_glob = \"*feather\"\ndb_names = \" \".join(glob.glob(db_glob))\n\n\ndb_names\n\n'hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings.feather'\n\n\nDownload a catalog of motif-to-TF associations\n\n!wget -nc https://resources.aertslab.org/cistarget/motif2tf/motifs-v9-nr.hgnc-m0.001-o0.0.tbl\n\nFile ‘motifs-v9-nr.hgnc-m0.001-o0.0.tbl’ already there; not retrieving.\n\n\n\n\n# motif databases\nmotif_path = \"motifs-v9-nr.hgnc-m0.001-o0.0.tbl\"\n\n\n!pyscenic ctx adj.csv \\\n    'hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings.feather' \\\n    --annotations_fname \"motifs-v9-nr.hgnc-m0.001-o0.0.tbl\" \\\n    --expression_mtx_fname \"pbmc10k_filtered_scenic.loom\" \\\n    --output reg.csv \\\n    --mask_dropouts \\\n    --num_workers 3 &gt; pyscenic_ctx_stdout.txt\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:68: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_to_dna(twobit: int, size: int) -&gt; str:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:85: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def dna_to_twobit(dna: str) -&gt; int:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:102: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_1hamming(twobit: int, size: int) -&gt; List[int]:\n\n2024-05-28 21:58:28,417 - pyscenic.cli.pyscenic - INFO - Creating modules.\n\n2024-05-28 21:58:28,483 - pyscenic.cli.pyscenic - INFO - Loading expression matrix.\n\n2024-05-28 21:58:29,374 - pyscenic.utils - INFO - Calculating Pearson correlations.\n\n2024-05-28 21:58:29,393 - pyscenic.utils - WARNING - Note on correlation calculation: the default behaviour for calculating the correlations has changed after pySCENIC verion 0.9.16. Previously, the default was to calculate the correlation between a TF and target gene using only cells with non-zero expression values (mask_dropouts=True). The current default is now to use all cells to match the behavior of the R verision of SCENIC. The original settings can be retained by setting 'rho_mask_dropouts=True' in the modules_from_adjacencies function, or '--mask_dropouts' from the CLI.\n    Dropout masking is currently set to [True].\n\n2024-05-28 21:58:36,405 - pyscenic.utils - INFO - Creating modules.\n\n2024-05-28 21:58:43,251 - pyscenic.cli.pyscenic - INFO - Loading databases.\n\n2024-05-28 21:58:43,507 - pyscenic.cli.pyscenic - INFO - Calculating regulons.\n\n2024-05-28 21:58:43,507 - pyscenic.prune - INFO - Using 3 workers.\n\n2024-05-28 21:58:43,507 - pyscenic.prune - INFO - Using 3 workers.\n\n2024-05-28 21:58:46,654 - pyscenic.prune - INFO - Worker hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings(2): database loaded in memory.\n\n2024-05-28 21:58:46,654 - pyscenic.prune - INFO - Worker hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings(2): database loaded in memory.\n\n2024-05-28 21:58:46,657 - pyscenic.prune - INFO - Worker hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings(3): database loaded in memory.\n\n2024-05-28 21:58:46,657 - pyscenic.prune - INFO - Worker hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings(3): database loaded in memory.\n\n2024-05-28 21:58:46,718 - pyscenic.prune - INFO - Worker hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings(1): database loaded in memory.\n\n2024-05-28 21:58:46,718 - pyscenic.prune - INFO - Worker hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings(1): database loaded in memory.\n\n2024-05-28 21:58:47,490 - pyscenic.prune - INFO - Worker hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings(1): motif annotations loaded in memory.\n\n2024-05-28 21:58:47,490 - pyscenic.prune - INFO - Worker hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings(1): motif annotations loaded in memory.\n\n2024-05-28 21:58:47,491 - pyscenic.prune - INFO - Worker hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings(2): motif annotations loaded in memory.\n\n2024-05-28 21:58:47,491 - pyscenic.prune - INFO - Worker hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings(3): motif annotations loaded in memory.\n\n2024-05-28 21:58:47,491 - pyscenic.prune - INFO - Worker hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings(2): motif annotations loaded in memory.\n\n2024-05-28 21:58:47,491 - pyscenic.prune - INFO - Worker hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings(3): motif annotations loaded in memory.\n\n2024-05-28 21:59:02,767 - pyscenic.transform - WARNING - Less than 80% of the genes in KLF12 could be mapped to hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings. Skipping this module.\n\n2024-05-28 21:59:04,752 - pyscenic.transform - WARNING - Less than 80% of the genes in Regulon for KLF12 could be mapped to hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings. Skipping this module.\n\n2024-05-28 21:59:28,614 - pyscenic.transform - WARNING - Less than 80% of the genes in Regulon for EGR1 could be mapped to hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings. Skipping this module.\n\n2024-05-28 21:59:34,853 - pyscenic.transform - WARNING - Less than 80% of the genes in Regulon for MYC could be mapped to hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings. Skipping this module.\n\n2024-05-28 21:59:36,692 - pyscenic.transform - WARNING - Less than 80% of the genes in PBX4 could be mapped to hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings. Skipping this module.\n\n2024-05-28 21:59:45,671 - pyscenic.transform - WARNING - Less than 80% of the genes in RPS6KA5 could be mapped to hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings. Skipping this module.\n\n2024-05-28 22:00:25,006 - pyscenic.transform - WARNING - Less than 80% of the genes in ZNF595 could be mapped to hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings. Skipping this module.\n\n2024-05-28 22:00:37,943 - pyscenic.transform - WARNING - Less than 80% of the genes in Regulon for MYC could be mapped to hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings. Skipping this module.\n\n2024-05-28 22:00:47,258 - pyscenic.transform - WARNING - Less than 80% of the genes in Regulon for PBX4 could be mapped to hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings. Skipping this module.\n\n2024-05-28 22:00:50,970 - pyscenic.transform - WARNING - Less than 80% of the genes in Regulon for ZNF367 could be mapped to hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings. Skipping this module.\n\n2024-05-28 22:00:51,277 - pyscenic.transform - WARNING - Less than 80% of the genes in Regulon for ZNF527 could be mapped to hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings. Skipping this module.\n\n2024-05-28 22:01:07,842 - pyscenic.transform - WARNING - Less than 80% of the genes in BNC2 could be mapped to hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings. Skipping this module.\n\n2024-05-28 22:01:18,465 - pyscenic.transform - WARNING - Less than 80% of the genes in CYB5R1 could be mapped to hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings. Skipping this module.\n\n2024-05-28 22:01:22,986 - pyscenic.transform - WARNING - Less than 80% of the genes in EBF1 could be mapped to hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings. Skipping this module.\n\n2024-05-28 22:01:30,579 - pyscenic.transform - WARNING - Less than 80% of the genes in Regulon for ZNF595 could be mapped to hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings. Skipping this module.\n\n2024-05-28 22:01:30,774 - pyscenic.transform - WARNING - Less than 80% of the genes in Regulon for ZNF83 could be mapped to hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings. Skipping this module.\n\n2024-05-28 22:02:02,012 - pyscenic.transform - WARNING - Less than 80% of the genes in KLF12 could be mapped to hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings. Skipping this module.\n\n2024-05-28 22:02:18,349 - pyscenic.transform - WARNING - Less than 80% of the genes in MYC could be mapped to hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings. Skipping this module.\n\n2024-05-28 22:02:23,860 - pyscenic.transform - WARNING - Less than 80% of the genes in NPDC1 could be mapped to hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings. Skipping this module.\n\n2024-05-28 22:03:15,944 - pyscenic.transform - WARNING - Less than 80% of the genes in ZEB1 could be mapped to hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings. Skipping this module.\n\n2024-05-28 22:03:16,218 - pyscenic.transform - WARNING - Less than 80% of the genes in ZNF527 could be mapped to hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings. Skipping this module.\n\n2024-05-28 22:03:26,272 - pyscenic.prune - INFO - Worker hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings(3): All regulons derived.\n\n2024-05-28 22:03:26,272 - pyscenic.prune - INFO - Worker hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings(3): All regulons derived.\n\n2024-05-28 22:03:26,413 - pyscenic.prune - INFO - Worker hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings(3): Done.\n\n2024-05-28 22:03:26,413 - pyscenic.prune - INFO - Worker hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings(3): Done.\n\n2024-05-28 22:03:51,601 - pyscenic.transform - WARNING - Less than 80% of the genes in EGR1 could be mapped to hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings. Skipping this module.\n\n2024-05-28 22:03:58,267 - pyscenic.transform - WARNING - Less than 80% of the genes in ESR2 could be mapped to hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings. Skipping this module.\n\n2024-05-28 22:04:12,091 - pyscenic.transform - WARNING - Less than 80% of the genes in Regulon for EBF1 could be mapped to hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings. Skipping this module.\n\n2024-05-28 22:04:16,918 - pyscenic.prune - INFO - Worker hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings(2): All regulons derived.\n\n2024-05-28 22:04:16,918 - pyscenic.prune - INFO - Worker hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings(2): All regulons derived.\n\n2024-05-28 22:04:16,977 - pyscenic.prune - INFO - Worker hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings(2): Done.\n\n2024-05-28 22:04:16,977 - pyscenic.prune - INFO - Worker hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings(2): Done.\n\n2024-05-28 22:04:46,928 - pyscenic.prune - INFO - Worker hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings(1): All regulons derived.\n\n2024-05-28 22:04:46,928 - pyscenic.prune - INFO - Worker hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings(1): All regulons derived.\n\n2024-05-28 22:04:47,025 - pyscenic.prune - INFO - Worker hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings(1): Done.\n\n2024-05-28 22:04:47,025 - pyscenic.prune - INFO - Worker hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings(1): Done.\n\n2024-05-28 22:04:47,095 - pyscenic.cli.pyscenic - INFO - Writing results to file.\n\n\nTo explore the candidates reported, it is recommended as a rule of thumb to explore the output by the ranking of relative contribution, or by top-quantile thresholds defined visually to obtain a high signal-to-noise ratio.\n\n\nDefine custom quantiles for further exploration\n\nimport numpy as np\n\nn_genes_detected_per_cell = np.sum(adata.X &gt; 0, axis=1)\npercentiles = pd.Series(n_genes_detected_per_cell.flatten()).quantile(\n    [0.01, 0.05, 0.10, 0.50, 1]\n)\nprint(percentiles)\n\n0.01     136.0\n0.05     155.0\n0.10     169.0\n0.50     237.0\n1.00    1017.0\ndtype: float64\n\n\nThe histogram below indicates the distribution of genes detected per cell. This visualization is convenient to define the parameter –auc_threshold in the next step. Specifically, the default parameter of –auc_threshold is 0.05, which in this plot would result in the selection of 144 genes, to be used as a reference per cell for AUCell calculations. The modification of this parameter affects the estimation of AUC values calculated by AUCell.\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 5), dpi=100)\nsns.distplot(n_genes_detected_per_cell, norm_hist=False, kde=False, bins=\"fd\")\nfor i, x in enumerate(percentiles):\n    fig.gca().axvline(x=x, ymin=0, ymax=1, color=\"red\")\n    ax.text(\n        x=x,\n        y=ax.get_ylim()[1],\n        s=f\"{int(x)} ({percentiles.index.values[i]*100}%)\",\n        color=\"red\",\n        rotation=30,\n        size=\"x-small\",\n        rotation_mode=\"anchor\",\n    )\nax.set_xlabel(\"# of genes\")\nax.set_ylabel(\"# of cells\")\nfig.tight_layout()\n\n/tmp/ipykernel_279481/1384461256.py:2: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(n_genes_detected_per_cell, norm_hist=False, kde=False, bins=\"fd\")\n\n\n\n\n\n\n\n\n\nThis step will use TFs to calculate Area Under the Curve scores, that summarize how well the gene expression observed in each cell can be associated by the regulation of target genes regulatred by the mentioned TFs.\nUsing the above-generated matrix of cell x TFs and those scores, we can calculate a new embedding using only those.\n\n\nStep 4: Cellular enrichment (aka AUCell) from CLI\nIt is important to check that most cells have a substantial fraction of expressed/detected genes in the calculation of the AUC. The following histogram gives an idea of the distribution and allows selection of an appropriate threshold. In this plot, a few thresholds are highlighted, with the number of genes selected shown in red text and the corresponding percentile in parentheses). See the relevant section in the R tutorial for more information.\nBy using the default setting for –auc_threshold of 0.05, we see that 1192 genes are selected for the rankings based on the plot below.\n\n!pyscenic aucell \"pbmc10k_filtered_scenic.loom\" \\\n    \"reg.csv\" \\\n    --output \"auc.csv\" \\\n    --num_workers 3 &gt; \"pyscenic_aucell_stdout.txt\"\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:68: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_to_dna(twobit: int, size: int) -&gt; str:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:85: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def dna_to_twobit(dna: str) -&gt; int:\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/loompy/bus_file.py:102: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def twobit_1hamming(twobit: int, size: int) -&gt; List[int]:\n\n2024-05-28 22:04:55,503 - pyscenic.cli.pyscenic - INFO - Loading expression matrix.\n\n2024-05-28 22:04:56,091 - pyscenic.cli.pyscenic - INFO - Loading gene signatures.\n\n2024-05-28 22:04:57,373 - pyscenic.cli.pyscenic - INFO - Calculating cellular enrichment.\n\n2024-05-28 22:05:05,478 - pyscenic.cli.pyscenic - INFO - Writing results to file.\n\n\n\nauc_df = pd.read_csv(\"auc.csv\", index_col=0)\n\n\nauc_df.head()\n\n\n\n\n\n\n\n\nARID5B(+)\nBACH2(+)\nBATF(+)\nBCL11A(+)\nBCL6(+)\nCEBPB(+)\nCEBPD(+)\nCREB5(+)\nE2F1(+)\nE2F2(+)\n...\nSTAT1(+)\nTAL1(+)\nTBX21(+)\nTCF4(+)\nTCF7(+)\nTCF7L2(+)\nUSF2(+)\nZNF595(+)\nZNF831(+)\nZNF85(+)\n\n\nCell\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAAACCCAAGCGCCCAT-1\n0.035164\n0.000000\n0.047850\n0.013579\n0.024272\n0.018955\n0.016766\n0.026724\n0.013183\n0.036084\n...\n0.034538\n0.017201\n0.041977\n0.000000\n0.053187\n0.015954\n0.024688\n0.035599\n0.035599\n0.000000\n\n\nAAACCCACAGAGTTGG-1\n0.004977\n0.010264\n0.023925\n0.013579\n0.156211\n0.087353\n0.074413\n0.095586\n0.005611\n0.101618\n...\n0.049658\n0.030772\n0.017486\n0.000000\n0.000000\n0.066439\n0.006935\n0.000000\n0.000000\n0.000000\n\n\nAAACCCACAGGTATGG-1\n0.015828\n0.004716\n0.176838\n0.003637\n0.000000\n0.008990\n0.030588\n0.008107\n0.023693\n0.000000\n...\n0.011459\n0.015724\n0.082923\n0.000000\n0.000000\n0.011178\n0.000000\n0.000000\n0.049083\n0.000000\n\n\nAAACCCACATAGTCAC-1\n0.087623\n0.046325\n0.000000\n0.083342\n0.000000\n0.002954\n0.003111\n0.000000\n0.013093\n0.006796\n...\n0.000000\n0.008041\n0.011048\n0.066163\n0.000000\n0.002572\n0.034951\n0.000000\n0.026969\n0.101942\n\n\nAAACCCACATCCAATG-1\n0.016399\n0.046325\n0.079404\n0.006239\n0.000871\n0.009863\n0.017225\n0.005705\n0.023158\n0.000000\n...\n0.011141\n0.005931\n0.072210\n0.012945\n0.024905\n0.021464\n0.001664\n0.000000\n0.056634\n0.037621\n\n\n\n\n5 rows × 59 columns\n\n\n\n\nad_auc_mtx = sc.AnnData(auc_df)\nsc.pp.neighbors(ad_auc_mtx, n_neighbors=10, metric=\"correlation\")\nsc.tl.umap(ad_auc_mtx)\n\nWARNING: You’re trying to run this on 59 dimensions of `.X`, if you really want this, set `use_rep='X'`.\n         Falling back to preprocessing with `sc.pp.pca` and default params.\n\n\n\nadata.obsm[\"X_umap_aucell\"] = ad_auc_mtx.obsm[\"X_umap\"]\n\n\nsc.pl.embedding(adata, basis=\"X_umap_aucell\", color=\"louvain\")\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py:394: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  cax = scatter(\n\n\n\n\n\n\n\n\n\n\nsc.tl.louvain(ad_auc_mtx, resolution=0.2)\nsc.pl.embedding(ad_auc_mtx, basis=\"X_umap\", color='louvain')\n\n/home/alex/anaconda3/envs/sctp/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py:394: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  cax = scatter(\n\n\n\n\n\n\n\n\n\n\nsc.tl.rank_genes_groups(ad_auc_mtx, 'louvain', method='t-test')\n\n\nmarker_genes = pd.DataFrame(ad_auc_mtx.uns[\"rank_genes_groups\"][\"names\"])\nmarker_genes.columns = [\"Cluster\" + str(x) for x in range(0, len(ad_auc_mtx.obs[\"louvain\"].unique()))]\nmarker_genes.head()\n\n\n\n\n\n\n\n\nCluster0\nCluster1\nCluster2\nCluster3\nCluster4\nCluster5\nCluster6\n\n\n\n\n0\nPRDM1(+)\nSPI1(+)\nSPIB(+)\nMEOX1(+)\nTCF7L2(+)\nTAL1(+)\nCEBPB(+)\n\n\n1\nEOMES(+)\nNFE2(+)\nBCL11A(+)\nLEF1(+)\nSPI1(+)\nGATA1(+)\nSPI1(+)\n\n\n2\nTBX21(+)\nCEBPD(+)\nPAX5(+)\nTCF7(+)\nCEBPB(+)\nMAFG(+)\nNFE2(+)\n\n\n3\nRUNX3(+)\nCEBPB(+)\nIRF8(+)\nKLF12(+)\nMAFB(+)\nNFE2(+)\nGATA3(+)\n\n\n4\nKLF12(+)\nMAFB(+)\nIRF4(+)\nBACH2(+)\nE2F1(+)\nE2F1(+)\nCEBPD(+)\n\n\n\n\n\n\n\n\nsc.pl.embedding(ad_auc_mtx, basis=\"X_umap\", color=['SPI1(+)',\n                                                  'SPIB(+)', 'MEOX1(+)', 'TCF7L2(+)'], ncols=2)\n\n\n\n\n\n\n\n\n\nregulon_df = pd.read_csv(\"reg.csv\", index_col=0)\n\n\nregulon_df.head()\n\n\n\n\n\n\n\n\nUnnamed: 1\nEnrichment\nEnrichment.1\nEnrichment.2\nEnrichment.3\nEnrichment.4\nEnrichment.5\nEnrichment.6\nEnrichment.7\n\n\n\n\nNaN\nNaN\nAUC\nNES\nMotifSimilarityQvalue\nOrthologousIdentity\nAnnotation\nContext\nTargetGenes\nRankAtMax\n\n\nTF\nMotifID\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nARID5B\ncisbp__M0110\n0.09194977843426884\n3.4110817264013087\n0.0\n0.880471\ngene is orthologous to ENSMUSG00000019947 in M...\nfrozenset({'weight&gt;75.0%', 'activating', 'hg38...\n[('ID3', 0.8384963869548077), ('KAT6B', 0.3927...\n1113\n\n\nARID5B\ncisbp__M0107\n0.0899019739492413\n3.2853287808894263\n0.0\n0.880471\ngene is orthologous to ENSMUSG00000019947 in M...\nfrozenset({'weight&gt;75.0%', 'activating', 'hg38...\n[('NXPH4', 0.883027200867596), ('TRIB2', 0.556...\n818\n\n\nBATF\ndbcorrdb__RCOR1__ENCSR000ECM_1__m1\n0.09647906429927953\n3.295409378378749\n0.000478\n1.0\ngene is annotated for similar motif factorbook...\nfrozenset({'weight&gt;75.0%', 'activating', 'hg38...\n[('NOL4L', 0.4502441398393177), ('LLGL2', 0.40...\n1072\n\n\n\n\n\n\n\n\nregulon_df.shape\n\n(4115, 9)\n\n\n\nregulon_df.loc[\"SPIB\"]\n\n\n\n\n\n\n\n\nUnnamed: 1\nEnrichment\nEnrichment.1\nEnrichment.2\nEnrichment.3\nEnrichment.4\nEnrichment.5\nEnrichment.6\nEnrichment.7\n\n\n\n\nSPIB\ndbcorrdb__IRF1__ENCSR000EGK_1__m1\n0.07806698870214379\n3.90354587822946\n0.000965\n0.82397\nmotif similar to hocomoco__SPIB_MOUSE.H11MO.0....\nfrozenset({'weight&gt;75.0%', 'activating', 'hg38...\n[('FCGR2B', 1.869718459857277), ('STX7', 6.892...\n4932\n\n\nSPIB\ncisbp__M6313\n0.07676953171783304\n3.77165735433781\n0.000466\n0.82397\nmotif similar to hocomoco__SPIB_MOUSE.H11MO.0....\nfrozenset({'weight&gt;75.0%', 'activating', 'hg38...\n[('PNOC', 7.0142442259089695), ('PEG10', 1.095...\n856\n\n\nSPIB\ndbcorrdb__TBL1XR1__ENCSR000DYZ_1__m1\n0.07155724779432313\n3.241820572780774\n3.1e-05\n1.0\nmotif similar to hocomoco__SPIB_HUMAN.H11MO.0....\nfrozenset({'weight&gt;75.0%', 'activating', 'hg38...\n[('BTK', 3.20425594834044), ('CYB561A3', 9.741...\n2320\n\n\nSPIB\ncisbp__M4489\n0.08201924228512117\n4.305298612545568\n1e-06\n1.0\ngene is annotated for similar motif homer__AAA...\nfrozenset({'weight&gt;75.0%', 'activating', 'hg38...\n[('SSBP2', 0.869120775272342), ('ARID5B', 2.30...\n1221\n\n\nSPIB\nhocomoco__SPIB_MOUSE.H11MO.0.A\n0.0760484450477065\n3.698357770867258\n0.0\n0.839695\ngene is orthologous to ENSMUSG00000008193 in M...\nfrozenset({'weight&gt;75.0%', 'activating', 'hg38...\n[('FCGR2B', 1.869718459857277), ('TMEM156', 4....\n4732\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nSPIB\ntaipale__Spic_DBD_AAAAAGMGGAAGTA\n0.07192604342454488\n4.096439803073309\n0.0\n1.0\ngene is annotated for similar motif taipale__S...\nfrozenset({'activating', 'hg38__refseq-r80__10...\n[('ALOX5AP', 0.2436475904030507), ('DOPEY2', 4...\n3194\n\n\nSPIB\nhocomoco__IRF8_HUMAN.H11MO.0.B\n0.08214495894116133\n5.281967790108696\n0.0\n1.0\nmotif similar to hocomoco__SPIB_HUMAN.H11MO.0....\nfrozenset({'activating', 'hg38__refseq-r80__10...\n[('ALOX5AP', 0.2436475904030507), ('P2RY14', 1...\n3184\n\n\nSPIB\ndbcorrdb__SPI1__ENCSR000BIJ_1__m1\n0.0709653406500041\n3.9849857036056426\n0.0\n1.0\nmotif similar to hocomoco__SPIB_HUMAN.H11MO.0....\nfrozenset({'activating', 'hg38__refseq-r80__10...\n[('KCNG1', 0.97964022364969), ('CXXC5', 8.0650...\n581\n\n\nSPIB\ndbcorrdb__EP300__ENCSR000DZG_1__m1\n0.07342458385785497\n4.270290112384945\n0.0\n1.0\nmotif similar to hocomoco__SPIB_HUMAN.H11MO.0....\nfrozenset({'activating', 'hg38__refseq-r80__10...\n[('TCF4', 3.091736175722826), ('MEF2C', 7.5755...\n1982\n\n\nSPIB\nhocomoco__IRF4_HUMAN.H11MO.0.A\n0.08185655324008216\n5.24850895294599\n0.0\n1.0\nmotif similar to hocomoco__SPIB_HUMAN.H11MO.0....\nfrozenset({'activating', 'hg38__refseq-r80__10...\n[('MYO1C', 1.9761547224955385), ('TPM2', 0.319...\n993\n\n\n\n\n143 rows × 9 columns\n\n\n\n\nregulon_df.loc[\"SPIB\"].iloc[0][\"Enrichment.6\"][:1000]\n\n\"[('FCGR2B', 1.869718459857277), ('STX7', 6.892725862395004), ('P2RY14', 1.8778070177222), ('TMEM156', 4.272053543910822), ('BLNK', 5.738561565806354), ('CD38', 0.5724236819241179), ('CXCR4', 1.0237407733006956), ('RIMKLB', 3.814839002835818), ('FCRL3', 1.6683434765092275), ('LINC01215', 2.4484182394885634), ('HLA-DMB', 5.275150829219921), ('BCL11A', 5.323875629596565), ('HLA-DQA1', 12.631876848026147), ('TCF4', 3.091736175722826), ('CD40', 5.98234166462111), ('DOPEY2', 4.554553673574461), ('AFF3', 3.926454150323302), ('CXXC5', 8.0650414459558), ('LPAR5', 1.3386477473776452), ('KLF8', 1.6187543679806742), ('SLC44A2', 0.3937692393269603), ('SHISA8', 0.9218564482844408), ('FCRL5', 2.534706255811908), ('IKZF3', 1.4164772622710846), ('MPP6', 0.7198674632752087), ('PLEKHG1', 3.1788634402443443), ('FMNL3', 0.4906561541272032), ('PKIG', 3.768067885404645), ('CCR6', 2.1375793714457263), ('PLAC8', 3.411256052315926), ('BLK', 7.363519956872278), ('SETBP1', 5.3370810233327015), ('HLA-DQA2', 4.0790\"\n\n\n\nsc.pl.embedding(adata, basis=\"X_umap_aucell\", use_raw=False,\n                color=['FCGR2B', 'STX7', 'P2RY14', 'TMEM156'], ncols=2)"
  },
  {
    "objectID": "ipynb/day3-4_velocity1.html",
    "href": "ipynb/day3-4_velocity1.html",
    "title": "RNA velocity with scvelo",
    "section": "",
    "text": "Download Presentation: RNA velocity\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport scanpy as sc\nimport scvelo as scv\nRNA velocity is a method for estimating the rate of change in gene expression in scRNA-seq dataset. For pseudotime trajectory inference, you need to specify a “root” cell at the start of the trajectory. On the other hand, RNA velocity tells you the direction along which cells are evolving in gene expression space. RNA velocity achieves this by examining the ratio of unspliced (intron-containing pre-mRNA) and spliced (exon-only mature mRNA) UMIs in a dataset."
  },
  {
    "objectID": "ipynb/day3-4_velocity1.html#loading-data-into-scvelo-and-examining-splicedunspliced-counts",
    "href": "ipynb/day3-4_velocity1.html#loading-data-into-scvelo-and-examining-splicedunspliced-counts",
    "title": "RNA velocity with scvelo",
    "section": "Loading data into scvelo and examining spliced/unspliced counts",
    "text": "Loading data into scvelo and examining spliced/unspliced counts\nIn the presentation, we walked through the theoretical foundations behind RNA velocity. Here, we will demonstrate how to practically apply it to a dataset using the scvelo package, which is nicely integrated with the scanpy framework we have been working with during the past two days. This exercise is adapted from tutorials on the scvelo documentation page.\nFirst, we will load a dataset on pancreatic endocrinogenesis from a recent study:\nBastidas-Ponce et al. “Comprehensive single cell mRNA profiling reveals a detailed roadmap for pancreatic endocrinogenesis.” Development 2019.\n\nadata = scv.datasets.pancreas()\nadata\n\nAnnData object with n_obs × n_vars = 3696 × 27998\n    obs: 'clusters_coarse', 'clusters', 'S_score', 'G2M_score'\n    var: 'highly_variable_genes'\n    uns: 'clusters_coarse_colors', 'clusters_colors', 'day_colors', 'neighbors', 'pca'\n    obsm: 'X_pca', 'X_umap'\n    layers: 'spliced', 'unspliced'\n    obsp: 'distances', 'connectivities'\n\n\nExercise 1: As we can see from the metadata, this dataset already contains a low dimensional UMAP embedding with cluster annotations. Can you plot this UMAP embedding, coloring the cells by clusters?\n\nClick for Answer\n\n\nAnswer:\n    sc.pl.umap(adata, color='clusters')\n\n\n\n\n# your code here\n\n\n\n\n\n\n\n\nExercise 2: Since we will run RNA velocity on these data, we need to have both a spliced and unspliced count matrix. These should be stored in the adata object. Can you find them in adata.layers?\nUsing the two matrices, can you calculate the fraction of unspliced UMI counts out of the total UMIs, averaged across all of the cells? Hint: You can do this computationally using the .sum(1) and .mean() functions.\n\nClick for Answer\n\n\nAnswer: The unspliced and spliced layers are located at adata.layers[“unspliced”] and adata.layers[“spliced”] respectively.\n    spliced_counts_per_cell = adata.layers[\"spliced\"].sum(1)\n    unspliced_counts_per_cell = adata.layers[\"unspliced\"].sum(1)\n    mean_unspliced_fraction_of_total = (unspliced_counts_per_cell.mean() / (spliced_counts_per_cell.mean() + unspliced_counts_per_cell.mean())) * 100\n\n\n\nWe can also use scvelo.pl.proportions to compute and display the proportions of spliced/unspliced counts. Depending on the protocol used, we typically have between 10%-25% of unspliced molecules containing intronic sequences. For single-nuclei data, you will have many more intronic reads, approximately 60%-70%. We suggest you to examine the variations on cluster level to verify consistency in splicing efficiency.\nExercise 3: Use scv.pl.proportions to plot the proportions of spliced and unspliced UMIs. Use the groupby argument to specify that you want to compute the proportions separately for each cluster annotation.\n\nClick for Answer\n\n\nAnswer:\n    scv.pl.proportions(adata, groupby=\"clusters\")\n\n\n\n\n# your code here\n\n\n\n\n\n\n\n\nHere, we find variations as expected, with slightly lower unspliced proportions at cycling ductal cells, then higher proportion at cell fate commitment in Ngn3-high and Pre-endocrine cells, where many genes start to be transcribed."
  },
  {
    "objectID": "ipynb/day3-4_velocity1.html#data-preprocessing",
    "href": "ipynb/day3-4_velocity1.html#data-preprocessing",
    "title": "RNA velocity with scvelo",
    "section": "Data preprocessing",
    "text": "Data preprocessing\nNext, as with our standard scRNA-seq analysis pipeline, we need to preprocess the data! This requires performing the following steps, which you have studied in previous exercises: - Gene filtering (with a minimum number of counts per cell) - Normalization - Log transformation\nIn scvelo, these steps are combined into a single function, called scv.pp.filter_and_normalize. We will run that command below with two parameters specified: - min_shared_counts requires a minimum number of counts (both spliced and unspliced) for all genes; any other genes are filtered out - n_top_genes is similar to sc.pp.highly_variable_genes from scanpy, finding the top variable genes and filtering out the others\nExercise 4: Run the filter_and_normalize command requiring a min_shared_counts of at least 20 and selecting 2,000 n_top_genes.\n\nClick for Answer\n\n\nAnswer:\n    scv.pp.filter_and_normalize(adata, min_shared_counts=20, n_top_genes=2000)\n\n\n\nExercise 5: As we mentioned, scv.pp.filter_and_normalize combines several scanpy functions into a single command. However, if you want full control over the filtering, normalization, and log-transformation steps, you can run each command individually. Can you write the five lines of code needed to achieve the above steps?\n\nClick for Answer\n\n\nAnswer: The five lines of code using just scanpy would be the following. Note that since scvelo filters based on shared counts between the spliced and unspliced, we need to update adata.X to be equal to the total spliced AND unspliced counts (by default, adata.X is just spliced counts for velocity datasets).\n    adata.X = adata.layers[\"spliced\"] + adata.layers[\"unspliced\"]\n    sc.pp.filter_genes(adata, min_counts=20)\n    sc.pp.normalize_total(adata)\n    sc.pp.log1p(adata)\n    sc.pp.highly_variable_genes(adata, n_top_genes=2000, subset=True)\n\n\n\nExercise 6: Next, we need to compute a PCA and neighborhood graph, as we have done previously. Write the scanpy commands to compute the PCA and then the neighborhood graph (using n_pcs=30 and n_neighbors=30)\n\nClick for Answer\n\n\nAnswer:\n    sc.pp.pca(adata)\n    sc.pp.neighbors(adata, n_pcs=30, n_neighbors=30)\n\n\n\nNext, we need to compute the first and second order moments (means and uncentered variances) computed among nearest neighbors in PCA space, summarized in scv.pp.moments.\n\nscv.pp.moments(adata, n_pcs=None, n_neighbors=None)\n\ncomputing moments based on connectivities\n    finished (0:00:00) --&gt; added \n    'Ms' and 'Mu', moments of un/spliced abundances (adata.layers)\n\n\nVelocities are vectors in gene expression space and represent the direction and speed of movement of the individual cells. The velocities are obtained by modeling transcriptional dynamics of splicing kinetics, either stochastically (default) or deterministically (by setting mode=‘deterministic’). For each gene, a steady-state-ratio of pre-mature (unspliced) and mature (spliced) mRNA counts is fitted, which constitutes a constant transcriptional state. Velocities are then obtained as residuals from this ratio. Positive velocity indicates that a gene is up-regulated, which occurs for cells that show higher abundance of unspliced mRNA for that gene than expected in steady state. Conversely, negative velocity indicates that a gene is down-regulated.\n\nscv.tl.velocity(adata)\n\ncomputing velocities\n    finished (0:00:01) --&gt; added \n    'velocity', velocity vectors for each individual cell (adata.layers)\n\n\nThe combination of velocities across genes can then be used to estimate the future state of an individual cell. In order to project the velocities into a lower-dimensional embedding, transition probabilities of cell-to-cell transitions are estimated. That is, for each velocity vector we find the likely cell transitions that are accordance with that direction. The transition probabilities are computed using cosine correlation between the potential cell-to-cell transitions and the velocity vector, and are stored in a matrix denoted as velocity graph. The resulting velocity graph has dimension 𝑛𝑜𝑏𝑠×𝑛𝑜𝑏𝑠 and summarizes the possible cell state changes that are well explained through the velocity vectors (for runtime speedup it can also be computed on reduced PCA space by setting approx=True).\n\nscv.tl.velocity_graph(adata)\n\ncomputing velocity graph (using 1/160 cores)\n\n\n\n\n\n    finished (0:00:10) --&gt; added \n    'velocity_graph', sparse matrix with cosine correlations (adata.uns)\n\n\nFinally, the velocities are projected onto any embedding, specified by basis, and visualized in one of these ways:\n\non cellular level with scv.pl.velocity_embedding\nas gridlines with scv.pl.velocity_embedding_grid\nas streamlines with scv.pl.velocity_embedding_stream.\n\nNote, that the data has an already pre-computed UMAP embedding, and annotated clusters. When applying to your own data, these can be obtained with scv.tl.umap and scv.tl.louvain, which are direct wrappers around the scanpy functions.\nThe most fine-grained resolution of the velocity vector field we get at single-cell level, with each arrow showing the direction and speed of movement of an individual cell.\nExercise 7: Plot the RNA velocity using the above functions scv.pl.velocity_embedding and scv.pl.velocity_embedding_stream. What looks different about the two representations? Can you color the embeddings by the clusters? For scv.pl.velocity_embedding, what happens what you change the values for the arrow_length and arrow_size parameters?\n\nClick for Answer\n\n\nAnswer: Run the following commands:\n    scv.pl.velocity_embedding(adata, arrow_length=3, arrow_size=2, color='clusters')\n    scv.pl.velocity_embedding_stream(adata, basis='umap', color='clusters')\n\nThese plots reveal several things about cell state transitions, e.g., the early endocrine commitment of Ngn3-cells (yellow) and a clear-cut difference between near-terminal α-cells (blue) and transient β-cells (green).\nThe velocity vector field displayed as streamlines also yields fine-grained insights into the developmental processes. It accurately delineates the cycling population of ductal cells and endocrine progenitors. Further, it illuminates cell states of lineage commitment, cell-cycle exit, and endocrine cell differentiation.\n\n\n\n# your code here\n\ncomputing velocity embedding\n    finished (0:00:00) --&gt; added\n    'velocity_umap', embedded velocity vectors (adata.obsm)\n\n\n\n\n\n\n\n\n\n\n# your code here"
  },
  {
    "objectID": "ipynb/day3-4_velocity1.html#dynamical-modeling-of-rna-velocity",
    "href": "ipynb/day3-4_velocity1.html#dynamical-modeling-of-rna-velocity",
    "title": "RNA velocity with scvelo",
    "section": "Dynamical modeling of RNA velocity",
    "text": "Dynamical modeling of RNA velocity\nSince RNA velocity yields insights into the directionality of gene expression change, we can use the approach to infer a trajectory. One way this is acheives is by recovering estimates of the full transcriptional dynamics (i.e., the transcription rate, the splicing rate, and the degradation rate) instead of using the steady-state asusmption and linear fits. This is particularly useful when you have a dataset without a cluster of cells representing the “steady-state”.\nDynamical modeling of RNA velocity is possible with scvelo and allows for: - Estimation of a latent time - Identification of possible driver genes\nWe run the dynamical model to learn the full transcriptional dynamics of splicing kinetics.\nIt is solved in a likelihood-based expectation-maximization framework, by iteratively estimating the parameters of reaction rates and latent cell-specific variables, i.e. transcriptional state and cell-internal latent time. It thereby aims to learn the unspliced/spliced phase trajectory for each gene.\nThe function scv.tl.recover_dynamics uses expectation maximization to recover the gene velocity dynamics. It will take about 5 mins to run. In the meantime, have a look at the following steps.\n\nscv.tl.recover_dynamics(adata)\n\nrecovering dynamics (using 1/160 cores)\n\n\n\n\n\n    finished (0:06:39) --&gt; added \n    'fit_pars', fitted parameters for splicing dynamics (adata.var)\n\n\nThen, we before we need to estiamte the velocity and compute the velocity graph, specifying this time the “dynamical” mode.\n\nscv.tl.velocity(adata, mode='dynamical')\nscv.tl.velocity_graph(adata)\n\ncomputing velocities\n    finished (0:00:04) --&gt; added \n    'velocity', velocity vectors for each individual cell (adata.layers)\ncomputing velocity graph (using 1/160 cores)\n\n\n\n\n\n    finished (0:00:06) --&gt; added \n    'velocity_graph', sparse matrix with cosine correlations (adata.uns)\n\n\n\nscv.pl.velocity_embedding_stream(adata, basis='umap')\n\ncomputing velocity embedding\n    finished (0:00:00) --&gt; added\n    'velocity_umap', embedded velocity vectors (adata.obsm)\n\n\n\n\n\n\n\n\n\nWith the dynamical model, The rates of RNA transcription, splicing and degradation are estimated without the need of any experimental data.\nThey can be useful to better understand the cell identity and phenotypic heterogeneity.\n\ndf = adata.var\ndf = df[(df['fit_likelihood'] &gt; .1) & df['velocity_genes'] == True]\n\nkwargs = dict(xscale='log', fontsize=16)\nwith scv.GridSpec(ncols=3) as pl:\n    pl.hist(df['fit_alpha'], xlabel='transcription rate', **kwargs)\n    pl.hist(df['fit_beta'] * df['fit_scaling'], xlabel='splicing rate', xticks=[.1, .4, 1], **kwargs)\n    pl.hist(df['fit_gamma'], xlabel='degradation rate', xticks=[.1, .4, 1], **kwargs)\n\nscv.get_df(adata, 'fit*', dropna=True).head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfit_alpha\nfit_beta\nfit_gamma\nfit_t_\nfit_scaling\nfit_std_u\nfit_std_s\nfit_likelihood\nfit_u0\nfit_s0\nfit_pval_steady\nfit_steady_u\nfit_steady_s\nfit_variance\nfit_alignment_scaling\nfit_r2\n\n\nindex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSntg1\n0.010688\n0.003801\n0.070298\n26.614919\n48.409039\n1.015340\n0.024470\n0.382195\n0.0\n0.0\n0.034819\n2.402895\n0.072442\n0.238926\n6.656648\n0.467566\n\n\nSbspon\n0.466790\n2.485521\n0.407607\n3.988951\n0.145663\n0.058015\n0.188189\n0.267242\n0.0\n0.0\n0.206473\n0.150839\n0.503312\n0.690054\n1.242929\n0.630782\n\n\nMcm3\n3.759589\n52.011484\n0.885513\n1.945249\n0.011923\n0.015849\n0.686825\n0.118566\n0.0\n0.0\n0.483623\n0.060455\n2.054615\n1.426064\n0.765975\n0.275293\n\n\nFam135a\n0.185720\n0.124718\n0.218989\n10.854791\n1.090526\n0.353996\n0.153378\n0.293041\n0.0\n0.0\n0.397257\n1.331401\n0.395752\n0.611777\n3.308631\n0.363101\n\n\nAdgrb3\n0.034546\n0.007763\n0.222066\n8.947714\n120.303430\n2.122017\n0.030074\n0.295653\n0.0\n0.0\n0.063172\n4.564255\n0.093443\n0.525843\n1.825699\n0.399879\n\n\n\n\n\n\n\nThe dynamical model recovers the latent time of the underlying cellular processes. This latent time represents the cell’s internal clock and approximates the real time experienced by cells as they differentiate, based only on its transcriptional dynamics. This offers advantages over traditional pseudotime trajectory inference approaches.\n\nscv.tl.latent_time(adata)\nscv.pl.scatter(adata, color='latent_time', color_map=plt.cm.Spectral, size=80)\n\ncomputing latent time using root_cells as prior\n    finished (0:00:01) --&gt; added \n    'latent_time', shared time (adata.obs)\n\n\n\n\n\n\n\n\n\nExercise 11: Take a look back at the previous exercise, where the diffusion pseudotime was calculated for the same dataset. Are there differences between the dpt_pseudotime and latent_time estimates?\nDriver genes display pronounced dynamic behavior and are systematically detected via their characterization by high likelihoods in the dynamic model. We can plot a heatmap of the top 300 genes expressed along the pseudotime.\n\ntop_genes = adata.var['fit_likelihood'].sort_values(ascending=False).index[:300]\nscv.pl.heatmap(adata, var_names=top_genes, sortby='latent_time', col_color='clusters', n_convolve=100)\n\n\n\n\n\n\n\n\nFor any top candidates that you might want to validate biologically, it is always essential to examine the phase portraits, to ensure that the gene is not too noisy.\n\ntop_genes = adata.var['fit_likelihood'].sort_values(ascending=False).index\nscv.pl.scatter(adata, basis=top_genes[:15], ncols=5, frameon=False)\n\n\n\n\n\n\n\n\n\nvar_names = ['Actn4', 'Ppp3ca', 'Cpe', 'Nnat']\nscv.pl.scatter(adata, var_names, frameon=False)\nscv.pl.scatter(adata, x='latent_time', y=var_names, frameon=False)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can then use scv.tl.rank_dynamical_genes to rank the top dynamical genes by cluster. These are the genes that most strongly contribute to the cell-wise velocity for cells belonging to each cluster.\n\nscv.tl.rank_dynamical_genes(adata, groupby='clusters')\ndf = scv.get_df(adata, 'rank_dynamical_genes/names')\ndf.head(5)\n\nranking genes by cluster-specific likelihoods\n    finished (0:00:03) --&gt; added \n    'rank_dynamical_genes', sorted scores by group ids (adata.uns)\n\n\n\n\n\n\n\n\n\nDuctal\nNgn3 low EP\nNgn3 high EP\nPre-endocrine\nBeta\nAlpha\nDelta\nEpsilon\n\n\n\n\n0\nNfib\nLockd\nRbfox3\nPcsk2\nPcsk2\nGnao1\nPcsk2\nPak3\n\n\n1\nTop2a\nDcdc2a\nBtbd17\nAbcc8\nAnk\nCpe\nAbcc8\nTox3\n\n\n2\nIncenp\nAdk\nTspan5\nRap1b\nAbcc8\nPak3\nPak3\nRap1gap2\n\n\n3\nWfdc15b\nBicc1\nRap1gap2\nTmem163\nTspan7\nRap1b\nRap1b\nRnf130\n\n\n4\nCdk1\nWfdc15b\nSulf2\nAnk\nCryba2\nPim2\nMeis2\nMeis2\n\n\n\n\n\n\n\n\nfor cluster in ['Ductal', 'Ngn3 high EP', 'Pre-endocrine', 'Beta']:\n    scv.pl.scatter(adata, df[cluster][:5], ylabel=cluster, frameon=False)"
  },
  {
    "objectID": "ipynb/day3-4_velocity1.html#velocities-in-cycling-progenitors",
    "href": "ipynb/day3-4_velocity1.html#velocities-in-cycling-progenitors",
    "title": "RNA velocity with scvelo",
    "section": "Velocities in cycling progenitors",
    "text": "Velocities in cycling progenitors\nThe cell cycle detected by RNA velocity, and it is biologically affirmed by cell cycle scores (standardized scores of mean expression levels of phase marker genes).\n\nscv.tl.score_genes_cell_cycle(adata)\nscv.pl.scatter(adata, color_gradients=['S_score', 'G2M_score'], smooth=True, perc=[5, 95])\n\ncalculating cell cycle phase\n--&gt;     'S_score' and 'G2M_score', scores of cell cycle phases (adata.obs)\n\n\n\n\n\n\n\n\n\nFor the cycling Ductal cells, we may screen through S and G2M phase markers. The previous module also computed a spearmans correlation score, which we can use to rank/sort the phase marker genes to then display their phase portraits.\n\ns_genes, g2m_genes = scv.utils.get_phase_marker_genes(adata)\ns_genes = scv.get_df(adata[:, s_genes], 'spearmans_score', sort_values=True).index\ng2m_genes = scv.get_df(adata[:, g2m_genes], 'spearmans_score', sort_values=True).index\n\nkwargs = dict(frameon=False, ylabel='cell cycle genes')\nscv.pl.scatter(adata, list(s_genes[:2]) + list(g2m_genes[:3]), **kwargs)\n\n\n\n\n\n\n\n\nParticularly Hells and Top2a are well-suited to explain the vector field in the cycling progenitors. Top2a gets assigned a high velocity shortly before it actually peaks in the G2M phase. There, the negative velocity then perfectly matches the immediately following down-regulation.\nExercise 12: As you did previously for other genes, can you use scvelo plotting functions to visualize the phase portraits and gene-wise velocity for the Hells and Top2a genes?\n\nClick for Answer\n\n\nAnswer:\n    scv.pl.velocity(adata, ['Hells',  'Top2a'], ncols=2)\n\n\n\n\n# your code here\n\n\n\n\n\n\n\n\nThe cell cycle is an interesting case for RNA velocity estimation, as pseudotime methods along often fail as estimations of cyclical processes. Moreover, RNA velocity corresponds roughly to cell cycle speed, which is both experimentally verifiable. The cell cycle also unfolds on a timescale of less than 24 hours, which is well suited for studying cell dynamics using RNA lifecycle kinetics, such as with RNA velocity."
  }
]